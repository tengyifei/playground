{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1: place_to_host, place_to_device are not recognized by torch AOTAutograd\n",
    "\n",
    "You can't call `place_to_host` on a torch functional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4, 4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace[abi:cxx11]()\n\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyVectorcall_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t_start\n*** End stack trace ***\nInput tensor is not an XLA tensor: XLAFloatType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m cloned_a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m torch_xla\u001b[38;5;241m.\u001b[39msync()\n\u001b[0;32m---> 30\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43maot_print_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloned_a\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m     (fake_mode, shape_env) \u001b[38;5;241m=\u001b[39m construct_fake_mode(flat_args, aot_config)\n\u001b[1;32m    884\u001b[0m     fake_flat_args: FakifiedFlatArgs \u001b[38;5;241m=\u001b[39m process_inputs(\n\u001b[1;32m    885\u001b[0m         flat_args, aot_config, fake_mode, shape_env\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 887\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:527\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    520\u001b[0m     flat_fn,\n\u001b[1;32m    521\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:635\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    633\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m--> 635\u001b[0m     fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mrun_functionalized_fw_and_collect_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_input_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_input_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_autograd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m req_subclass_dispatch \u001b[38;5;241m=\u001b[39m requires_subclass_dispatch(\n\u001b[1;32m    645\u001b[0m     fake_flat_args, fw_metadata\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m try_record_chromium_data(\n\u001b[1;32m    648\u001b[0m     requires_subclass_dispatch\u001b[38;5;241m=\u001b[39mreq_subclass_dispatch\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py:197\u001b[0m, in \u001b[0;36mrun_functionalized_fw_and_collect_metadata.<locals>.inner\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m disable_above, mode, suppress_pending:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     flat_f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(_to_fun, flat_args)\n\u001b[0;32m--> 197\u001b[0m     flat_f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_f_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# We didn't do any tracing, so we don't need to process the\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# unbacked symbols, they will just disappear into the ether.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Also, prevent memoization from applying.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fake_mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:184\u001b[0m, in \u001b[0;36mcreate_tree_flattened_fn.<locals>.flat_fn\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m out_spec\n\u001b[1;32m    183\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, tensor_args_spec)\n\u001b[0;32m--> 184\u001b[0m tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m flat_out, spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flat_out:\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mfn\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mplace_to_host\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m a \u001b[38;5;241m=\u001b[39m place_to_device(a)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py:38\u001b[0m, in \u001b[0;36mplace_to_host\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplace_to_host\u001b[39m(a: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstablehlo_custom_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotate_device_placement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_side_effect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfrontend_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_xla_buffer_placement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpinned_host\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py:16\u001b[0m, in \u001b[0;36mstablehlo_custom_call\u001b[0;34m(args, call_target, output_shapes, output_dtypes, has_side_effect, backend_config, api_version, frontend_attributes)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstablehlo_custom_call\u001b[39m(args,\n\u001b[1;32m      8\u001b[0m                           call_target,\n\u001b[1;32m      9\u001b[0m                           output_shapes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                           api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     14\u001b[0m                           frontend_attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m   frontend_attributes \u001b[38;5;241m=\u001b[39m frontend_attributes \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 16\u001b[0m   res \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_XLAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_custom_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_side_effect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mfrontend_attributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output_shapes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace[abi:cxx11]()\n\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n\t\n\t\n\t\n\t_PyObject_MakeTpCall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPyVectorcall_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\tPy_RunMain\n\tPy_BytesMain\n\t__libc_start_main\n\t_start\n*** End stack trace ***\nInput tensor is not an XLA tensor: XLAFloatType"
     ]
    }
   ],
   "source": [
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "from torch_xla.experimental.stablehlo_custom_call import place_to_host, place_to_device\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "def fn(a):\n",
    "  \"\"\"The identity function but moves the input to host then to device.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  a = place_to_host(a)\n",
    "  a = place_to_device(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a, b, c, d = [torch.randn(4, 4, 4, 4, requires_grad=True, device=device) for _ in range(4)]\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: AOTAutograd ignores saved_tensors_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3):\n",
      "    t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    unsqueeze = torch.ops.aten.unsqueeze.default(primals_3, 0);  primals_3 = None\n",
      "    mm = torch.ops.aten.mm.default(unsqueeze, t)\n",
      "    squeeze = torch.ops.aten.squeeze.dim(mm, 0);  mm = None\n",
      "    add = torch.ops.aten.add.Tensor(squeeze, primals_2);  squeeze = primals_2 = None\n",
      "    sin = torch.ops.aten.sin.default(add)\n",
      "    return (sin, t, unsqueeze, add)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "from functorch.compile import aot_module\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    def pack(x):\n",
    "      print(f\"Packing {type(x)} {x.shape}\")\n",
    "      return place_to_host(x)\n",
    "\n",
    "    def unpack(x):\n",
    "      print(f\"Unpacking {type(x)} {x.shape}\")\n",
    "      return place_to_device(x)\n",
    "\n",
    "    with saved_tensors_hooks(pack, unpack):\n",
    "      return self.m(*args, **kwargs)\n",
    "    \n",
    "class Layer(torch.nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.l = nn.Linear(4, 4)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.l(x)\n",
    "    x = torch.sin(x)\n",
    "    return x\n",
    "\n",
    "with torch_xla.runtime.xla_device():\n",
    "  layer = Layer()\n",
    "  layer = OffloadingModule(layer)\n",
    "\n",
    "a = torch.randn(4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_module(layer, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
