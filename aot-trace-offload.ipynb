{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1: place_to_host, place_to_device are not recognized by torch AOTAutograd\n",
    "\n",
    "You can't call `place_to_host` on a torch functional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4, 4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n",
      "\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyObject_MakeTpCall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPyVectorcall_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPy_RunMain\n",
      "\tPy_BytesMain\n",
      "\t__libc_start_main\n",
      "\t_start\n",
      "*** End stack trace ***\n",
      "Input tensor is not an XLA tensor: XLAFloatType\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_382270/90495241.py\", line 31, in <module>\n",
      "    res = aot_print_fn(cloned_a)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 635, in _create_aot_dispatcher_function\n",
      "    fw_metadata = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 197, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_382270/90495241.py\", line 17, in fn\n",
      "    a = place_to_host(a)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py\", line 38, in place_to_host\n",
      "    return stablehlo_custom_call(\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py\", line 16, in stablehlo_custom_call\n",
      "    res = torch_xla._XLAC._xla_custom_call(args, call_target, output_shapes,\n",
      "RuntimeError: torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n",
      "\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyObject_MakeTpCall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPyVectorcall_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPy_RunMain\n",
      "\tPy_BytesMain\n",
      "\t__libc_start_main\n",
      "\t_start\n",
      "*** End stack trace ***\n",
      "Input tensor is not an XLA tensor: XLAFloatType\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "from torch_xla.experimental.stablehlo_custom_call import place_to_host, place_to_device\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "def fn(a):\n",
    "  \"\"\"The identity function but moves the input to host then to device.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  a = place_to_host(a)\n",
    "  a = place_to_device(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a, b, c, d = [torch.randn(4, 4, 4, 4, requires_grad=True, device=device) for _ in range(4)]\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "try:\n",
    "  res = aot_print_fn(cloned_a)\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: AOTAutograd ignores saved_tensors_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3):\n",
      "    t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    unsqueeze = torch.ops.aten.unsqueeze.default(primals_3, 0);  primals_3 = None\n",
      "    mm = torch.ops.aten.mm.default(unsqueeze, t)\n",
      "    squeeze = torch.ops.aten.squeeze.dim(mm, 0);  mm = None\n",
      "    add = torch.ops.aten.add.Tensor(squeeze, primals_2);  squeeze = primals_2 = None\n",
      "    sin = torch.ops.aten.sin.default(add)\n",
      "    return (sin, t, unsqueeze, add)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "from functorch.compile import aot_module\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    def pack(x):\n",
    "      print(f\"Packing {type(x)} {x.shape}\")\n",
    "      return place_to_host(x)\n",
    "\n",
    "    def unpack(x):\n",
    "      print(f\"Unpacking {type(x)} {x.shape}\")\n",
    "      return place_to_device(x)\n",
    "\n",
    "    with saved_tensors_hooks(pack, unpack):\n",
    "      return self.m(*args, **kwargs)\n",
    "    \n",
    "class Layer(torch.nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.l = nn.Linear(4, 4)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.l(x)\n",
    "    x = torch.sin(x)\n",
    "    return x\n",
    "\n",
    "with torch_xla.runtime.xla_device():\n",
    "  layer = Layer()\n",
    "  layer = OffloadingModule(layer)\n",
    "\n",
    "a = torch.randn(4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_module(layer, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
