{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: place_to_host, place_to_device are not recognized by torch AOTAutograd\n",
    "\n",
    "You can't call `place_to_host` on a torch functional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:250: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4, 4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n",
      "\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyObject_MakeTpCall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPyVectorcall_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPy_RunMain\n",
      "\tPy_BytesMain\n",
      "\t__libc_start_main\n",
      "\t_start\n",
      "*** End stack trace ***\n",
      "Input tensor is not an XLA tensor: XLAFloatType\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2707011/90495241.py\", line 31, in <module>\n",
      "    res = aot_print_fn(cloned_a)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 635, in _create_aot_dispatcher_function\n",
      "    fw_metadata = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 197, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2707011/90495241.py\", line 17, in fn\n",
      "    a = place_to_host(a)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py\", line 38, in place_to_host\n",
      "    return stablehlo_custom_call(\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/experimental/stablehlo_custom_call.py\", line 16, in stablehlo_custom_call\n",
      "    res = torch_xla._XLAC._xla_custom_call(args, call_target, output_shapes,\n",
      "RuntimeError: torch_xla/csrc/aten_xla_bridge.cpp:105 : Check failed: xtensor \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\ttorch_xla::bridge::GetXlaTensor(at::Tensor const&)\n",
      "\ttorch_xla::bridge::GetXlaTensors(c10::IListRef<at::Tensor> const&)\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyObject_MakeTpCall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPyVectorcall_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPy_RunMain\n",
      "\tPy_BytesMain\n",
      "\t__libc_start_main\n",
      "\t_start\n",
      "*** End stack trace ***\n",
      "Input tensor is not an XLA tensor: XLAFloatType\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "from torch_xla.experimental.stablehlo_custom_call import place_to_host, place_to_device\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "def fn(a):\n",
    "  \"\"\"The identity function but moves the input to host then to device.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  a = place_to_host(a)\n",
    "  a = place_to_device(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a, b, c, d = [torch.randn(4, 4, 4, 4, requires_grad=True, device=device) for _ in range(4)]\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "try:\n",
    "  res = aot_print_fn(cloned_a)\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: AOTAutograd ignores saved_tensors_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3):\n",
      "    t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    unsqueeze = torch.ops.aten.unsqueeze.default(primals_3, 0);  primals_3 = None\n",
      "    mm = torch.ops.aten.mm.default(unsqueeze, t)\n",
      "    squeeze = torch.ops.aten.squeeze.dim(mm, 0);  mm = None\n",
      "    add = torch.ops.aten.add.Tensor(squeeze, primals_2);  squeeze = primals_2 = None\n",
      "    sin = torch.ops.aten.sin.default(add)\n",
      "    return (sin, t, unsqueeze, add)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "from functorch.compile import aot_module\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    def pack(x):\n",
    "      print(f\"Packing {type(x)} {x.shape}\")\n",
    "      return place_to_host(x)\n",
    "\n",
    "    def unpack(x):\n",
    "      print(f\"Unpacking {type(x)} {x.shape}\")\n",
    "      return place_to_device(x)\n",
    "\n",
    "    with saved_tensors_hooks(pack, unpack):\n",
    "      return self.m(*args, **kwargs)\n",
    "    \n",
    "class Layer(torch.nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.l = nn.Linear(4, 4)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.l(x)\n",
    "    x = torch.sin(x)\n",
    "    return x\n",
    "\n",
    "with torch_xla.runtime.xla_device():\n",
    "  layer = Layer()\n",
    "  layer = OffloadingModule(layer)\n",
    "\n",
    "a = torch.randn(4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_module(layer, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: trace a function into fw,bw, then wrap the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:250: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "from torch.library import impl, register_fake\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "from torch_xla.experimental.stablehlo_custom_call import place_to_host, place_to_device\n",
    "\n",
    "@torch.library.custom_op(\"xla::place_to_host\", mutates_args=())\n",
    "def to_host(t: torch.Tensor) -> torch.Tensor:\n",
    "  return place_to_host(t)\n",
    "\n",
    "@to_host.register_fake\n",
    "def _(t: torch.Tensor) -> torch.Tensor:\n",
    "  return torch.empty_like(t)\n",
    "\n",
    "def to_host_backward(ctx, grad):\n",
    "    return grad\n",
    "\n",
    "to_host.register_autograd(to_host_backward)\n",
    "\n",
    "@torch.library.custom_op(\"xla::place_to_device\", mutates_args=())\n",
    "def to_device(t: torch.Tensor) -> torch.Tensor:\n",
    "  return place_to_device(t)\n",
    "\n",
    "@to_device.register_fake\n",
    "def _(t: torch.Tensor) -> torch.Tensor:\n",
    "  return torch.empty_like(t)\n",
    "\n",
    "def to_device_backward(ctx, grad):\n",
    "    return grad\n",
    "\n",
    "to_device.register_autograd(to_device_backward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured forward graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1):\n",
      "    add = torch.ops.aten.add.Tensor(primals_1, 456);  primals_1 = None\n",
      "    place_to_host = torch.ops.xla.place_to_host.default(add);  add = None\n",
      "    place_to_device = torch.ops.xla.place_to_device.default(place_to_host);  place_to_host = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(place_to_device, 123);  place_to_device = None\n",
      "    return (add_1,)\n",
      "    \n",
      "Captured backward graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, tangents_1):\n",
      "    return (tangents_1,)\n",
      "    \n",
      "Res: tensor([578.8382, 579.8059, 579.2380, 580.3248], device='xla:0',\n",
      "       grad_fn=<CompiledFunctionBackward>), cloned a grad: tensor([1., 1., 1., 1.], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from functorch.compile import aot_function, make_boxed_func  # type:ignore\n",
    "from functools import partial\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "a = torch.randn(4, requires_grad=True, device=device)\n",
    "\n",
    "def my_layer(t):\n",
    "  t = t + 456\n",
    "  host_t = torch.ops.xla.place_to_host(t)  # type:ignore\n",
    "  device_t = torch.ops.xla.place_to_device(host_t)   # type:ignore\n",
    "  return device_t + 123\n",
    "\n",
    "def compiler_fn(name: str, m: torch.fx.GraphModule, _):\n",
    "  print(f\"Captured {name} graph:\")\n",
    "  print(m.code)\n",
    "  import time\n",
    "  time.sleep(3)\n",
    "  return make_boxed_func(m)\n",
    "\n",
    "a = torch.randn(4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(\n",
    "  my_layer,\n",
    "  fw_compiler=partial(compiler_fn, \"forward\"),\n",
    "  bw_compiler=partial(compiler_fn, \"backward\")\n",
    ")\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a)\n",
    "torch_xla.sync()\n",
    "res.sum().backward()\n",
    "torch_xla.sync()\n",
    "print(f\"Res: {res}, cloned a grad: {cloned_a.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
