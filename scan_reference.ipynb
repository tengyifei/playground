{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan op reference implementation\n",
    "\n",
    "We present one Python based for-loop impl, and one JAX impl, to compare correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=CPU\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723077440.079486 1033945 cpu_client.cc:466] TfrtCpuClient created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(249., device='xla:0', grad_fn=<SumBackward0>)\n",
      "init_carry grad: tensor([12., 15., 18.], device='xla:0')\n",
      "xs grad: tensor([[12., 14., 16.],\n",
      "        [ 9., 11., 13.],\n",
      "        [ 6.,  8., 10.]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from typing import Callable, TypeVar, Tuple\n",
    "\n",
    "Carry = TypeVar('Carry')\n",
    "X = torch.Tensor\n",
    "Y = torch.Tensor\n",
    "\n",
    "\n",
    "def scan(fn: Callable[[Carry, X], Tuple[Carry, Y]], init: Carry,\n",
    "         xs: X) -> Tuple[Carry, Y]:\n",
    "  carry = init\n",
    "  ys = []\n",
    "\n",
    "  for i in range(xs.size(0)):\n",
    "    carry, y = fn(carry, xs[i])\n",
    "    ys.append(y)\n",
    "\n",
    "  # Stack the list of outputs into a single tensor\n",
    "  ys = torch.stack(ys)\n",
    "\n",
    "  return carry, ys\n",
    "\n",
    "\n",
    "# Test Function\n",
    "def step_fn(carry, x):\n",
    "  new_carry = carry + x\n",
    "  y = carry * x\n",
    "  return new_carry, y\n",
    "\n",
    "\n",
    "# Test the simplified scan implementation\n",
    "device = torch_xla.device()\n",
    "\n",
    "# Initial carry (let's make it a scalar with requires_grad)\n",
    "init_carry = torch.tensor([1.0, 1.0, 1.0], requires_grad=True, device=device)\n",
    "\n",
    "# Example input tensor of shape (batch_size, features)\n",
    "xs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                  requires_grad=True,\n",
    "                  device=device)\n",
    "\n",
    "# Use the scan function\n",
    "final_carry, ys = scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Loss for backward pass (sum of the outputs)\n",
    "loss = ys.sum()\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Output gradients\n",
    "print(\"init_carry grad:\", init_carry.grad)\n",
    "print(\"xs grad:\", xs.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x7fe2075cef50>\n",
      "  <StackBackward0 object at 0x7fe2075cf490>\n",
      "    <MulBackward0 object at 0x7fe2075cf520>\n",
      "      <AccumulateGrad object at 0x7fe2075cf0d0>\n",
      "      <SelectBackward0 object at 0x7fe2075cf010>\n",
      "        <AccumulateGrad object at 0x7fe2075cf070>\n",
      "    <MulBackward0 object at 0x7fe2075cf850>\n",
      "      <AddBackward0 object at 0x7fe2075cf010>\n",
      "        <AccumulateGrad object at 0x7fe2075cf070>\n",
      "        <SelectBackward0 object at 0x7fe2075cf040>\n",
      "          <AccumulateGrad object at 0x7fe2075cebc0>\n",
      "      <SelectBackward0 object at 0x7fe2075cf0a0>\n",
      "        <AccumulateGrad object at 0x7fe2075cf040>\n",
      "    <MulBackward0 object at 0x7fe2075cf340>\n",
      "      <AddBackward0 object at 0x7fe2075cf0a0>\n",
      "        <AddBackward0 object at 0x7fe2075cf040>\n",
      "          <AccumulateGrad object at 0x7fe2075cebc0>\n",
      "          <SelectBackward0 object at 0x7fe2075ceb90>\n",
      "            <AccumulateGrad object at 0x7fe2075cef20>\n",
      "        <SelectBackward0 object at 0x7fe2075cebf0>\n",
      "          <AccumulateGrad object at 0x7fe2075ceb90>\n",
      "      <SelectBackward0 object at 0x7fe2075cf0d0>\n",
      "        <AccumulateGrad object at 0x7fe2075cebf0>\n"
     ]
    }
   ],
   "source": [
    "def print_grad_fn(grad_fn, level=0):\n",
    "    if grad_fn is None:\n",
    "        return\n",
    "\n",
    "    print(\"  \" * level + str(grad_fn))\n",
    "    for next_fn in grad_fn.next_functions:\n",
    "        if next_fn[0] is not None:\n",
    "            print_grad_fn(next_fn[0], level + 1)\n",
    "\n",
    "print_grad_fn(loss.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we capture the IR graph of the backward part of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(249., device='xla:0', grad_fn=<SumBackward0>)\n",
      "HLO graph:\n",
      "digraph G {\n",
      "  node0 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node1 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node2 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node3 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node4 [label=\"aten::expand\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node5 [label=\"aten::expand\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node6 [label=\"xla::generic_slice\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node7 [label=\"aten::view\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node8 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node9 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node10 [label=\"xla::generic_slice\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node11 [label=\"aten::view\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node12 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node13 [label=\"aten::add\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node14 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node15 [label=\"xla::generic_slice\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node16 [label=\"aten::view\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node17 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node18 [label=\"aten::add\\nf32[3]{0}\\nxla_shape=f32[3]{0}\\nROOT=0\"]\n",
      "  node19 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node20 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node21 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node22 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node23 [label=\"aten::add\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node24 [label=\"aten::view\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node25 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node26 [label=\"aten::expand\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node27 [label=\"xla::update_slice\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node28 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node29 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node30 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node31 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node32 [label=\"aten::add\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node33 [label=\"aten::view\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node34 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node35 [label=\"aten::expand\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node36 [label=\"xla::update_slice\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node37 [label=\"xla::device_data\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node38 [label=\"aten::mul\\nf32[3]{0}\\nxla_shape=f32[3]{0}\"]\n",
      "  node39 [label=\"aten::view\\nf32[1,3]{1,0}\\nxla_shape=f32[1,3]{1,0}\"]\n",
      "  node40 [label=\"prim::Constant\\nf32[]\\nxla_shape=f32[]\"]\n",
      "  node41 [label=\"aten::expand\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node42 [label=\"xla::update_slice\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node43 [label=\"aten::add\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\"]\n",
      "  node44 [label=\"aten::add\\nf32[3,3]{1,0}\\nxla_shape=f32[3,3]{1,0}\\nROOT=1\"]\n",
      "  node43 -> node44 [label=\"i=0\"]\n",
      "  node27 -> node44 [label=\"i=1\"]\n",
      "  node19 -> node44 [label=\"i=2\"]\n",
      "  node42 -> node43 [label=\"i=0\"]\n",
      "  node36 -> node43 [label=\"i=1\"]\n",
      "  node28 -> node43 [label=\"i=2\"]\n",
      "  node41 -> node42 [label=\"i=0\"]\n",
      "  node39 -> node42 [label=\"i=1\"]\n",
      "  node40 -> node41\n",
      "  node38 -> node39\n",
      "  node7 -> node38 [label=\"i=0\"]\n",
      "  node37 -> node38 [label=\"i=1\"]\n",
      "  node35 -> node36 [label=\"i=0\"]\n",
      "  node33 -> node36 [label=\"i=1\"]\n",
      "  node34 -> node35\n",
      "  node32 -> node33\n",
      "  node31 -> node32 [label=\"i=0\"]\n",
      "  node8 -> node32 [label=\"i=1\"]\n",
      "  node29 -> node32 [label=\"i=2\"]\n",
      "  node11 -> node31 [label=\"i=0\"]\n",
      "  node30 -> node31 [label=\"i=1\"]\n",
      "  node26 -> node27 [label=\"i=0\"]\n",
      "  node24 -> node27 [label=\"i=1\"]\n",
      "  node25 -> node26\n",
      "  node23 -> node24\n",
      "  node22 -> node23 [label=\"i=0\"]\n",
      "  node13 -> node23 [label=\"i=1\"]\n",
      "  node20 -> node23 [label=\"i=2\"]\n",
      "  node16 -> node22 [label=\"i=0\"]\n",
      "  node21 -> node22 [label=\"i=1\"]\n",
      "  node17 -> node18 [label=\"i=0\"]\n",
      "  node13 -> node18 [label=\"i=1\"]\n",
      "  node0 -> node18 [label=\"i=2\"]\n",
      "  node16 -> node17 [label=\"i=0\"]\n",
      "  node14 -> node17 [label=\"i=1\"]\n",
      "  node15 -> node16\n",
      "  node5 -> node15\n",
      "  node12 -> node13 [label=\"i=0\"]\n",
      "  node8 -> node13 [label=\"i=1\"]\n",
      "  node1 -> node13 [label=\"i=2\"]\n",
      "  node11 -> node12 [label=\"i=0\"]\n",
      "  node9 -> node12 [label=\"i=1\"]\n",
      "  node10 -> node11\n",
      "  node5 -> node10\n",
      "  node7 -> node8 [label=\"i=0\"]\n",
      "  node2 -> node8 [label=\"i=1\"]\n",
      "  node6 -> node7\n",
      "  node5 -> node6\n",
      "  node4 -> node5\n",
      "  node3 -> node4\n",
      "}\n",
      "\n",
      "init_carry grad: tensor([12., 15., 18.], device='xla:0')\n",
      "xs grad: tensor([[12., 14., 16.],\n",
      "        [ 9., 11., 13.],\n",
      "        [ 6.,  8., 10.]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch_xla.device()\n",
    "\n",
    "init_carry = torch.tensor([1.0, 1.0, 1.0], requires_grad=True, device=device)\n",
    "\n",
    "xs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],\n",
    "                  requires_grad=True,\n",
    "                  device=device)\n",
    "\n",
    "# Use the scan function\n",
    "final_carry, ys = scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Loss for backward pass (sum of the outputs)\n",
    "loss = ys.sum()\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Cut the graph off at forward.\n",
    "torch_xla.sync()\n",
    "_ = f\"{init_carry}\"\n",
    "_ = f\"{xs}\"\n",
    "_ = f\"{loss}\"\n",
    "\n",
    "# Now trace the backwards.\n",
    "loss.backward()\n",
    "\n",
    "tensors = [init_carry.grad, xs.grad]\n",
    "\n",
    "print(\"HLO graph:\")\n",
    "print(torch_xla._XLAC._get_xla_tensors_dot(tensors))\n",
    "\n",
    "# Output gradients\n",
    "print(\"init_carry grad:\", init_carry.grad)\n",
    "print(\"xs grad:\", xs.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivalent example in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 249.0\n",
      "Gradients of init_carry: [12. 15. 18.]\n",
      "Gradients of xs: [[12. 14. 16.]\n",
      " [ 9. 11. 13.]\n",
      " [ 6.  8. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad\n",
    "from jax import lax\n",
    "\n",
    "# Define the step function\n",
    "def step_fn(carry, x):\n",
    "    new_carry = carry + x\n",
    "    y = carry * x\n",
    "    return new_carry, y\n",
    "\n",
    "# Initial carry (same as init_carry in PyTorch)\n",
    "init_carry = jnp.array([1.0, 1.0, 1.0])\n",
    "\n",
    "# Example input tensor of shape (batch_size, features)\n",
    "xs = jnp.array([[1.0, 2.0, 3.0], \n",
    "                [4.0, 5.0, 6.0], \n",
    "                [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Use jax.lax.scan to apply the step function\n",
    "final_carry, ys = lax.scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Define a function to compute the loss\n",
    "def compute_loss(init_carry, xs):\n",
    "    _, ys = lax.scan(step_fn, init_carry, xs)\n",
    "    return jnp.sum(ys)\n",
    "\n",
    "# Compute the gradients\n",
    "loss_value, grads = value_and_grad(compute_loss, argnums=(0, 1))(init_carry, xs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Loss:\", loss_value)\n",
    "print(\"Gradients of init_carry:\", grads[0])\n",
    "print(\"Gradients of xs:\", grads[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of pytree support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final carry: (tensor([10.], device='xla:0'), tensor([46., 47.], device='xla:0'))\n",
      "Outputs ys: (tensor([[2., 4.],\n",
      "        [6., 8.]], device='xla:0'), tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]], device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "# Hypothetical `scan` function that supports PyTrees\n",
    "def scan(fn, init, xs):\n",
    "    carry = init\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(xs[0])):\n",
    "        carry, y = fn(carry, tree_map(lambda x: x[i], xs))\n",
    "        ys.append(y)\n",
    "\n",
    "    # Stack the results of y (if it's a tensor) into a single tensor\n",
    "    ys = tree_map(lambda *x: torch.stack(x), *ys)\n",
    "    return carry, ys\n",
    "\n",
    "# Step function that operates on a tuple (carry, (x1, x2)) where x1 and x2 have different sizes\n",
    "def step_fn(carry, x):\n",
    "    carry1, carry2 = carry\n",
    "    x1, x2 = x\n",
    "\n",
    "    new_carry1 = carry1 + x1.sum()\n",
    "    new_carry2 = carry2 + x2.sum()\n",
    "\n",
    "    y1 = x1 * 2\n",
    "    y2 = x2 * 2\n",
    "\n",
    "    return (new_carry1, new_carry2), (y1, y2)\n",
    "\n",
    "# Initial carry: tuple of tensors with different sizes\n",
    "init_carry = (torch.tensor([0.0], device=device), torch.tensor([1.0, 2.0], device=device))\n",
    "\n",
    "# Example input: tuple of tensors with different sizes\n",
    "xs = (\n",
    "    torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=device),  # Shape (2, 2)\n",
    "    torch.tensor([[5.0, 6.0, 7.0], [8.0, 9.0, 10.0]], device=device)  # Shape (2, 3)\n",
    ")\n",
    "\n",
    "# Call the scan function\n",
    "final_carry, ys = scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Final carry:\", final_carry)\n",
    "print(\"Outputs ys:\", ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX version of pytree example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final carry: (Array([10.], dtype=float32), Array([46., 47.], dtype=float32))\n",
      "Outputs ys: (Array([[2., 4.],\n",
      "       [6., 8.]], dtype=float32), Array([[10., 12., 14.],\n",
      "       [16., 18., 20.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "# Step function that operates on a tuple (carry, (x1, x2)) where x1 and x2 have different sizes\n",
    "def step_fn(carry, x):\n",
    "    carry1, carry2 = carry\n",
    "    x1, x2 = x\n",
    "\n",
    "    new_carry1 = carry1 + jnp.sum(x1)\n",
    "    new_carry2 = carry2 + jnp.sum(x2)\n",
    "\n",
    "    y1 = x1 * 2\n",
    "    y2 = x2 * 2\n",
    "\n",
    "    return (new_carry1, new_carry2), (y1, y2)\n",
    "\n",
    "# Initial carry: tuple of arrays with different sizes\n",
    "init_carry = (jnp.array([0.0]), jnp.array([1.0, 2.0]))\n",
    "\n",
    "# Example input: tuple of arrays with different sizes\n",
    "xs = (\n",
    "    jnp.array([[1.0, 2.0], [3.0, 4.0]]),  # Shape (2, 2)\n",
    "    jnp.array([[5.0, 6.0, 7.0], [8.0, 9.0, 10.0]])  # Shape (2, 3)\n",
    ")\n",
    "\n",
    "# Call the scan function\n",
    "final_carry, ys = lax.scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Final carry:\", final_carry)\n",
    "print(\"Outputs ys:\", ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
