{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `scan` and `scan_layers` in Llama 3\n",
    "\n",
    "Hugging Face usage follows https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    "\n",
    "To test scan, we need to use a custom modification of the transformer repo:\n",
    "https://github.com/tengyifei/transformers/tree/test-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_USE_SPMD=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_USE_SPMD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:250: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "  torch.random.manual_seed(42)\n",
    "  torch_xla.manual_seed(42)\n",
    "  np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.bos_token_id = 128000\n",
    "tokenizer.eos_token_id = 128001\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"][1].keys(), lm_datasets[\"validation\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"validation\"])  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 01:58:04.865781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732154284.875010 3460421 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732154284.877923 3460421 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,  # Model size\n",
    "    num_hidden_layers=28,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=2048,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=True,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=4000,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    tpu_num_cores=4,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 18:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.496500</td>\n",
       "      <td>5.361655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=5.915705322265625, metrics={'train_runtime': 1125.3595, 'train_samples_per_second': 227.483, 'train_steps_per_second': 3.554, 'total_flos': 8.3505639849984e+16, 'train_loss': 5.915705322265625, 'epoch': 0.1421110597932284})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 8139\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 69\n",
      "  Accumulator: 01m11s120ms701.270us\n",
      "  ValueRate: 066ms564.830us / second\n",
      "  Rate: 0.0636107 / second\n",
      "  Percentiles: 1%=016ms343.669us; 5%=016ms498.910us; 10%=017ms598.440us; 20%=018ms714.550us; 50%=019ms575.050us; 80%=020ms217.039us; 90%=058ms739.589us; 95%=06s942ms973.927us; 99%=39s543ms724.966us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 8208\n",
      "  Accumulator: 09m30s118ms628.649us\n",
      "  ValueRate: 473ms710.038us / second\n",
      "  Rate: 8.12761 / second\n",
      "  Percentiles: 1%=007ms678.049us; 5%=010ms987.030us; 10%=018ms994.459us; 20%=018ms259.150us; 50%=028ms071.749us; 80%=106ms075.548us; 90%=107ms778.038us; 95%=107ms076.217us; 99%=108ms515.718us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 20541\n",
      "  Accumulator: 02s691ms241.802us\n",
      "  ValueRate: 002ms588.671us / second\n",
      "  Rate: 19.3117 / second\n",
      "  Percentiles: 1%=055.190us; 5%=059.290us; 10%=061.810us; 20%=064.630us; 50%=075.140us; 80%=098.970us; 90%=105.770us; 95%=122.570us; 99%=162.270us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 66\n",
      "  Accumulator: 018ms886.540us\n",
      "  ValueRate: 019.127us / second\n",
      "  Rate: 0.070576 / second\n",
      "  Percentiles: 1%=059.640us; 5%=063.400us; 10%=064.390us; 20%=067.910us; 50%=092.470us; 80%=148.930us; 90%=192.310us; 95%=207.250us; 99%=007ms393.440us\n",
      "Counter: MarkStep\n",
      "  Value: 12213\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again, this time using scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=1024,  # Model size\n",
    "    num_hidden_layers=28,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=2048,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=False,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 2:26:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.496400</td>\n",
       "      <td>5.361534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=5.9156449584960935, metrics={'train_runtime': 8777.0858, 'train_samples_per_second': 29.167, 'train_steps_per_second': 0.456, 'total_flos': 8.3505639849984e+16, 'train_loss': 5.9156449584960935, 'epoch': 0.1421110597932284})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the for loop and scan model train to the same loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 8202\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 6\n",
      "  Accumulator: 34s537ms668.277us\n",
      "  ValueRate: 004ms825.021us / second\n",
      "  Rate: 0.000684329 / second\n",
      "  Percentiles: 1%=03s587ms571.823us; 5%=03s587ms571.823us; 10%=03s587ms571.823us; 20%=03s088ms129.233us; 50%=06s646ms065.223us; 80%=08s915ms679.426us; 90%=09s820ms070.436us; 95%=09s820ms070.436us; 99%=09s820ms070.436us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 8208\n",
      "  Accumulator: 12m46s869ms342.175us\n",
      "  ValueRate: 083ms579.890us / second\n",
      "  Rate: 1.07778 / second\n",
      "  Percentiles: 1%=021ms105.969us; 5%=022ms332.640us; 10%=023ms903.870us; 20%=024ms949.790us; 50%=027ms231.439us; 80%=149ms236.057us; 90%=150ms545.617us; 95%=150ms300.906us; 99%=151ms025.827us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 209355\n",
      "  Accumulator: 26s776ms849.127us\n",
      "  ValueRate: 004ms228.496us / second\n",
      "  Rate: 38.2862 / second\n",
      "  Percentiles: 1%=073.370us; 5%=081.360us; 10%=084.510us; 20%=088.070us; 50%=096.530us; 80%=119.130us; 90%=155.340us; 95%=170.470us; 99%=337.390us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 8125\n",
      "  Accumulator: 04m07s797ms214.109us\n",
      "  ValueRate: 028ms659.696us / second\n",
      "  Rate: 0.954316 / second\n",
      "  Percentiles: 1%=004ms761.190us; 5%=004ms834.070us; 10%=004ms863.670us; 20%=004ms914.240us; 50%=004ms156.510us; 80%=058ms968.319us; 90%=059ms783.319us; 95%=059ms335.119us; 99%=061ms790.698us\n",
      "Counter: MarkStep\n",
      "  Value: 12213\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the numerical correctness of `scan_layers`\n",
    "\n",
    "Under the same weights, and the same input tokens, both the for loop based\n",
    "implementation and `scan_layers` based implementation should produce the same\n",
    "output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "input_ids = torch.tensor(tokenized_datasets[\"train\"][3][\"input_ids\"]).unsqueeze(0).type(torch.LongTensor) # type:ignore\n",
    "attention_mask = torch.tensor(tokenized_datasets[\"train\"][3][\"attention_mask\"]).unsqueeze(0) # type:ignore\n",
    "input_ids = input_ids.to(torch_xla.device())\n",
    "attention_mask = attention_mask.to(torch_xla.device())\n",
    "torch_xla.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   5476,     73,  56761,    912,  86262,     88,   4298,    220,\n",
       "             18,    551,    366,   3200,     29,  66416,    320,  11002,    551,\n",
       "          50534,     99,  75267,  16144, 115687,  33710, 123283, 104612,     18,\n",
       "           1174,  13318,    662,  86262,     88,   4298,    315,    279,  71735,\n",
       "            220,     18,    883,   1174,  17037,  14183,    311,    439,  86262,\n",
       "             88,   4298,  66416,  14767,   4994,   6457,   1174,    374,    264,\n",
       "          39747,   3560,    571,     12,     31,   5737,   2835,   1847,   8040,\n",
       "            555,  80949,    323,   7972,   5168,   1854,    369,    279,  32365,\n",
       "          42585,    662,  45894,    304,   6186,    220,    679,     16,    304,\n",
       "           6457,   1174,    433,    374,    279,   4948,   1847,    304,    279,\n",
       "          86262,     88,   4298,   4101,    662,  21445,    287,    279,   1890,\n",
       "          37608,    315,  39747,    323,   1972,    571,     12,     31,    892,\n",
       "          27120,    439,   1202,  62540,   1174,    279,   3446,   8640,  15638,\n",
       "            311,    279,   1176,   1847,    323,  11263,    279,    330,   4076,\n",
       "           1752,    330]], device='xla:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-5.1699, -3.6749, -4.5099,  ..., -4.1783, -4.7814, -4.8963],\n",
       "          [-3.8756, -3.2109, -3.7001,  ..., -3.9365, -3.8906, -4.1175],\n",
       "          [-3.8278, -3.3501, -3.6548,  ..., -3.9579, -3.8159, -4.0969],\n",
       "          ...,\n",
       "          [-5.7814, -5.6866, -6.0840,  ..., -5.2086, -6.3461, -5.3524],\n",
       "          [-5.6320, -5.2927, -6.1013,  ..., -5.0950, -6.1226, -5.2884],\n",
       "          [-6.3101, -6.1876, -6.6119,  ..., -5.9928, -6.0297, -5.7701]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_token(logits):\n",
    "  return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  571,  301,  574, 1174, 1174,  320,  320,   17,  320,  220, 3200,\n",
       "           29,  366, 1174,  220,  551,  366,  366,  366,  366,  366,  366,  366,\n",
       "          883,  883,  883,  220,  366,  220,  571,  366,  883,  279,  220,  571,\n",
       "           17,  571,  374,  574, 3967,  311,  279,  264,  571, 1174, 1174, 1174,\n",
       "          662,  279,  662,  323,  264,  220,  571,  304,   12,   31,  709, 1174,\n",
       "         1847,  662,  555,  279,  662,  279,  662,   13,  662,  279,  220,  220,\n",
       "          662,  578,  304,  220,  220,  679,   15, 1174,  279, 1174,  279,  574,\n",
       "          264, 1176,  571,  304,  279,  220,  315,  315,  662,  315,  720,  220,\n",
       "          279, 1847, 1060, 1174,  279,  571,  279,  571,   12,   31,  892, 1174,\n",
       "         1174, 1664, 1176, 1174,  323, 1176,  374,  389,  311,  279, 1847,  892,\n",
       "          304,  279,  279, 1847,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pick_token(logits)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = @el was,, ( (2 ( unk> <,  : < < < < < < < ) ) )  <  @ < ) the  @2 @ is was known to the a @,,,. the. and a  @ in-@ up, game. by the. the... the  . The in  2010, the, the was a first @ in the  of of. of \\n  the game year, the @ the @-@ time,, well first, and first is on to the game time in the the game < \" \".'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-5.1699, -3.6749, -4.5099,  ..., -4.1783, -4.7814, -4.8963],\n",
       "          [-3.8756, -3.2109, -3.7001,  ..., -3.9365, -3.8906, -4.1175],\n",
       "          [-3.8278, -3.3501, -3.6548,  ..., -3.9579, -3.8159, -4.0969],\n",
       "          ...,\n",
       "          [-5.7814, -5.6866, -6.0840,  ..., -5.2086, -6.3461, -5.3524],\n",
       "          [-5.6320, -5.2927, -6.1013,  ..., -5.0950, -6.1226, -5.2884],\n",
       "          [-6.3101, -6.1876, -6.6119,  ..., -5.9928, -6.0297, -5.7701]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "for_loop_logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "for_loop_logits.shape, for_loop_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  571,  301,  574, 1174, 1174,  320,  320,   17,  320,  220, 3200,\n",
       "           29,  366, 1174,  220,  551,  366,  366,  366,  366,  366,  366,  366,\n",
       "          883,  883,  883,  220,  366,  220,  571,  366,  883,  279,  220,  571,\n",
       "           17,  571,  374,  574, 3967,  311,  279,  264,  571, 1174, 1174, 1174,\n",
       "          662,  279,  662,  323,  264,  220,  571,  304,   12,   31,  709, 1174,\n",
       "         1847,  662,  555,  279,  662,  279,  662,   13,  662,  279,  220,  220,\n",
       "          662,  578,  304,  220,  220,  679,   15, 1174,  279, 1174,  279,  574,\n",
       "          264, 1176,  571,  304,  279,  220,  315,  315,  662,  315,  720,  220,\n",
       "          279, 1847, 1060, 1174,  279,  571,  279,  571,   12,   31,  892, 1174,\n",
       "         1174, 1664, 1176, 1174,  323, 1176,  374,  389,  311,  279, 1847,  892,\n",
       "          304,  279,  279, 1847,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_tokens = pick_token(for_loop_logits)\n",
    "for_loop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = @el was,, ( (2 ( unk> <,  : < < < < < < < ) ) )  <  @ < ) the  @2 @ is was known to the a @,,,. the. and a  @ in-@ up, game. by the. the... the  . The in  2010, the, the was a first @ in the  of of. of \\n  the game year, the @ the @-@ time,, well first, and first is on to the game time in the the game < \" \".'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(for_loop_tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(logits, for_loop_logits, atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the gradients of scan\n",
    "\n",
    "After I run both scan and for loop versions of the model on the same input, their\n",
    "gradients should also be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n",
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "\n",
    "input_ids.requires_grad_(False)\n",
    "attention_mask.requires_grad_(False)\n",
    "labels = input_ids[:, :].clone().contiguous()\n",
    "\n",
    "# Run for loop model and collect the gradients\n",
    "reset_seed()\n",
    "for_loop_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  for_loop_grads.append((name, param.grad.clone().detach()))\n",
    "\n",
    "# Run scan model and collect the gradients\n",
    "reset_seed()\n",
    "scan_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  scan_grads.append((name, param.grad.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the gradients\n",
    "assert len(for_loop_grads) == len(scan_grads)\n",
    "assert len(for_loop_grads) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: model.embed_tokens.weight\n",
      "Pass: model.layers.0.self_attn.q_proj.weight\n",
      "Pass: model.layers.0.self_attn.k_proj.weight\n",
      "Pass: model.layers.0.self_attn.v_proj.weight\n",
      "Pass: model.layers.0.self_attn.o_proj.weight\n",
      "Pass: model.layers.0.mlp.gate_proj.weight\n",
      "Pass: model.layers.0.mlp.up_proj.weight\n",
      "Pass: model.layers.0.mlp.down_proj.weight\n",
      "Pass: model.layers.0.input_layernorm.weight\n",
      "Pass: model.layers.0.post_attention_layernorm.weight\n",
      "Pass: model.layers.1.self_attn.q_proj.weight\n",
      "Pass: model.layers.1.self_attn.k_proj.weight\n",
      "Pass: model.layers.1.self_attn.v_proj.weight\n",
      "Pass: model.layers.1.self_attn.o_proj.weight\n",
      "Pass: model.layers.1.mlp.gate_proj.weight\n",
      "Pass: model.layers.1.mlp.up_proj.weight\n",
      "Pass: model.layers.1.mlp.down_proj.weight\n",
      "Pass: model.layers.1.input_layernorm.weight\n",
      "Pass: model.layers.1.post_attention_layernorm.weight\n",
      "Pass: model.layers.2.self_attn.q_proj.weight\n",
      "Pass: model.layers.2.self_attn.k_proj.weight\n",
      "Pass: model.layers.2.self_attn.v_proj.weight\n",
      "Pass: model.layers.2.self_attn.o_proj.weight\n",
      "Pass: model.layers.2.mlp.gate_proj.weight\n",
      "Pass: model.layers.2.mlp.up_proj.weight\n",
      "Pass: model.layers.2.mlp.down_proj.weight\n",
      "Pass: model.layers.2.input_layernorm.weight\n",
      "Pass: model.layers.2.post_attention_layernorm.weight\n",
      "Pass: model.layers.3.self_attn.q_proj.weight\n",
      "Pass: model.layers.3.self_attn.k_proj.weight\n",
      "Pass: model.layers.3.self_attn.v_proj.weight\n",
      "Pass: model.layers.3.self_attn.o_proj.weight\n",
      "Pass: model.layers.3.mlp.gate_proj.weight\n",
      "Pass: model.layers.3.mlp.up_proj.weight\n",
      "Pass: model.layers.3.mlp.down_proj.weight\n",
      "Pass: model.layers.3.input_layernorm.weight\n",
      "Pass: model.layers.3.post_attention_layernorm.weight\n",
      "Pass: model.layers.4.self_attn.q_proj.weight\n",
      "Pass: model.layers.4.self_attn.k_proj.weight\n",
      "Pass: model.layers.4.self_attn.v_proj.weight\n",
      "Pass: model.layers.4.self_attn.o_proj.weight\n",
      "Pass: model.layers.4.mlp.gate_proj.weight\n",
      "Pass: model.layers.4.mlp.up_proj.weight\n",
      "Pass: model.layers.4.mlp.down_proj.weight\n",
      "Pass: model.layers.4.input_layernorm.weight\n",
      "Pass: model.layers.4.post_attention_layernorm.weight\n",
      "Pass: model.layers.5.self_attn.q_proj.weight\n",
      "Pass: model.layers.5.self_attn.k_proj.weight\n",
      "Pass: model.layers.5.self_attn.v_proj.weight\n",
      "Pass: model.layers.5.self_attn.o_proj.weight\n",
      "Pass: model.layers.5.mlp.gate_proj.weight\n",
      "Pass: model.layers.5.mlp.up_proj.weight\n",
      "Pass: model.layers.5.mlp.down_proj.weight\n",
      "Pass: model.layers.5.input_layernorm.weight\n",
      "Pass: model.layers.5.post_attention_layernorm.weight\n",
      "Pass: model.layers.6.self_attn.q_proj.weight\n",
      "Pass: model.layers.6.self_attn.k_proj.weight\n",
      "Pass: model.layers.6.self_attn.v_proj.weight\n",
      "Pass: model.layers.6.self_attn.o_proj.weight\n",
      "Pass: model.layers.6.mlp.gate_proj.weight\n",
      "Pass: model.layers.6.mlp.up_proj.weight\n",
      "Pass: model.layers.6.mlp.down_proj.weight\n",
      "Pass: model.layers.6.input_layernorm.weight\n",
      "Pass: model.layers.6.post_attention_layernorm.weight\n",
      "Pass: model.layers.7.self_attn.q_proj.weight\n",
      "Pass: model.layers.7.self_attn.k_proj.weight\n",
      "Pass: model.layers.7.self_attn.v_proj.weight\n",
      "Pass: model.layers.7.self_attn.o_proj.weight\n",
      "Pass: model.layers.7.mlp.gate_proj.weight\n",
      "Pass: model.layers.7.mlp.up_proj.weight\n",
      "Pass: model.layers.7.mlp.down_proj.weight\n",
      "Pass: model.layers.7.input_layernorm.weight\n",
      "Pass: model.layers.7.post_attention_layernorm.weight\n",
      "Pass: model.layers.8.self_attn.q_proj.weight\n",
      "Pass: model.layers.8.self_attn.k_proj.weight\n",
      "Pass: model.layers.8.self_attn.v_proj.weight\n",
      "Pass: model.layers.8.self_attn.o_proj.weight\n",
      "Pass: model.layers.8.mlp.gate_proj.weight\n",
      "Pass: model.layers.8.mlp.up_proj.weight\n",
      "Pass: model.layers.8.mlp.down_proj.weight\n",
      "Pass: model.layers.8.input_layernorm.weight\n",
      "Pass: model.layers.8.post_attention_layernorm.weight\n",
      "Pass: model.layers.9.self_attn.q_proj.weight\n",
      "Pass: model.layers.9.self_attn.k_proj.weight\n",
      "Pass: model.layers.9.self_attn.v_proj.weight\n",
      "Pass: model.layers.9.self_attn.o_proj.weight\n",
      "Pass: model.layers.9.mlp.gate_proj.weight\n",
      "Pass: model.layers.9.mlp.up_proj.weight\n",
      "Pass: model.layers.9.mlp.down_proj.weight\n",
      "Pass: model.layers.9.input_layernorm.weight\n",
      "Pass: model.layers.9.post_attention_layernorm.weight\n",
      "Pass: model.layers.10.self_attn.q_proj.weight\n",
      "Pass: model.layers.10.self_attn.k_proj.weight\n",
      "Pass: model.layers.10.self_attn.v_proj.weight\n",
      "Pass: model.layers.10.self_attn.o_proj.weight\n",
      "Pass: model.layers.10.mlp.gate_proj.weight\n",
      "Pass: model.layers.10.mlp.up_proj.weight\n",
      "Pass: model.layers.10.mlp.down_proj.weight\n",
      "Pass: model.layers.10.input_layernorm.weight\n",
      "Pass: model.layers.10.post_attention_layernorm.weight\n",
      "Pass: model.layers.11.self_attn.q_proj.weight\n",
      "Pass: model.layers.11.self_attn.k_proj.weight\n",
      "Pass: model.layers.11.self_attn.v_proj.weight\n",
      "Pass: model.layers.11.self_attn.o_proj.weight\n",
      "Pass: model.layers.11.mlp.gate_proj.weight\n",
      "Pass: model.layers.11.mlp.up_proj.weight\n",
      "Pass: model.layers.11.mlp.down_proj.weight\n",
      "Pass: model.layers.11.input_layernorm.weight\n",
      "Pass: model.layers.11.post_attention_layernorm.weight\n",
      "Pass: model.layers.12.self_attn.q_proj.weight\n",
      "Pass: model.layers.12.self_attn.k_proj.weight\n",
      "Pass: model.layers.12.self_attn.v_proj.weight\n",
      "Pass: model.layers.12.self_attn.o_proj.weight\n",
      "Pass: model.layers.12.mlp.gate_proj.weight\n",
      "Pass: model.layers.12.mlp.up_proj.weight\n",
      "Pass: model.layers.12.mlp.down_proj.weight\n",
      "Pass: model.layers.12.input_layernorm.weight\n",
      "Pass: model.layers.12.post_attention_layernorm.weight\n",
      "Pass: model.layers.13.self_attn.q_proj.weight\n",
      "Pass: model.layers.13.self_attn.k_proj.weight\n",
      "Pass: model.layers.13.self_attn.v_proj.weight\n",
      "Pass: model.layers.13.self_attn.o_proj.weight\n",
      "Pass: model.layers.13.mlp.gate_proj.weight\n",
      "Pass: model.layers.13.mlp.up_proj.weight\n",
      "Pass: model.layers.13.mlp.down_proj.weight\n",
      "Pass: model.layers.13.input_layernorm.weight\n",
      "Pass: model.layers.13.post_attention_layernorm.weight\n",
      "Pass: model.layers.14.self_attn.q_proj.weight\n",
      "Pass: model.layers.14.self_attn.k_proj.weight\n",
      "Pass: model.layers.14.self_attn.v_proj.weight\n",
      "Pass: model.layers.14.self_attn.o_proj.weight\n",
      "Pass: model.layers.14.mlp.gate_proj.weight\n",
      "Pass: model.layers.14.mlp.up_proj.weight\n",
      "Pass: model.layers.14.mlp.down_proj.weight\n",
      "Pass: model.layers.14.input_layernorm.weight\n",
      "Pass: model.layers.14.post_attention_layernorm.weight\n",
      "Pass: model.layers.15.self_attn.q_proj.weight\n",
      "Pass: model.layers.15.self_attn.k_proj.weight\n",
      "Pass: model.layers.15.self_attn.v_proj.weight\n",
      "Pass: model.layers.15.self_attn.o_proj.weight\n",
      "Pass: model.layers.15.mlp.gate_proj.weight\n",
      "Pass: model.layers.15.mlp.up_proj.weight\n",
      "Pass: model.layers.15.mlp.down_proj.weight\n",
      "Pass: model.layers.15.input_layernorm.weight\n",
      "Pass: model.layers.15.post_attention_layernorm.weight\n",
      "Pass: model.layers.16.self_attn.q_proj.weight\n",
      "Pass: model.layers.16.self_attn.k_proj.weight\n",
      "Pass: model.layers.16.self_attn.v_proj.weight\n",
      "Pass: model.layers.16.self_attn.o_proj.weight\n",
      "Pass: model.layers.16.mlp.gate_proj.weight\n",
      "Pass: model.layers.16.mlp.up_proj.weight\n",
      "Pass: model.layers.16.mlp.down_proj.weight\n",
      "Pass: model.layers.16.input_layernorm.weight\n",
      "Pass: model.layers.16.post_attention_layernorm.weight\n",
      "Pass: model.layers.17.self_attn.q_proj.weight\n",
      "Pass: model.layers.17.self_attn.k_proj.weight\n",
      "Pass: model.layers.17.self_attn.v_proj.weight\n",
      "Pass: model.layers.17.self_attn.o_proj.weight\n",
      "Pass: model.layers.17.mlp.gate_proj.weight\n",
      "Pass: model.layers.17.mlp.up_proj.weight\n",
      "Pass: model.layers.17.mlp.down_proj.weight\n",
      "Pass: model.layers.17.input_layernorm.weight\n",
      "Pass: model.layers.17.post_attention_layernorm.weight\n",
      "Pass: model.layers.18.self_attn.q_proj.weight\n",
      "Pass: model.layers.18.self_attn.k_proj.weight\n",
      "Pass: model.layers.18.self_attn.v_proj.weight\n",
      "Pass: model.layers.18.self_attn.o_proj.weight\n",
      "Pass: model.layers.18.mlp.gate_proj.weight\n",
      "Pass: model.layers.18.mlp.up_proj.weight\n",
      "Pass: model.layers.18.mlp.down_proj.weight\n",
      "Pass: model.layers.18.input_layernorm.weight\n",
      "Pass: model.layers.18.post_attention_layernorm.weight\n",
      "Pass: model.layers.19.self_attn.q_proj.weight\n",
      "Pass: model.layers.19.self_attn.k_proj.weight\n",
      "Pass: model.layers.19.self_attn.v_proj.weight\n",
      "Pass: model.layers.19.self_attn.o_proj.weight\n",
      "Pass: model.layers.19.mlp.gate_proj.weight\n",
      "Pass: model.layers.19.mlp.up_proj.weight\n",
      "Pass: model.layers.19.mlp.down_proj.weight\n",
      "Pass: model.layers.19.input_layernorm.weight\n",
      "Pass: model.layers.19.post_attention_layernorm.weight\n",
      "Pass: model.layers.20.self_attn.q_proj.weight\n",
      "Pass: model.layers.20.self_attn.k_proj.weight\n",
      "Pass: model.layers.20.self_attn.v_proj.weight\n",
      "Pass: model.layers.20.self_attn.o_proj.weight\n",
      "Pass: model.layers.20.mlp.gate_proj.weight\n",
      "Pass: model.layers.20.mlp.up_proj.weight\n",
      "Pass: model.layers.20.mlp.down_proj.weight\n",
      "Pass: model.layers.20.input_layernorm.weight\n",
      "Pass: model.layers.20.post_attention_layernorm.weight\n",
      "Pass: model.layers.21.self_attn.q_proj.weight\n",
      "Pass: model.layers.21.self_attn.k_proj.weight\n",
      "Pass: model.layers.21.self_attn.v_proj.weight\n",
      "Pass: model.layers.21.self_attn.o_proj.weight\n",
      "Pass: model.layers.21.mlp.gate_proj.weight\n",
      "Pass: model.layers.21.mlp.up_proj.weight\n",
      "Pass: model.layers.21.mlp.down_proj.weight\n",
      "Pass: model.layers.21.input_layernorm.weight\n",
      "Pass: model.layers.21.post_attention_layernorm.weight\n",
      "Pass: model.layers.22.self_attn.q_proj.weight\n",
      "Pass: model.layers.22.self_attn.k_proj.weight\n",
      "Pass: model.layers.22.self_attn.v_proj.weight\n",
      "Pass: model.layers.22.self_attn.o_proj.weight\n",
      "Pass: model.layers.22.mlp.gate_proj.weight\n",
      "Pass: model.layers.22.mlp.up_proj.weight\n",
      "Pass: model.layers.22.mlp.down_proj.weight\n",
      "Pass: model.layers.22.input_layernorm.weight\n",
      "Pass: model.layers.22.post_attention_layernorm.weight\n",
      "Pass: model.layers.23.self_attn.q_proj.weight\n",
      "Pass: model.layers.23.self_attn.k_proj.weight\n",
      "Pass: model.layers.23.self_attn.v_proj.weight\n",
      "Pass: model.layers.23.self_attn.o_proj.weight\n",
      "Pass: model.layers.23.mlp.gate_proj.weight\n",
      "Pass: model.layers.23.mlp.up_proj.weight\n",
      "Pass: model.layers.23.mlp.down_proj.weight\n",
      "Pass: model.layers.23.input_layernorm.weight\n",
      "Pass: model.layers.23.post_attention_layernorm.weight\n",
      "Pass: model.layers.24.self_attn.q_proj.weight\n",
      "Pass: model.layers.24.self_attn.k_proj.weight\n",
      "Pass: model.layers.24.self_attn.v_proj.weight\n",
      "Pass: model.layers.24.self_attn.o_proj.weight\n",
      "Pass: model.layers.24.mlp.gate_proj.weight\n",
      "Pass: model.layers.24.mlp.up_proj.weight\n",
      "Pass: model.layers.24.mlp.down_proj.weight\n",
      "Pass: model.layers.24.input_layernorm.weight\n",
      "Pass: model.layers.24.post_attention_layernorm.weight\n",
      "Pass: model.layers.25.self_attn.q_proj.weight\n",
      "Pass: model.layers.25.self_attn.k_proj.weight\n",
      "Pass: model.layers.25.self_attn.v_proj.weight\n",
      "Pass: model.layers.25.self_attn.o_proj.weight\n",
      "Pass: model.layers.25.mlp.gate_proj.weight\n",
      "Pass: model.layers.25.mlp.up_proj.weight\n",
      "Pass: model.layers.25.mlp.down_proj.weight\n",
      "Pass: model.layers.25.input_layernorm.weight\n",
      "Pass: model.layers.25.post_attention_layernorm.weight\n",
      "Pass: model.layers.26.self_attn.q_proj.weight\n",
      "Pass: model.layers.26.self_attn.k_proj.weight\n",
      "Pass: model.layers.26.self_attn.v_proj.weight\n",
      "Pass: model.layers.26.self_attn.o_proj.weight\n",
      "Pass: model.layers.26.mlp.gate_proj.weight\n",
      "Pass: model.layers.26.mlp.up_proj.weight\n",
      "Pass: model.layers.26.mlp.down_proj.weight\n",
      "Pass: model.layers.26.input_layernorm.weight\n",
      "Pass: model.layers.26.post_attention_layernorm.weight\n",
      "Pass: model.layers.27.self_attn.q_proj.weight\n",
      "Pass: model.layers.27.self_attn.k_proj.weight\n",
      "Pass: model.layers.27.self_attn.v_proj.weight\n",
      "Pass: model.layers.27.self_attn.o_proj.weight\n",
      "Pass: model.layers.27.mlp.gate_proj.weight\n",
      "Pass: model.layers.27.mlp.up_proj.weight\n",
      "Pass: model.layers.27.mlp.down_proj.weight\n",
      "Pass: model.layers.27.input_layernorm.weight\n",
      "Pass: model.layers.27.post_attention_layernorm.weight\n",
      "Pass: model.norm.weight\n",
      "Pass: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for ((for_loop_name, for_loop_grad), (scan_name, scan_grad)) in zip(for_loop_grads, scan_grads):\n",
    "  assert for_loop_name == scan_name\n",
    "  assert torch.allclose(for_loop_grad, scan_grad, atol=1e-5, rtol=1e-5), f\"{for_loop_name} mismatch by: {torch.max(torch.abs(for_loop_grad - scan_grad))}\"\n",
    "  print(f\"Pass: {for_loop_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[ 6.7186e-05,  1.6983e-04,  4.7841e-04,  ..., -1.6211e-04,\n",
       "            3.0965e-04,  1.3230e-03],\n",
       "          [ 3.8496e-03, -2.2651e-03,  9.5748e-05,  ...,  6.6169e-04,\n",
       "            2.5951e-03, -4.4781e-03],\n",
       "          [ 1.4842e-03, -3.7740e-04, -1.4062e-03,  ...,  9.7732e-04,\n",
       "            2.5738e-03, -2.3177e-03],\n",
       "          ...,\n",
       "          [ 3.9044e-04,  8.9442e-04, -3.1140e-04,  ..., -2.0575e-03,\n",
       "           -2.3838e-03,  2.9901e-04],\n",
       "          [-1.0933e-04,  1.2664e-03, -1.2857e-03,  ..., -1.6495e-03,\n",
       "           -8.3783e-04,  2.4912e-03],\n",
       "          [ 6.9341e-04, -1.1699e-03,  5.8735e-04,  ..., -6.4733e-04,\n",
       "            2.2107e-03,  3.0538e-05]], device='xla:0')),\n",
       " tensor(0.0275, device='xla:0'),\n",
       " tensor(-0.0279, device='xla:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_grads[3], torch.max(for_loop_grads[3][1]), torch.min(for_loop_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[ 6.7186e-05,  1.6983e-04,  4.7841e-04,  ..., -1.6211e-04,\n",
       "            3.0965e-04,  1.3230e-03],\n",
       "          [ 3.8496e-03, -2.2651e-03,  9.5748e-05,  ...,  6.6169e-04,\n",
       "            2.5951e-03, -4.4781e-03],\n",
       "          [ 1.4842e-03, -3.7740e-04, -1.4062e-03,  ...,  9.7732e-04,\n",
       "            2.5738e-03, -2.3177e-03],\n",
       "          ...,\n",
       "          [ 3.9044e-04,  8.9442e-04, -3.1140e-04,  ..., -2.0575e-03,\n",
       "           -2.3838e-03,  2.9901e-04],\n",
       "          [-1.0933e-04,  1.2664e-03, -1.2857e-03,  ..., -1.6495e-03,\n",
       "           -8.3783e-04,  2.4912e-03],\n",
       "          [ 6.9341e-04, -1.1699e-03,  5.8735e-04,  ..., -6.4733e-04,\n",
       "            2.2107e-03,  3.0538e-05]], device='xla:0')),\n",
       " tensor(0.0275, device='xla:0'),\n",
       " tensor(-0.0279, device='xla:0'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_grads[3], torch.max(scan_grads[3][1]), torch.min(scan_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
