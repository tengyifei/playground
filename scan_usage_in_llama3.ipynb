{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `scan` and `apply_layers` in Llama 3\n",
    "\n",
    "Hugging Face usage follows https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    "\n",
    "To test scan, we need to use a custom modification of the transformer repo:\n",
    "https://github.com/tengyifei/transformers/commit/646a575928d8514f220384c29d27c8b956826a91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_USE_SPMD=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_USE_SPMD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.bos_token_id = 128000\n",
    "tokenizer.eos_token_id = 128001\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"][1].keys(), lm_datasets[\"validation\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"validation\"])  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=True,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=48,\n",
    "    per_device_eval_batch_size=48,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=250,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    tpu_num_cores=4,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.322300</td>\n",
       "      <td>9.280924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/core/xla_model.py:1457: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  xldata.append(torch.load(xbio))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=9.857639526367187, metrics={'train_runtime': 494.3655, 'train_samples_per_second': 24.274, 'train_steps_per_second': 0.506, 'total_flos': 1377380597760000.0, 'train_loss': 9.857639526367187, 'epoch': 0.006661515094993205})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 418\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 88\n",
      "  Accumulator: 04m08s065ms120.553us\n",
      "  ValueRate: 585ms226.465us / second\n",
      "  Rate: 0.207606 / second\n",
      "  Percentiles: 1%=025ms831.227us; 5%=025ms416.577us; 10%=026ms513.888us; 20%=026ms870.888us; 50%=027ms288.307us; 80%=029ms990.057us; 90%=046ms746.155us; 95%=09s818ms285.753us; 99%=01m21s677ms248.163us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 506\n",
      "  Accumulator: 21s165ms334.633us\n",
      "  ValueRate: 050ms997.647us / second\n",
      "  Rate: 1.19529 / second\n",
      "  Percentiles: 1%=970.529us; 5%=003ms791.809us; 10%=003ms069.959us; 20%=016ms755.449us; 50%=046ms001.205us; 80%=067ms610.363us; 90%=067ms895.353us; 95%=067ms195.213us; 99%=078ms117.212us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 1879\n",
      "  Accumulator: 165ms703.039us\n",
      "  ValueRate: 359.445us / second\n",
      "  Rate: 4.22028 / second\n",
      "  Percentiles: 1%=044.170us; 5%=048.610us; 10%=051.500us; 20%=056.890us; 50%=074.190us; 80%=111.590us; 90%=143.580us; 95%=152.080us; 99%=184.720us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 42\n",
      "  Accumulator: 188ms632.703us\n",
      "  ValueRate: 634.928us / second\n",
      "  Rate: 0.142123 / second\n",
      "  Percentiles: 1%=214.150us; 5%=252.670us; 10%=291.160us; 20%=319.130us; 50%=385.500us; 80%=990.820us; 90%=004ms740.620us; 95%=043ms998.646us; 99%=046ms569.645us\n",
      "Counter: MarkStep\n",
      "  Value: 760\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again, this time using scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=False,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 06:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.322000</td>\n",
       "      <td>9.292122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=9.864378784179687, metrics={'train_runtime': 389.1826, 'train_samples_per_second': 30.834, 'train_steps_per_second': 0.642, 'total_flos': 1377380597760000.0, 'train_loss': 9.864378784179687, 'epoch': 0.006661515094993205})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 501\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 5\n",
      "  Accumulator: 02m51s682ms544.317us\n",
      "  ValueRate: 308ms604.997us / second\n",
      "  Rate: 0.0138959 / second\n",
      "  Percentiles: 1%=06s818ms504.432us; 5%=06s818ms504.432us; 10%=06s818ms504.432us; 20%=07s942ms549.106us; 50%=26s231ms246.452us; 80%=37s972ms181.958us; 90%=37s972ms181.958us; 95%=37s972ms181.958us; 99%=37s972ms181.958us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 506\n",
      "  Accumulator: 30s212ms458.665us\n",
      "  ValueRate: 084ms995.421us / second\n",
      "  Rate: 1.40676 / second\n",
      "  Percentiles: 1%=976.630us; 5%=013ms494.628us; 10%=014ms839.659us; 20%=017ms191.778us; 50%=076ms731.302us; 80%=101ms265.730us; 90%=102ms558.240us; 95%=102ms812.550us; 99%=107ms823.029us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 9316\n",
      "  Accumulator: 856ms251.618us\n",
      "  ValueRate: 011ms750.635us / second\n",
      "  Rate: 110.407 / second\n",
      "  Percentiles: 1%=049.060us; 5%=055.760us; 10%=068.300us; 20%=079.510us; 50%=089.610us; 80%=114.160us; 90%=149.000us; 95%=164.300us; 99%=180.100us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 1200\n",
      "  Accumulator: 07s249ms384.650us\n",
      "  ValueRate: 022ms166.232us / second\n",
      "  Rate: 3.69053 / second\n",
      "  Percentiles: 1%=322.140us; 5%=004ms710.999us; 10%=004ms993.190us; 20%=004ms271.779us; 50%=005ms059.810us; 80%=007ms485.639us; 90%=008ms060.620us; 95%=009ms592.679us; 99%=011ms454.948us\n",
      "Counter: MarkStep\n",
      "  Value: 760\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the numerical correctness of `apply_layers`\n",
    "\n",
    "Under the same weights, and the same input tokens, both the for loop based\n",
    "implementation and `apply_layers` based implementation should produce the same\n",
    "output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "input_ids = torch.tensor(tokenized_datasets[\"train\"][3][\"input_ids\"]).unsqueeze(0).type(torch.LongTensor) # type:ignore\n",
    "attention_mask = torch.tensor(tokenized_datasets[\"train\"][3][\"attention_mask\"]).unsqueeze(0) # type:ignore\n",
    "input_ids = input_ids.to(torch_xla.device())\n",
    "attention_mask = attention_mask.to(torch_xla.device())\n",
    "torch_xla.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   5476,     73,  56761,    912,  86262,     88,   4298,    220,\n",
       "             18,    551,    366,   3200,     29,  66416,    320,  11002,    551,\n",
       "          50534,     99,  75267,  16144, 115687,  33710, 123283, 104612,     18,\n",
       "           1174,  13318,    662,  86262,     88,   4298,    315,    279,  71735,\n",
       "            220,     18,    883,   1174,  17037,  14183,    311,    439,  86262,\n",
       "             88,   4298,  66416,  14767,   4994,   6457,   1174,    374,    264,\n",
       "          39747,   3560,    571,     12,     31,   5737,   2835,   1847,   8040,\n",
       "            555,  80949,    323,   7972,   5168,   1854,    369,    279,  32365,\n",
       "          42585,    662,  45894,    304,   6186,    220,    679,     16,    304,\n",
       "           6457,   1174,    433,    374,    279,   4948,   1847,    304,    279,\n",
       "          86262,     88,   4298,   4101,    662,  21445,    287,    279,   1890,\n",
       "          37608,    315,  39747,    323,   1972,    571,     12,     31,    892,\n",
       "          27120,    439,   1202,  62540,   1174,    279,   3446,   8640,  15638,\n",
       "            311,    279,   1176,   1847,    323,  11263,    279,    330,   4076,\n",
       "           1752,    330]], device='xla:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-0.5572, -0.6444, -0.1399,  ..., -0.1027, -1.0320, -1.4400],\n",
       "          [-0.9679, -1.3558, -0.9182,  ..., -0.6744, -2.1547, -1.2086],\n",
       "          [-0.9505, -1.4370, -0.9905,  ..., -0.7459, -2.1894, -1.0792],\n",
       "          ...,\n",
       "          [-0.9731, -1.5157, -1.0472,  ..., -0.8122, -2.0453, -0.9087],\n",
       "          [-0.9745, -1.5194, -1.0236,  ..., -0.8299, -2.0414, -0.9306],\n",
       "          [-0.9767, -1.5013, -1.0378,  ..., -0.8184, -2.0358, -0.9563]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_token(logits):\n",
    "  return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174]], device='xla:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pick_token(logits)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' =,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-0.5572, -0.6444, -0.1399,  ..., -0.1027, -1.0320, -1.4400],\n",
       "          [-0.9679, -1.3558, -0.9182,  ..., -0.6744, -2.1547, -1.2086],\n",
       "          [-0.9505, -1.4370, -0.9905,  ..., -0.7459, -2.1894, -1.0792],\n",
       "          ...,\n",
       "          [-0.9729, -1.5153, -1.0475,  ..., -0.8124, -2.0456, -0.9089],\n",
       "          [-0.9745, -1.5185, -1.0242,  ..., -0.8295, -2.0408, -0.9304],\n",
       "          [-0.9772, -1.5007, -1.0376,  ..., -0.8187, -2.0361, -0.9560]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "for_loop_logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "for_loop_logits.shape, for_loop_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174,\n",
       "         1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174]], device='xla:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_tokens = pick_token(for_loop_logits)\n",
    "for_loop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' =,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(for_loop_tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be accurate to within 1%\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shouldn't be completely the same\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-6, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the gradients of scan\n",
    "\n",
    "After I run both scan and for loop versions of the model on the same input, their\n",
    "gradients should also be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n",
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "\n",
    "input_ids.requires_grad_(False)\n",
    "attention_mask.requires_grad_(False)\n",
    "\n",
    "# Run for loop model and collect the gradients\n",
    "torch.manual_seed(42)\n",
    "for_loop_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  for_loop_logits = model(input_ids, attention_mask).logits  # type: ignore\n",
    "torch.sum(for_loop_logits).backward()\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  # print(name, param.shape, param.grad.shape if param.grad is not None else None)\n",
    "  assert param.grad is not None\n",
    "  for_loop_grads.append((name, param.grad.clone().detach()))\n",
    "\n",
    "# Run scan model and collect the gradients\n",
    "torch.manual_seed(42)\n",
    "scan_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  scan_logits = model(input_ids, attention_mask).logits  # type: ignore\n",
    "torch.sum(scan_logits).backward()\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  # print(name, param.shape, param.grad.shape if param.grad is not None else None)\n",
    "  assert param.grad is not None\n",
    "  scan_grads.append((name, param.grad.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the gradients\n",
    "assert len(for_loop_grads) == len(scan_grads)\n",
    "assert len(for_loop_grads) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: model.embed_tokens.weight\n",
      "Pass: model.layers.0.self_attn.q_proj.weight\n",
      "Pass: model.layers.0.self_attn.k_proj.weight\n",
      "Pass: model.layers.0.self_attn.v_proj.weight\n",
      "Pass: model.layers.0.self_attn.o_proj.weight\n",
      "Pass: model.layers.0.mlp.gate_proj.weight\n",
      "Pass: model.layers.0.mlp.up_proj.weight\n",
      "Pass: model.layers.0.mlp.down_proj.weight\n",
      "Pass: model.layers.0.input_layernorm.weight\n",
      "Pass: model.layers.0.post_attention_layernorm.weight\n",
      "Pass: model.layers.1.self_attn.q_proj.weight\n",
      "Pass: model.layers.1.self_attn.k_proj.weight\n",
      "Pass: model.layers.1.self_attn.v_proj.weight\n",
      "Pass: model.layers.1.self_attn.o_proj.weight\n",
      "Pass: model.layers.1.mlp.gate_proj.weight\n",
      "Pass: model.layers.1.mlp.up_proj.weight\n",
      "Pass: model.layers.1.mlp.down_proj.weight\n",
      "Pass: model.layers.1.input_layernorm.weight\n",
      "Pass: model.layers.1.post_attention_layernorm.weight\n",
      "Pass: model.layers.2.self_attn.q_proj.weight\n",
      "Pass: model.layers.2.self_attn.k_proj.weight\n",
      "Pass: model.layers.2.self_attn.v_proj.weight\n",
      "Pass: model.layers.2.self_attn.o_proj.weight\n",
      "Pass: model.layers.2.mlp.gate_proj.weight\n",
      "Pass: model.layers.2.mlp.up_proj.weight\n",
      "Pass: model.layers.2.mlp.down_proj.weight\n",
      "Pass: model.layers.2.input_layernorm.weight\n",
      "Pass: model.layers.2.post_attention_layernorm.weight\n",
      "Pass: model.layers.3.self_attn.q_proj.weight\n",
      "Pass: model.layers.3.self_attn.k_proj.weight\n",
      "Pass: model.layers.3.self_attn.v_proj.weight\n",
      "Pass: model.layers.3.self_attn.o_proj.weight\n",
      "Pass: model.layers.3.mlp.gate_proj.weight\n",
      "Pass: model.layers.3.mlp.up_proj.weight\n",
      "Pass: model.layers.3.mlp.down_proj.weight\n",
      "Pass: model.layers.3.input_layernorm.weight\n",
      "Pass: model.layers.3.post_attention_layernorm.weight\n",
      "Pass: model.layers.4.self_attn.q_proj.weight\n",
      "Pass: model.layers.4.self_attn.k_proj.weight\n",
      "Pass: model.layers.4.self_attn.v_proj.weight\n",
      "Pass: model.layers.4.self_attn.o_proj.weight\n",
      "Pass: model.layers.4.mlp.gate_proj.weight\n",
      "Pass: model.layers.4.mlp.up_proj.weight\n",
      "Pass: model.layers.4.mlp.down_proj.weight\n",
      "Pass: model.layers.4.input_layernorm.weight\n",
      "Pass: model.layers.4.post_attention_layernorm.weight\n",
      "Pass: model.layers.5.self_attn.q_proj.weight\n",
      "Pass: model.layers.5.self_attn.k_proj.weight\n",
      "Pass: model.layers.5.self_attn.v_proj.weight\n",
      "Pass: model.layers.5.self_attn.o_proj.weight\n",
      "Pass: model.layers.5.mlp.gate_proj.weight\n",
      "Pass: model.layers.5.mlp.up_proj.weight\n",
      "Pass: model.layers.5.mlp.down_proj.weight\n",
      "Pass: model.layers.5.input_layernorm.weight\n",
      "Pass: model.layers.5.post_attention_layernorm.weight\n",
      "Pass: model.layers.6.self_attn.q_proj.weight\n",
      "Pass: model.layers.6.self_attn.k_proj.weight\n",
      "Pass: model.layers.6.self_attn.v_proj.weight\n",
      "Pass: model.layers.6.self_attn.o_proj.weight\n",
      "Pass: model.layers.6.mlp.gate_proj.weight\n",
      "Pass: model.layers.6.mlp.up_proj.weight\n",
      "Pass: model.layers.6.mlp.down_proj.weight\n",
      "Pass: model.layers.6.input_layernorm.weight\n",
      "Pass: model.layers.6.post_attention_layernorm.weight\n",
      "Pass: model.layers.7.self_attn.q_proj.weight\n",
      "Pass: model.layers.7.self_attn.k_proj.weight\n",
      "Pass: model.layers.7.self_attn.v_proj.weight\n",
      "Pass: model.layers.7.self_attn.o_proj.weight\n",
      "Pass: model.layers.7.mlp.gate_proj.weight\n",
      "Pass: model.layers.7.mlp.up_proj.weight\n",
      "Pass: model.layers.7.mlp.down_proj.weight\n",
      "Pass: model.layers.7.input_layernorm.weight\n",
      "Pass: model.layers.7.post_attention_layernorm.weight\n",
      "Pass: model.layers.8.self_attn.q_proj.weight\n",
      "Pass: model.layers.8.self_attn.k_proj.weight\n",
      "Pass: model.layers.8.self_attn.v_proj.weight\n",
      "Pass: model.layers.8.self_attn.o_proj.weight\n",
      "Pass: model.layers.8.mlp.gate_proj.weight\n",
      "Pass: model.layers.8.mlp.up_proj.weight\n",
      "Pass: model.layers.8.mlp.down_proj.weight\n",
      "Pass: model.layers.8.input_layernorm.weight\n",
      "Pass: model.layers.8.post_attention_layernorm.weight\n",
      "Pass: model.layers.9.self_attn.q_proj.weight\n",
      "Pass: model.layers.9.self_attn.k_proj.weight\n",
      "Pass: model.layers.9.self_attn.v_proj.weight\n",
      "Pass: model.layers.9.self_attn.o_proj.weight\n",
      "Pass: model.layers.9.mlp.gate_proj.weight\n",
      "Pass: model.layers.9.mlp.up_proj.weight\n",
      "Pass: model.layers.9.mlp.down_proj.weight\n",
      "Pass: model.layers.9.input_layernorm.weight\n",
      "Pass: model.layers.9.post_attention_layernorm.weight\n",
      "Pass: model.layers.10.self_attn.q_proj.weight\n",
      "Pass: model.layers.10.self_attn.k_proj.weight\n",
      "Pass: model.layers.10.self_attn.v_proj.weight\n",
      "Pass: model.layers.10.self_attn.o_proj.weight\n",
      "Pass: model.layers.10.mlp.gate_proj.weight\n",
      "Pass: model.layers.10.mlp.up_proj.weight\n",
      "Pass: model.layers.10.mlp.down_proj.weight\n",
      "Pass: model.layers.10.input_layernorm.weight\n",
      "Pass: model.layers.10.post_attention_layernorm.weight\n",
      "Pass: model.layers.11.self_attn.q_proj.weight\n",
      "Pass: model.layers.11.self_attn.k_proj.weight\n",
      "Pass: model.layers.11.self_attn.v_proj.weight\n",
      "Pass: model.layers.11.self_attn.o_proj.weight\n",
      "Pass: model.layers.11.mlp.gate_proj.weight\n",
      "Pass: model.layers.11.mlp.up_proj.weight\n",
      "Pass: model.layers.11.mlp.down_proj.weight\n",
      "Pass: model.layers.11.input_layernorm.weight\n",
      "Pass: model.layers.11.post_attention_layernorm.weight\n",
      "Pass: model.layers.12.self_attn.q_proj.weight\n",
      "Pass: model.layers.12.self_attn.k_proj.weight\n",
      "Pass: model.layers.12.self_attn.v_proj.weight\n",
      "Pass: model.layers.12.self_attn.o_proj.weight\n",
      "Pass: model.layers.12.mlp.gate_proj.weight\n",
      "Pass: model.layers.12.mlp.up_proj.weight\n",
      "Pass: model.layers.12.mlp.down_proj.weight\n",
      "Pass: model.layers.12.input_layernorm.weight\n",
      "Pass: model.layers.12.post_attention_layernorm.weight\n",
      "Pass: model.layers.13.self_attn.q_proj.weight\n",
      "Pass: model.layers.13.self_attn.k_proj.weight\n",
      "Pass: model.layers.13.self_attn.v_proj.weight\n",
      "Pass: model.layers.13.self_attn.o_proj.weight\n",
      "Pass: model.layers.13.mlp.gate_proj.weight\n",
      "Pass: model.layers.13.mlp.up_proj.weight\n",
      "Pass: model.layers.13.mlp.down_proj.weight\n",
      "Pass: model.layers.13.input_layernorm.weight\n",
      "Pass: model.layers.13.post_attention_layernorm.weight\n",
      "Pass: model.layers.14.self_attn.q_proj.weight\n",
      "Pass: model.layers.14.self_attn.k_proj.weight\n",
      "Pass: model.layers.14.self_attn.v_proj.weight\n",
      "Pass: model.layers.14.self_attn.o_proj.weight\n",
      "Pass: model.layers.14.mlp.gate_proj.weight\n",
      "Pass: model.layers.14.mlp.up_proj.weight\n",
      "Pass: model.layers.14.mlp.down_proj.weight\n",
      "Pass: model.layers.14.input_layernorm.weight\n",
      "Pass: model.layers.14.post_attention_layernorm.weight\n",
      "Pass: model.layers.15.self_attn.q_proj.weight\n",
      "Pass: model.layers.15.self_attn.k_proj.weight\n",
      "Pass: model.layers.15.self_attn.v_proj.weight\n",
      "Pass: model.layers.15.self_attn.o_proj.weight\n",
      "Pass: model.layers.15.mlp.gate_proj.weight\n",
      "Pass: model.layers.15.mlp.up_proj.weight\n",
      "Pass: model.layers.15.mlp.down_proj.weight\n",
      "Pass: model.layers.15.input_layernorm.weight\n",
      "Pass: model.layers.15.post_attention_layernorm.weight\n",
      "Pass: model.layers.16.self_attn.q_proj.weight\n",
      "Pass: model.layers.16.self_attn.k_proj.weight\n",
      "Pass: model.layers.16.self_attn.v_proj.weight\n",
      "Pass: model.layers.16.self_attn.o_proj.weight\n",
      "Pass: model.layers.16.mlp.gate_proj.weight\n",
      "Pass: model.layers.16.mlp.up_proj.weight\n",
      "Pass: model.layers.16.mlp.down_proj.weight\n",
      "Pass: model.layers.16.input_layernorm.weight\n",
      "Pass: model.layers.16.post_attention_layernorm.weight\n",
      "Pass: model.layers.17.self_attn.q_proj.weight\n",
      "Pass: model.layers.17.self_attn.k_proj.weight\n",
      "Pass: model.layers.17.self_attn.v_proj.weight\n",
      "Pass: model.layers.17.self_attn.o_proj.weight\n",
      "Pass: model.layers.17.mlp.gate_proj.weight\n",
      "Pass: model.layers.17.mlp.up_proj.weight\n",
      "Pass: model.layers.17.mlp.down_proj.weight\n",
      "Pass: model.layers.17.input_layernorm.weight\n",
      "Pass: model.layers.17.post_attention_layernorm.weight\n",
      "Pass: model.layers.18.self_attn.q_proj.weight\n",
      "Pass: model.layers.18.self_attn.k_proj.weight\n",
      "Pass: model.layers.18.self_attn.v_proj.weight\n",
      "Pass: model.layers.18.self_attn.o_proj.weight\n",
      "Pass: model.layers.18.mlp.gate_proj.weight\n",
      "Pass: model.layers.18.mlp.up_proj.weight\n",
      "Pass: model.layers.18.mlp.down_proj.weight\n",
      "Pass: model.layers.18.input_layernorm.weight\n",
      "Pass: model.layers.18.post_attention_layernorm.weight\n",
      "Pass: model.layers.19.self_attn.q_proj.weight\n",
      "Pass: model.layers.19.self_attn.k_proj.weight\n",
      "Pass: model.layers.19.self_attn.v_proj.weight\n",
      "Pass: model.layers.19.self_attn.o_proj.weight\n",
      "Pass: model.layers.19.mlp.gate_proj.weight\n",
      "Pass: model.layers.19.mlp.up_proj.weight\n",
      "Pass: model.layers.19.mlp.down_proj.weight\n",
      "Pass: model.layers.19.input_layernorm.weight\n",
      "Pass: model.layers.19.post_attention_layernorm.weight\n",
      "Pass: model.layers.20.self_attn.q_proj.weight\n",
      "Pass: model.layers.20.self_attn.k_proj.weight\n",
      "Pass: model.layers.20.self_attn.v_proj.weight\n",
      "Pass: model.layers.20.self_attn.o_proj.weight\n",
      "Pass: model.layers.20.mlp.gate_proj.weight\n",
      "Pass: model.layers.20.mlp.up_proj.weight\n",
      "Pass: model.layers.20.mlp.down_proj.weight\n",
      "Pass: model.layers.20.input_layernorm.weight\n",
      "Pass: model.layers.20.post_attention_layernorm.weight\n",
      "Pass: model.layers.21.self_attn.q_proj.weight\n",
      "Pass: model.layers.21.self_attn.k_proj.weight\n",
      "Pass: model.layers.21.self_attn.v_proj.weight\n",
      "Pass: model.layers.21.self_attn.o_proj.weight\n",
      "Pass: model.layers.21.mlp.gate_proj.weight\n",
      "Pass: model.layers.21.mlp.up_proj.weight\n",
      "Pass: model.layers.21.mlp.down_proj.weight\n",
      "Pass: model.layers.21.input_layernorm.weight\n",
      "Pass: model.layers.21.post_attention_layernorm.weight\n",
      "Pass: model.layers.22.self_attn.q_proj.weight\n",
      "Pass: model.layers.22.self_attn.k_proj.weight\n",
      "Pass: model.layers.22.self_attn.v_proj.weight\n",
      "Pass: model.layers.22.self_attn.o_proj.weight\n",
      "Pass: model.layers.22.mlp.gate_proj.weight\n",
      "Pass: model.layers.22.mlp.up_proj.weight\n",
      "Pass: model.layers.22.mlp.down_proj.weight\n",
      "Pass: model.layers.22.input_layernorm.weight\n",
      "Pass: model.layers.22.post_attention_layernorm.weight\n",
      "Pass: model.layers.23.self_attn.q_proj.weight\n",
      "Pass: model.layers.23.self_attn.k_proj.weight\n",
      "Pass: model.layers.23.self_attn.v_proj.weight\n",
      "Pass: model.layers.23.self_attn.o_proj.weight\n",
      "Pass: model.layers.23.mlp.gate_proj.weight\n",
      "Pass: model.layers.23.mlp.up_proj.weight\n",
      "Pass: model.layers.23.mlp.down_proj.weight\n",
      "Pass: model.layers.23.input_layernorm.weight\n",
      "Pass: model.layers.23.post_attention_layernorm.weight\n",
      "Pass: model.layers.24.self_attn.q_proj.weight\n",
      "Pass: model.layers.24.self_attn.k_proj.weight\n",
      "Pass: model.layers.24.self_attn.v_proj.weight\n",
      "Pass: model.layers.24.self_attn.o_proj.weight\n",
      "Pass: model.layers.24.mlp.gate_proj.weight\n",
      "Pass: model.layers.24.mlp.up_proj.weight\n",
      "Pass: model.layers.24.mlp.down_proj.weight\n",
      "Pass: model.layers.24.input_layernorm.weight\n",
      "Pass: model.layers.24.post_attention_layernorm.weight\n",
      "Pass: model.layers.25.self_attn.q_proj.weight\n",
      "Pass: model.layers.25.self_attn.k_proj.weight\n",
      "Pass: model.layers.25.self_attn.v_proj.weight\n",
      "Pass: model.layers.25.self_attn.o_proj.weight\n",
      "Pass: model.layers.25.mlp.gate_proj.weight\n",
      "Pass: model.layers.25.mlp.up_proj.weight\n",
      "Pass: model.layers.25.mlp.down_proj.weight\n",
      "Pass: model.layers.25.input_layernorm.weight\n",
      "Pass: model.layers.25.post_attention_layernorm.weight\n",
      "Pass: model.layers.26.self_attn.q_proj.weight\n",
      "Pass: model.layers.26.self_attn.k_proj.weight\n",
      "Pass: model.layers.26.self_attn.v_proj.weight\n",
      "Pass: model.layers.26.self_attn.o_proj.weight\n",
      "Pass: model.layers.26.mlp.gate_proj.weight\n",
      "Pass: model.layers.26.mlp.up_proj.weight\n",
      "Pass: model.layers.26.mlp.down_proj.weight\n",
      "Pass: model.layers.26.input_layernorm.weight\n",
      "Pass: model.layers.26.post_attention_layernorm.weight\n",
      "Pass: model.layers.27.self_attn.q_proj.weight\n",
      "Pass: model.layers.27.self_attn.k_proj.weight\n",
      "Pass: model.layers.27.self_attn.v_proj.weight\n",
      "Pass: model.layers.27.self_attn.o_proj.weight\n",
      "Pass: model.layers.27.mlp.gate_proj.weight\n",
      "Pass: model.layers.27.mlp.up_proj.weight\n",
      "Pass: model.layers.27.mlp.down_proj.weight\n",
      "Pass: model.layers.27.input_layernorm.weight\n",
      "Pass: model.layers.27.post_attention_layernorm.weight\n",
      "Pass: model.layers.28.self_attn.q_proj.weight\n",
      "Pass: model.layers.28.self_attn.k_proj.weight\n",
      "Pass: model.layers.28.self_attn.v_proj.weight\n",
      "Pass: model.layers.28.self_attn.o_proj.weight\n",
      "Pass: model.layers.28.mlp.gate_proj.weight\n",
      "Pass: model.layers.28.mlp.up_proj.weight\n",
      "Pass: model.layers.28.mlp.down_proj.weight\n",
      "Pass: model.layers.28.input_layernorm.weight\n",
      "Pass: model.layers.28.post_attention_layernorm.weight\n",
      "Pass: model.layers.29.self_attn.q_proj.weight\n",
      "Pass: model.layers.29.self_attn.k_proj.weight\n",
      "Pass: model.layers.29.self_attn.v_proj.weight\n",
      "Pass: model.layers.29.self_attn.o_proj.weight\n",
      "Pass: model.layers.29.mlp.gate_proj.weight\n",
      "Pass: model.layers.29.mlp.up_proj.weight\n",
      "Pass: model.layers.29.mlp.down_proj.weight\n",
      "Pass: model.layers.29.input_layernorm.weight\n",
      "Pass: model.layers.29.post_attention_layernorm.weight\n",
      "Pass: model.layers.30.self_attn.q_proj.weight\n",
      "Pass: model.layers.30.self_attn.k_proj.weight\n",
      "Pass: model.layers.30.self_attn.v_proj.weight\n",
      "Pass: model.layers.30.self_attn.o_proj.weight\n",
      "Pass: model.layers.30.mlp.gate_proj.weight\n",
      "Pass: model.layers.30.mlp.up_proj.weight\n",
      "Pass: model.layers.30.mlp.down_proj.weight\n",
      "Pass: model.layers.30.input_layernorm.weight\n",
      "Pass: model.layers.30.post_attention_layernorm.weight\n",
      "Pass: model.layers.31.self_attn.q_proj.weight\n",
      "Pass: model.layers.31.self_attn.k_proj.weight\n",
      "Pass: model.layers.31.self_attn.v_proj.weight\n",
      "Pass: model.layers.31.self_attn.o_proj.weight\n",
      "Pass: model.layers.31.mlp.gate_proj.weight\n",
      "Pass: model.layers.31.mlp.up_proj.weight\n",
      "Pass: model.layers.31.mlp.down_proj.weight\n",
      "Pass: model.layers.31.input_layernorm.weight\n",
      "Pass: model.layers.31.post_attention_layernorm.weight\n",
      "Pass: model.norm.weight\n",
      "Pass: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for ((for_loop_name, for_loop_grad), (scan_name, scan_grad)) in zip(for_loop_grads, scan_grads):\n",
    "  assert for_loop_name == scan_name\n",
    "  assert torch.allclose(for_loop_grad, scan_grad, atol=150, rtol=5e-2), f\"{for_loop_name} mismatch by: {torch.max(torch.abs(for_loop_grad - scan_grad))}\"\n",
    "  print(f\"Pass: {for_loop_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.layers.0.self_attn.v_proj.weight',\n",
       " tensor([[-2941.8899,  -524.6237,   955.9371,  ...,  -303.0801,  2649.9629,\n",
       "           5254.5259],\n",
       "         [-2609.9597,  1783.7124,   987.2698,  ...,  -145.8702,   593.4179,\n",
       "           1061.9828],\n",
       "         [-2566.4500,  -767.1675,   282.2457,  ...,  -266.0605,  1773.7897,\n",
       "           4529.5469],\n",
       "         ...,\n",
       "         [-1018.5014, -5451.6758,  1914.1949,  ...,  3249.9912,  4745.0952,\n",
       "          10958.9365],\n",
       "         [ 2208.6790, -4751.6416,  1141.7797,  ...,  2499.1401,  1118.8171,\n",
       "           2555.9319],\n",
       "         [-1710.6656,  1940.0388,   103.2529,  ...,  -963.4113,   703.6531,\n",
       "           2087.0320]], device='xla:0'))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_grads[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.layers.0.self_attn.v_proj.weight',\n",
       " tensor([[-2929.3120,  -549.2886,   954.0081,  ...,  -302.0883,  2650.4756,\n",
       "           5271.4229],\n",
       "         [-2612.5220,  1782.0955,   991.0074,  ...,  -151.0744,   588.5607,\n",
       "           1072.1655],\n",
       "         [-2563.7986,  -755.3365,   280.3193,  ...,  -271.0489,  1768.2189,\n",
       "           4514.1919],\n",
       "         ...,\n",
       "         [-1020.3356, -5457.8105,  1913.4497,  ...,  3252.3542,  4746.0688,\n",
       "          10957.5186],\n",
       "         [ 2202.8054, -4738.2310,  1140.2487,  ...,  2488.2073,  1114.0991,\n",
       "           2532.7532],\n",
       "         [-1719.0941,  1943.4293,   105.6401,  ...,  -956.2529,   711.2629,\n",
       "           2101.1606]], device='xla:0'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_grads[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0043, -0.0449,  0.0020,  ..., -0.0033,  0.0002,  0.0032],\n",
       "        [-0.0010,  0.0009,  0.0038,  ..., -0.0344,  0.0083,  0.0095],\n",
       "        [-0.0010, -0.0157,  0.0069,  ..., -0.0184,  0.0032,  0.0034],\n",
       "        ...,\n",
       "        [-0.0018, -0.0011,  0.0004,  ...,  0.0007,  0.0002,  0.0001],\n",
       "        [ 0.0027, -0.0028,  0.0013,  ...,  0.0044,  0.0042,  0.0092],\n",
       "        [-0.0049,  0.0017,  0.0226,  ..., -0.0075,  0.0107,  0.0067]],\n",
       "       device='xla:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(for_loop_grads[3][1] - scan_grads[3][1]) / scan_grads[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
