{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `scan` and `apply_layers` in Llama 3\n",
    "\n",
    "Hugging Face usage follows https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    "\n",
    "To test scan, we need to use a custom modification of the transformer repo:\n",
    "https://github.com/tengyifei/transformers/commit/646a575928d8514f220384c29d27c8b956826a91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_USE_SPMD=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_USE_SPMD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:250: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "  torch.random.manual_seed(42)\n",
    "  torch_xla.manual_seed(42)\n",
    "  np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:00<00:00, 5492.63 examples/s]\n",
      "Map: 100%|██████████| 1801350/1801350 [02:28<00:00, 12148.35 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:00<00:00, 10411.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.bos_token_id = 128000\n",
    "tokenizer.eos_token_id = 128001\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:02<00:00, 1820.75 examples/s]\n",
      "Map: 100%|██████████| 1801350/1801350 [17:01<00:00, 1762.87 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:02<00:00, 1816.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"][1].keys(), lm_datasets[\"validation\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"validation\"])  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=True,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "2024-11-08 07:27:01.068015: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 07:27:01.068116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 07:27:01.069501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 07:27:02.305201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=2500,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    tpu_num_cores=4,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m----> 4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mlm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mlm_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 34:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.202100</td>\n",
       "      <td>6.074296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/core/xla_model.py:1472: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  xldata.append(torch.load(xbio))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=6.7848349609375, metrics={'train_runtime': 2166.4797, 'train_samples_per_second': 73.853, 'train_steps_per_second': 1.154, 'total_flos': 1.83650746368e+16, 'train_loss': 6.7848349609375, 'epoch': 0.08881941237076775})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 2628\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 68\n",
      "  Accumulator: 04m45s661ms943.517us\n",
      "  ValueRate: 112ms992.609us / second\n",
      "  Rate: 0.0338977 / second\n",
      "  Percentiles: 1%=027ms010.525us; 5%=027ms205.166us; 10%=027ms374.196us; 20%=028ms867.906us; 50%=029ms050.266us; 80%=031ms631.485us; 90%=051ms153.823us; 95%=12s953ms540.568us; 99%=01m12s953ms510.969us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 2696\n",
      "  Accumulator: 04m40s292ms833.041us\n",
      "  ValueRate: 135ms694.627us / second\n",
      "  Rate: 1.81133 / second\n",
      "  Percentiles: 1%=002ms342.130us; 5%=003ms790.429us; 10%=022ms129.746us; 20%=086ms793.757us; 50%=086ms367.937us; 80%=087ms787.907us; 90%=087ms078.627us; 95%=087ms355.897us; 99%=089ms792.497us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 13063\n",
      "  Accumulator: 01s116ms795.332us\n",
      "  ValueRate: 753.166us / second\n",
      "  Rate: 9.17877 / second\n",
      "  Percentiles: 1%=042.330us; 5%=045.490us; 10%=047.450us; 20%=050.260us; 50%=066.650us; 80%=129.369us; 90%=153.390us; 95%=158.160us; 99%=180.250us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 42\n",
      "  Accumulator: 268ms884.261us\n",
      "  ValueRate: 206.953us / second\n",
      "  Rate: 0.0324469 / second\n",
      "  Percentiles: 1%=167.750us; 5%=226.120us; 10%=235.929us; 20%=288.220us; 50%=449.110us; 80%=839.670us; 90%=004ms024.550us; 95%=061ms318.351us; 99%=063ms970.770us\n",
      "Counter: MarkStep\n",
      "  Value: 5200\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again, this time using scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=False,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.unsqueeze.default(tensor([...], device='xla:0', size=(1, 128, 64)), 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/trainer.py:1991\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/trainer.py:2327\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2327\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2330\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2331\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2333\u001b[0m ):\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/trainer.py:3419\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3419\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3421\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3424\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3425\u001b[0m ):\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/trainer.py:3466\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3465\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3466\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:1229\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1226\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:1019\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m   1018\u001b[0m     curried_layers \u001b[38;5;241m=\u001b[39m [ CurriedLayer(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers ]\n\u001b[0;32m-> 1019\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurried_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOTE: Using for loop to run decoder layers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/apply_layers.py:64\u001b[0m, in \u001b[0;36mapply_layers\u001b[0;34m(layers, input_data)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m output, torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m stacked_params_buffers \u001b[38;5;241m=\u001b[39m (stacked_params, stacked_buffers)\n\u001b[0;32m---> 64\u001b[0m final_carry, _ \u001b[38;5;241m=\u001b[39m \u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacked_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_carry\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/scan.py:149\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(fn, init, xs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xs_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`xs` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is an empty PyTree.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m forward, backward \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad_partitioned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m carry, ys \u001b[38;5;241m=\u001b[39m Scan\u001b[38;5;241m.\u001b[39mapply(forward, backward, init, xs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m carry, ys\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/scan.py:466\u001b[0m, in \u001b[0;36mvalue_and_grad_partitioned\u001b[0;34m(fn, init, xs)\u001b[0m\n\u001b[1;32m    463\u001b[0m   fw_compiler, get_outputs \u001b[38;5;241m=\u001b[39m make_get_outputs_compiler()\n\u001b[1;32m    464\u001b[0m   fn_compiled_no_grad \u001b[38;5;241m=\u001b[39m aot_function(\n\u001b[1;32m    465\u001b[0m       fn, fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler, bw_compiler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 466\u001b[0m   \u001b[43mfn_compiled_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_carry_pytree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_x_pytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m   no_grad_out \u001b[38;5;241m=\u001b[39m get_outputs()\n\u001b[1;32m    469\u001b[0m fwd_fx_module_arr \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m     (fake_mode, shape_env) \u001b[38;5;241m=\u001b[39m construct_fake_mode(flat_args, aot_config)\n\u001b[1;32m    884\u001b[0m     fake_flat_args: FakifiedFlatArgs \u001b[38;5;241m=\u001b[39m process_inputs(\n\u001b[1;32m    885\u001b[0m         flat_args, aot_config, fake_mode, shape_env\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 887\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:527\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    520\u001b[0m     flat_fn,\n\u001b[1;32m    521\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:635\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    633\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m--> 635\u001b[0m     fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mrun_functionalized_fw_and_collect_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_input_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_input_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_autograd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m req_subclass_dispatch \u001b[38;5;241m=\u001b[39m requires_subclass_dispatch(\n\u001b[1;32m    645\u001b[0m     fake_flat_args, fw_metadata\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m try_record_chromium_data(\n\u001b[1;32m    648\u001b[0m     requires_subclass_dispatch\u001b[38;5;241m=\u001b[39mreq_subclass_dispatch\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py:197\u001b[0m, in \u001b[0;36mrun_functionalized_fw_and_collect_metadata.<locals>.inner\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m disable_above, mode, suppress_pending:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     flat_f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(_to_fun, flat_args)\n\u001b[0;32m--> 197\u001b[0m     flat_f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_f_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# We didn't do any tracing, so we don't need to process the\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# unbacked symbols, they will just disappear into the ether.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Also, prevent memoization from applying.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fake_mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:184\u001b[0m, in \u001b[0;36mcreate_tree_flattened_fn.<locals>.flat_fn\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m out_spec\n\u001b[1;32m    183\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, tensor_args_spec)\n\u001b[0;32m--> 184\u001b[0m tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m flat_out, spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flat_out:\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/apply_layers.py:56\u001b[0m, in \u001b[0;36mapply_layers.<locals>.one_layer\u001b[0;34m(carry, params_buffers)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_layer\u001b[39m(carry, params_buffers):\n\u001b[1;32m     54\u001b[0m   \u001b[38;5;66;03m# Apply the current layer's weights and biases to the example layer,\u001b[39;00m\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;66;03m# then run the resulting layer.\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexample_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_buffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;66;03m# TODO(yifeit): it should be possible to return `None` as opposed to\u001b[39;00m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# `torch.zeros(1)`, for additional clarity. There is no extra\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# computation since we discard `ys` right after.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m output, torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/functional_call.py:148\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/utils/stateless.py:300\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    298\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    299\u001b[0m ):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:1005\u001b[0m, in \u001b[0;36mLlamaModel.forward.<locals>.CurriedLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m-> 1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:728\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:409\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 409\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m/workspaces/torch/transformers/src/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(q, k, cos, sin, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unsqueeze_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies Rotary Position Embedding to the query and key tensors.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43munsqueeze_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    276\u001b[0m     q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:545\u001b[0m, in \u001b[0;36mFunctionalTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     outs_wrapped \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    537\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, wrap, outs_unwrapped\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# When we dispatch to the C++ functionalization kernel, we might need to jump back to the\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# PreDispatch mode stack afterwards, to handle any other PreDispatch modes underneath\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode. If we call func() directly, we would need to exclude PreDispatch\u001b[39;00m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# from the TLS in order to avoid infinite looping, but this would prevent us from coming\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;66;03m# back to PreDispatch later\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     outs_unwrapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_dk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatchKey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunctionalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# We don't allow any mutation on result of dropout or _to_copy\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1266\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1263\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m ), func\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1808\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1376\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1376\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1906\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         avoiding_device_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;66;03m# Recompute flat_arg_fake_tensors here again in case some of the inputs\u001b[39;00m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;66;03m# were real tensors and fakified in validate_and_convert_non_fake_tensors\u001b[39;00m\n\u001b[0;32m-> 1906\u001b[0m (flat_args, flat_arg_fake_tensors) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_and_convert_non_fake_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\n\u001b[1;32m   1908\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m args, kwargs  \u001b[38;5;66;03m# Invalidated\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# The current constant handling only support tracing systems\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# (aot autograd, torchdynamo) where each operation is run consecutively.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# Because each operation is run in order, we can trace out and support\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2282\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors\u001b[0;34m(self, func, converter, flat_args, args_spec)\u001b[0m\n\u001b[1;32m   2279\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2282\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [validate(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2282\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2279\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2282\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2270\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors.<locals>.validate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing fake modes NYI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2269\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, args_spec)\n\u001b[0;32m-> 2270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   2271\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_non_fake_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_call(func,\u001b[38;5;250m \u001b[39margs,\u001b[38;5;250m \u001b[39mkwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2273\u001b[0m         )\n\u001b[1;32m   2275\u001b[0m     out \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mfrom_real_tensor(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.unsqueeze.default(tensor([...], device='xla:0', size=(1, 128, 64)), 1)"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the for loop and scan model train to the same loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 2689\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 7\n",
      "  Accumulator: 03m53s782ms040.051us\n",
      "  ValueRate: 055ms660.536us / second\n",
      "  Rate: 0.00221449 / second\n",
      "  Percentiles: 1%=07s096ms216.112us; 5%=07s096ms216.112us; 10%=07s096ms216.112us; 20%=08s578ms547.271us; 50%=32s036ms769.307us; 80%=33s879ms083.254us; 90%=36s811ms836.505us; 95%=36s811ms836.505us; 99%=36s811ms836.505us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 2696\n",
      "  Accumulator: 06m36s541ms094.942us\n",
      "  ValueRate: 104ms571.321us / second\n",
      "  Rate: 0.918485 / second\n",
      "  Percentiles: 1%=009ms357.388us; 5%=021ms740.458us; 10%=024ms257.247us; 20%=131ms199.266us; 50%=132ms933.316us; 80%=132ms433.307us; 90%=133ms727.416us; 95%=133ms989.236us; 99%=135ms132.886us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 78760\n",
      "  Accumulator: 07s139ms091.466us\n",
      "  ValueRate: 004ms247.511us / second\n",
      "  Rate: 41.5308 / second\n",
      "  Percentiles: 1%=053.389us; 5%=059.270us; 10%=065.400us; 20%=079.460us; 50%=094.330us; 80%=115.780us; 90%=161.000us; 95%=185.950us; 99%=199.690us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 10160\n",
      "  Accumulator: 01m17s730ms373.129us\n",
      "  ValueRate: 025ms187.573us / second\n",
      "  Rate: 3.34968 / second\n",
      "  Percentiles: 1%=004ms431.610us; 5%=005ms144.420us; 10%=005ms352.859us; 20%=006ms584.180us; 50%=007ms519.259us; 80%=010ms529.359us; 90%=010ms244.569us; 95%=011ms551.049us; 99%=012ms163.969us\n",
      "Counter: MarkStep\n",
      "  Value: 5200\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the numerical correctness of `apply_layers`\n",
    "\n",
    "Under the same weights, and the same input tokens, both the for loop based\n",
    "implementation and `apply_layers` based implementation should produce the same\n",
    "output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "input_ids = torch.tensor(tokenized_datasets[\"train\"][3][\"input_ids\"]).unsqueeze(0).type(torch.LongTensor) # type:ignore\n",
    "attention_mask = torch.tensor(tokenized_datasets[\"train\"][3][\"attention_mask\"]).unsqueeze(0) # type:ignore\n",
    "input_ids = input_ids.to(torch_xla.device())\n",
    "attention_mask = attention_mask.to(torch_xla.device())\n",
    "torch_xla.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   5476,     73,  56761,    912,  86262,     88,   4298,    220,\n",
       "             18,    551,    366,   3200,     29,  66416,    320,  11002,    551,\n",
       "          50534,     99,  75267,  16144, 115687,  33710, 123283, 104612,     18,\n",
       "           1174,  13318,    662,  86262,     88,   4298,    315,    279,  71735,\n",
       "            220,     18,    883,   1174,  17037,  14183,    311,    439,  86262,\n",
       "             88,   4298,  66416,  14767,   4994,   6457,   1174,    374,    264,\n",
       "          39747,   3560,    571,     12,     31,   5737,   2835,   1847,   8040,\n",
       "            555,  80949,    323,   7972,   5168,   1854,    369,    279,  32365,\n",
       "          42585,    662,  45894,    304,   6186,    220,    679,     16,    304,\n",
       "           6457,   1174,    433,    374,    279,   4948,   1847,    304,    279,\n",
       "          86262,     88,   4298,   4101,    662,  21445,    287,    279,   1890,\n",
       "          37608,    315,  39747,    323,   1972,    571,     12,     31,    892,\n",
       "          27120,    439,   1202,  62540,   1174,    279,   3446,   8640,  15638,\n",
       "            311,    279,   1176,   1847,    323,  11263,    279,    330,   4076,\n",
       "           1752,    330]], device='xla:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-3.5386, -3.6623, -3.4869,  ..., -3.0051, -4.4353, -4.4233],\n",
       "          [-4.9098, -4.5080, -4.4443,  ..., -4.2112, -5.0679, -4.4701],\n",
       "          [-4.6143, -4.4359, -4.2461,  ..., -4.3088, -4.6903, -4.2176],\n",
       "          ...,\n",
       "          [-4.7349, -4.1849, -4.3566,  ..., -4.7857, -5.0580, -4.8042],\n",
       "          [-4.4976, -4.0455, -4.2522,  ..., -4.6340, -4.9632, -4.8398],\n",
       "          [-4.8248, -5.4161, -4.9124,  ..., -4.9636, -5.9460, -5.1975]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_token(logits):\n",
    "  return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  315, 1174,  574,   13,  574,  574, 1174,   16, 1389,  220, 3200,\n",
       "           29,  720,  720,  220,  883,  220,  883,  883,  883,  883,  883,  883,\n",
       "          883,  883,  883,  220,  883,  720, 1174, 1174,  883,  279,  220,  315,\n",
       "         1049, 1389,  720,  323,  220,  311,  279,  264,  311,  662,  662,  662,\n",
       "          662,  279,  662,  323,  264,  220,  571,  315,   12,   31,  220,  571,\n",
       "          662,  662,  555,  279,  571,  279,  662,  366,  662,  279,  220,  220,\n",
       "          662,  720, 1174,  279,  220, 1049,   15, 1174,  279, 1174,  279,  574,\n",
       "          264, 1176,  571, 1174,  279,  220,  315,  662,  662,  662,  720,  374,\n",
       "          304, 1176,  220,  315,  279,  571,  279, 1174,   12,   31,  220, 1174,\n",
       "          662,  264, 1176,  662,  323, 1176,  315,  304,  311,  279,  220,  571,\n",
       "          662,  279,  279, 1176,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pick_token(logits)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = of, was. was was,1 – unk> \\n \\n  )  ) ) ) ) ) ) ) ) )  ) \\n,, ) the  of200 – \\n and  to the a to.... the. and a  @ of-@  @.. by the @ the. <. the  . \\n, the 2000, the, the was a first @, the  of... \\n is in first  of the @ the,-@ ,. a first. and first of in to the  @. the the first < \" \".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-3.5386, -3.6623, -3.4869,  ..., -3.0051, -4.4353, -4.4233],\n",
       "          [-4.9098, -4.5080, -4.4443,  ..., -4.2112, -5.0679, -4.4701],\n",
       "          [-4.6143, -4.4359, -4.2461,  ..., -4.3088, -4.6903, -4.2176],\n",
       "          ...,\n",
       "          [-4.7349, -4.1843, -4.3579,  ..., -4.7867, -5.0578, -4.8052],\n",
       "          [-4.4990, -4.0472, -4.2530,  ..., -4.6336, -4.9631, -4.8392],\n",
       "          [-4.8254, -5.4172, -4.9121,  ..., -4.9634, -5.9421, -5.1983]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "for_loop_logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "for_loop_logits.shape, for_loop_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  315, 1174,  574,   13,  574,  574, 1174,   16, 1389,  220, 3200,\n",
       "           29,  720,  720,  220,  883,  220,  883,  883,  883,  883,  883,  883,\n",
       "          883,  883,  883,  220,  883,  720, 1174, 1174,  883,  279,  220,  315,\n",
       "         1049, 1389,  720,  323,  220,  311,  279,  264,  311,  662,  662,  662,\n",
       "          662,  279,  662,  323,  264,  220,  571,  315,   12,   31,  220,  571,\n",
       "          662,  662,  555,  279,  571,  279,  662,  366,  662,  279,  220,  220,\n",
       "          662,  720, 1174,  279,  220, 1049,   15, 1174,  279, 1174,  279,  574,\n",
       "          264, 1176,  571, 1174,  279,  220,  315,  662,  662,  662,  720,  374,\n",
       "          304, 1176,  220,  315,  279,  571,  279, 1174,   12,   31,  220, 1174,\n",
       "          662,  264, 1176,  662,  323, 1176,  315,  304,  311,  279,  220,  571,\n",
       "          662,  279,  279, 1176,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_tokens = pick_token(for_loop_logits)\n",
    "for_loop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = of, was. was was,1 – unk> \\n \\n  )  ) ) ) ) ) ) ) ) )  ) \\n,, ) the  of200 – \\n and  to the a to.... the. and a  @ of-@  @.. by the @ the. <. the  . \\n, the 2000, the, the was a first @, the  of... \\n is in first  of the @ the,-@ ,. a first. and first of in to the  @. the the first < \" \".'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(for_loop_tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be accurate to within 1%\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shouldn't be completely the same\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-6, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the gradients of scan\n",
    "\n",
    "After I run both scan and for loop versions of the model on the same input, their\n",
    "gradients should also be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n",
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "\n",
    "input_ids.requires_grad_(False)\n",
    "attention_mask.requires_grad_(False)\n",
    "labels = input_ids[:, :].clone().contiguous()\n",
    "\n",
    "# Run for loop model and collect the gradients\n",
    "reset_seed()\n",
    "for_loop_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  for_loop_grads.append((name, param.grad.clone().detach()))\n",
    "\n",
    "# Run scan model and collect the gradients\n",
    "reset_seed()\n",
    "scan_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  scan_grads.append((name, param.grad.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the gradients\n",
    "assert len(for_loop_grads) == len(scan_grads)\n",
    "assert len(for_loop_grads) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: model.embed_tokens.weight\n",
      "Pass: model.layers.0.self_attn.q_proj.weight\n",
      "Pass: model.layers.0.self_attn.k_proj.weight\n",
      "Pass: model.layers.0.self_attn.v_proj.weight\n",
      "Pass: model.layers.0.self_attn.o_proj.weight\n",
      "Pass: model.layers.0.mlp.gate_proj.weight\n",
      "Pass: model.layers.0.mlp.up_proj.weight\n",
      "Pass: model.layers.0.mlp.down_proj.weight\n",
      "Pass: model.layers.0.input_layernorm.weight\n",
      "Pass: model.layers.0.post_attention_layernorm.weight\n",
      "Pass: model.layers.1.self_attn.q_proj.weight\n",
      "Pass: model.layers.1.self_attn.k_proj.weight\n",
      "Pass: model.layers.1.self_attn.v_proj.weight\n",
      "Pass: model.layers.1.self_attn.o_proj.weight\n",
      "Pass: model.layers.1.mlp.gate_proj.weight\n",
      "Pass: model.layers.1.mlp.up_proj.weight\n",
      "Pass: model.layers.1.mlp.down_proj.weight\n",
      "Pass: model.layers.1.input_layernorm.weight\n",
      "Pass: model.layers.1.post_attention_layernorm.weight\n",
      "Pass: model.layers.2.self_attn.q_proj.weight\n",
      "Pass: model.layers.2.self_attn.k_proj.weight\n",
      "Pass: model.layers.2.self_attn.v_proj.weight\n",
      "Pass: model.layers.2.self_attn.o_proj.weight\n",
      "Pass: model.layers.2.mlp.gate_proj.weight\n",
      "Pass: model.layers.2.mlp.up_proj.weight\n",
      "Pass: model.layers.2.mlp.down_proj.weight\n",
      "Pass: model.layers.2.input_layernorm.weight\n",
      "Pass: model.layers.2.post_attention_layernorm.weight\n",
      "Pass: model.layers.3.self_attn.q_proj.weight\n",
      "Pass: model.layers.3.self_attn.k_proj.weight\n",
      "Pass: model.layers.3.self_attn.v_proj.weight\n",
      "Pass: model.layers.3.self_attn.o_proj.weight\n",
      "Pass: model.layers.3.mlp.gate_proj.weight\n",
      "Pass: model.layers.3.mlp.up_proj.weight\n",
      "Pass: model.layers.3.mlp.down_proj.weight\n",
      "Pass: model.layers.3.input_layernorm.weight\n",
      "Pass: model.layers.3.post_attention_layernorm.weight\n",
      "Pass: model.layers.4.self_attn.q_proj.weight\n",
      "Pass: model.layers.4.self_attn.k_proj.weight\n",
      "Pass: model.layers.4.self_attn.v_proj.weight\n",
      "Pass: model.layers.4.self_attn.o_proj.weight\n",
      "Pass: model.layers.4.mlp.gate_proj.weight\n",
      "Pass: model.layers.4.mlp.up_proj.weight\n",
      "Pass: model.layers.4.mlp.down_proj.weight\n",
      "Pass: model.layers.4.input_layernorm.weight\n",
      "Pass: model.layers.4.post_attention_layernorm.weight\n",
      "Pass: model.layers.5.self_attn.q_proj.weight\n",
      "Pass: model.layers.5.self_attn.k_proj.weight\n",
      "Pass: model.layers.5.self_attn.v_proj.weight\n",
      "Pass: model.layers.5.self_attn.o_proj.weight\n",
      "Pass: model.layers.5.mlp.gate_proj.weight\n",
      "Pass: model.layers.5.mlp.up_proj.weight\n",
      "Pass: model.layers.5.mlp.down_proj.weight\n",
      "Pass: model.layers.5.input_layernorm.weight\n",
      "Pass: model.layers.5.post_attention_layernorm.weight\n",
      "Pass: model.layers.6.self_attn.q_proj.weight\n",
      "Pass: model.layers.6.self_attn.k_proj.weight\n",
      "Pass: model.layers.6.self_attn.v_proj.weight\n",
      "Pass: model.layers.6.self_attn.o_proj.weight\n",
      "Pass: model.layers.6.mlp.gate_proj.weight\n",
      "Pass: model.layers.6.mlp.up_proj.weight\n",
      "Pass: model.layers.6.mlp.down_proj.weight\n",
      "Pass: model.layers.6.input_layernorm.weight\n",
      "Pass: model.layers.6.post_attention_layernorm.weight\n",
      "Pass: model.layers.7.self_attn.q_proj.weight\n",
      "Pass: model.layers.7.self_attn.k_proj.weight\n",
      "Pass: model.layers.7.self_attn.v_proj.weight\n",
      "Pass: model.layers.7.self_attn.o_proj.weight\n",
      "Pass: model.layers.7.mlp.gate_proj.weight\n",
      "Pass: model.layers.7.mlp.up_proj.weight\n",
      "Pass: model.layers.7.mlp.down_proj.weight\n",
      "Pass: model.layers.7.input_layernorm.weight\n",
      "Pass: model.layers.7.post_attention_layernorm.weight\n",
      "Pass: model.layers.8.self_attn.q_proj.weight\n",
      "Pass: model.layers.8.self_attn.k_proj.weight\n",
      "Pass: model.layers.8.self_attn.v_proj.weight\n",
      "Pass: model.layers.8.self_attn.o_proj.weight\n",
      "Pass: model.layers.8.mlp.gate_proj.weight\n",
      "Pass: model.layers.8.mlp.up_proj.weight\n",
      "Pass: model.layers.8.mlp.down_proj.weight\n",
      "Pass: model.layers.8.input_layernorm.weight\n",
      "Pass: model.layers.8.post_attention_layernorm.weight\n",
      "Pass: model.layers.9.self_attn.q_proj.weight\n",
      "Pass: model.layers.9.self_attn.k_proj.weight\n",
      "Pass: model.layers.9.self_attn.v_proj.weight\n",
      "Pass: model.layers.9.self_attn.o_proj.weight\n",
      "Pass: model.layers.9.mlp.gate_proj.weight\n",
      "Pass: model.layers.9.mlp.up_proj.weight\n",
      "Pass: model.layers.9.mlp.down_proj.weight\n",
      "Pass: model.layers.9.input_layernorm.weight\n",
      "Pass: model.layers.9.post_attention_layernorm.weight\n",
      "Pass: model.layers.10.self_attn.q_proj.weight\n",
      "Pass: model.layers.10.self_attn.k_proj.weight\n",
      "Pass: model.layers.10.self_attn.v_proj.weight\n",
      "Pass: model.layers.10.self_attn.o_proj.weight\n",
      "Pass: model.layers.10.mlp.gate_proj.weight\n",
      "Pass: model.layers.10.mlp.up_proj.weight\n",
      "Pass: model.layers.10.mlp.down_proj.weight\n",
      "Pass: model.layers.10.input_layernorm.weight\n",
      "Pass: model.layers.10.post_attention_layernorm.weight\n",
      "Pass: model.layers.11.self_attn.q_proj.weight\n",
      "Pass: model.layers.11.self_attn.k_proj.weight\n",
      "Pass: model.layers.11.self_attn.v_proj.weight\n",
      "Pass: model.layers.11.self_attn.o_proj.weight\n",
      "Pass: model.layers.11.mlp.gate_proj.weight\n",
      "Pass: model.layers.11.mlp.up_proj.weight\n",
      "Pass: model.layers.11.mlp.down_proj.weight\n",
      "Pass: model.layers.11.input_layernorm.weight\n",
      "Pass: model.layers.11.post_attention_layernorm.weight\n",
      "Pass: model.layers.12.self_attn.q_proj.weight\n",
      "Pass: model.layers.12.self_attn.k_proj.weight\n",
      "Pass: model.layers.12.self_attn.v_proj.weight\n",
      "Pass: model.layers.12.self_attn.o_proj.weight\n",
      "Pass: model.layers.12.mlp.gate_proj.weight\n",
      "Pass: model.layers.12.mlp.up_proj.weight\n",
      "Pass: model.layers.12.mlp.down_proj.weight\n",
      "Pass: model.layers.12.input_layernorm.weight\n",
      "Pass: model.layers.12.post_attention_layernorm.weight\n",
      "Pass: model.layers.13.self_attn.q_proj.weight\n",
      "Pass: model.layers.13.self_attn.k_proj.weight\n",
      "Pass: model.layers.13.self_attn.v_proj.weight\n",
      "Pass: model.layers.13.self_attn.o_proj.weight\n",
      "Pass: model.layers.13.mlp.gate_proj.weight\n",
      "Pass: model.layers.13.mlp.up_proj.weight\n",
      "Pass: model.layers.13.mlp.down_proj.weight\n",
      "Pass: model.layers.13.input_layernorm.weight\n",
      "Pass: model.layers.13.post_attention_layernorm.weight\n",
      "Pass: model.layers.14.self_attn.q_proj.weight\n",
      "Pass: model.layers.14.self_attn.k_proj.weight\n",
      "Pass: model.layers.14.self_attn.v_proj.weight\n",
      "Pass: model.layers.14.self_attn.o_proj.weight\n",
      "Pass: model.layers.14.mlp.gate_proj.weight\n",
      "Pass: model.layers.14.mlp.up_proj.weight\n",
      "Pass: model.layers.14.mlp.down_proj.weight\n",
      "Pass: model.layers.14.input_layernorm.weight\n",
      "Pass: model.layers.14.post_attention_layernorm.weight\n",
      "Pass: model.layers.15.self_attn.q_proj.weight\n",
      "Pass: model.layers.15.self_attn.k_proj.weight\n",
      "Pass: model.layers.15.self_attn.v_proj.weight\n",
      "Pass: model.layers.15.self_attn.o_proj.weight\n",
      "Pass: model.layers.15.mlp.gate_proj.weight\n",
      "Pass: model.layers.15.mlp.up_proj.weight\n",
      "Pass: model.layers.15.mlp.down_proj.weight\n",
      "Pass: model.layers.15.input_layernorm.weight\n",
      "Pass: model.layers.15.post_attention_layernorm.weight\n",
      "Pass: model.layers.16.self_attn.q_proj.weight\n",
      "Pass: model.layers.16.self_attn.k_proj.weight\n",
      "Pass: model.layers.16.self_attn.v_proj.weight\n",
      "Pass: model.layers.16.self_attn.o_proj.weight\n",
      "Pass: model.layers.16.mlp.gate_proj.weight\n",
      "Pass: model.layers.16.mlp.up_proj.weight\n",
      "Pass: model.layers.16.mlp.down_proj.weight\n",
      "Pass: model.layers.16.input_layernorm.weight\n",
      "Pass: model.layers.16.post_attention_layernorm.weight\n",
      "Pass: model.layers.17.self_attn.q_proj.weight\n",
      "Pass: model.layers.17.self_attn.k_proj.weight\n",
      "Pass: model.layers.17.self_attn.v_proj.weight\n",
      "Pass: model.layers.17.self_attn.o_proj.weight\n",
      "Pass: model.layers.17.mlp.gate_proj.weight\n",
      "Pass: model.layers.17.mlp.up_proj.weight\n",
      "Pass: model.layers.17.mlp.down_proj.weight\n",
      "Pass: model.layers.17.input_layernorm.weight\n",
      "Pass: model.layers.17.post_attention_layernorm.weight\n",
      "Pass: model.layers.18.self_attn.q_proj.weight\n",
      "Pass: model.layers.18.self_attn.k_proj.weight\n",
      "Pass: model.layers.18.self_attn.v_proj.weight\n",
      "Pass: model.layers.18.self_attn.o_proj.weight\n",
      "Pass: model.layers.18.mlp.gate_proj.weight\n",
      "Pass: model.layers.18.mlp.up_proj.weight\n",
      "Pass: model.layers.18.mlp.down_proj.weight\n",
      "Pass: model.layers.18.input_layernorm.weight\n",
      "Pass: model.layers.18.post_attention_layernorm.weight\n",
      "Pass: model.layers.19.self_attn.q_proj.weight\n",
      "Pass: model.layers.19.self_attn.k_proj.weight\n",
      "Pass: model.layers.19.self_attn.v_proj.weight\n",
      "Pass: model.layers.19.self_attn.o_proj.weight\n",
      "Pass: model.layers.19.mlp.gate_proj.weight\n",
      "Pass: model.layers.19.mlp.up_proj.weight\n",
      "Pass: model.layers.19.mlp.down_proj.weight\n",
      "Pass: model.layers.19.input_layernorm.weight\n",
      "Pass: model.layers.19.post_attention_layernorm.weight\n",
      "Pass: model.layers.20.self_attn.q_proj.weight\n",
      "Pass: model.layers.20.self_attn.k_proj.weight\n",
      "Pass: model.layers.20.self_attn.v_proj.weight\n",
      "Pass: model.layers.20.self_attn.o_proj.weight\n",
      "Pass: model.layers.20.mlp.gate_proj.weight\n",
      "Pass: model.layers.20.mlp.up_proj.weight\n",
      "Pass: model.layers.20.mlp.down_proj.weight\n",
      "Pass: model.layers.20.input_layernorm.weight\n",
      "Pass: model.layers.20.post_attention_layernorm.weight\n",
      "Pass: model.layers.21.self_attn.q_proj.weight\n",
      "Pass: model.layers.21.self_attn.k_proj.weight\n",
      "Pass: model.layers.21.self_attn.v_proj.weight\n",
      "Pass: model.layers.21.self_attn.o_proj.weight\n",
      "Pass: model.layers.21.mlp.gate_proj.weight\n",
      "Pass: model.layers.21.mlp.up_proj.weight\n",
      "Pass: model.layers.21.mlp.down_proj.weight\n",
      "Pass: model.layers.21.input_layernorm.weight\n",
      "Pass: model.layers.21.post_attention_layernorm.weight\n",
      "Pass: model.layers.22.self_attn.q_proj.weight\n",
      "Pass: model.layers.22.self_attn.k_proj.weight\n",
      "Pass: model.layers.22.self_attn.v_proj.weight\n",
      "Pass: model.layers.22.self_attn.o_proj.weight\n",
      "Pass: model.layers.22.mlp.gate_proj.weight\n",
      "Pass: model.layers.22.mlp.up_proj.weight\n",
      "Pass: model.layers.22.mlp.down_proj.weight\n",
      "Pass: model.layers.22.input_layernorm.weight\n",
      "Pass: model.layers.22.post_attention_layernorm.weight\n",
      "Pass: model.layers.23.self_attn.q_proj.weight\n",
      "Pass: model.layers.23.self_attn.k_proj.weight\n",
      "Pass: model.layers.23.self_attn.v_proj.weight\n",
      "Pass: model.layers.23.self_attn.o_proj.weight\n",
      "Pass: model.layers.23.mlp.gate_proj.weight\n",
      "Pass: model.layers.23.mlp.up_proj.weight\n",
      "Pass: model.layers.23.mlp.down_proj.weight\n",
      "Pass: model.layers.23.input_layernorm.weight\n",
      "Pass: model.layers.23.post_attention_layernorm.weight\n",
      "Pass: model.layers.24.self_attn.q_proj.weight\n",
      "Pass: model.layers.24.self_attn.k_proj.weight\n",
      "Pass: model.layers.24.self_attn.v_proj.weight\n",
      "Pass: model.layers.24.self_attn.o_proj.weight\n",
      "Pass: model.layers.24.mlp.gate_proj.weight\n",
      "Pass: model.layers.24.mlp.up_proj.weight\n",
      "Pass: model.layers.24.mlp.down_proj.weight\n",
      "Pass: model.layers.24.input_layernorm.weight\n",
      "Pass: model.layers.24.post_attention_layernorm.weight\n",
      "Pass: model.layers.25.self_attn.q_proj.weight\n",
      "Pass: model.layers.25.self_attn.k_proj.weight\n",
      "Pass: model.layers.25.self_attn.v_proj.weight\n",
      "Pass: model.layers.25.self_attn.o_proj.weight\n",
      "Pass: model.layers.25.mlp.gate_proj.weight\n",
      "Pass: model.layers.25.mlp.up_proj.weight\n",
      "Pass: model.layers.25.mlp.down_proj.weight\n",
      "Pass: model.layers.25.input_layernorm.weight\n",
      "Pass: model.layers.25.post_attention_layernorm.weight\n",
      "Pass: model.layers.26.self_attn.q_proj.weight\n",
      "Pass: model.layers.26.self_attn.k_proj.weight\n",
      "Pass: model.layers.26.self_attn.v_proj.weight\n",
      "Pass: model.layers.26.self_attn.o_proj.weight\n",
      "Pass: model.layers.26.mlp.gate_proj.weight\n",
      "Pass: model.layers.26.mlp.up_proj.weight\n",
      "Pass: model.layers.26.mlp.down_proj.weight\n",
      "Pass: model.layers.26.input_layernorm.weight\n",
      "Pass: model.layers.26.post_attention_layernorm.weight\n",
      "Pass: model.layers.27.self_attn.q_proj.weight\n",
      "Pass: model.layers.27.self_attn.k_proj.weight\n",
      "Pass: model.layers.27.self_attn.v_proj.weight\n",
      "Pass: model.layers.27.self_attn.o_proj.weight\n",
      "Pass: model.layers.27.mlp.gate_proj.weight\n",
      "Pass: model.layers.27.mlp.up_proj.weight\n",
      "Pass: model.layers.27.mlp.down_proj.weight\n",
      "Pass: model.layers.27.input_layernorm.weight\n",
      "Pass: model.layers.27.post_attention_layernorm.weight\n",
      "Pass: model.layers.28.self_attn.q_proj.weight\n",
      "Pass: model.layers.28.self_attn.k_proj.weight\n",
      "Pass: model.layers.28.self_attn.v_proj.weight\n",
      "Pass: model.layers.28.self_attn.o_proj.weight\n",
      "Pass: model.layers.28.mlp.gate_proj.weight\n",
      "Pass: model.layers.28.mlp.up_proj.weight\n",
      "Pass: model.layers.28.mlp.down_proj.weight\n",
      "Pass: model.layers.28.input_layernorm.weight\n",
      "Pass: model.layers.28.post_attention_layernorm.weight\n",
      "Pass: model.layers.29.self_attn.q_proj.weight\n",
      "Pass: model.layers.29.self_attn.k_proj.weight\n",
      "Pass: model.layers.29.self_attn.v_proj.weight\n",
      "Pass: model.layers.29.self_attn.o_proj.weight\n",
      "Pass: model.layers.29.mlp.gate_proj.weight\n",
      "Pass: model.layers.29.mlp.up_proj.weight\n",
      "Pass: model.layers.29.mlp.down_proj.weight\n",
      "Pass: model.layers.29.input_layernorm.weight\n",
      "Pass: model.layers.29.post_attention_layernorm.weight\n",
      "Pass: model.layers.30.self_attn.q_proj.weight\n",
      "Pass: model.layers.30.self_attn.k_proj.weight\n",
      "Pass: model.layers.30.self_attn.v_proj.weight\n",
      "Pass: model.layers.30.self_attn.o_proj.weight\n",
      "Pass: model.layers.30.mlp.gate_proj.weight\n",
      "Pass: model.layers.30.mlp.up_proj.weight\n",
      "Pass: model.layers.30.mlp.down_proj.weight\n",
      "Pass: model.layers.30.input_layernorm.weight\n",
      "Pass: model.layers.30.post_attention_layernorm.weight\n",
      "Pass: model.layers.31.self_attn.q_proj.weight\n",
      "Pass: model.layers.31.self_attn.k_proj.weight\n",
      "Pass: model.layers.31.self_attn.v_proj.weight\n",
      "Pass: model.layers.31.self_attn.o_proj.weight\n",
      "Pass: model.layers.31.mlp.gate_proj.weight\n",
      "Pass: model.layers.31.mlp.up_proj.weight\n",
      "Pass: model.layers.31.mlp.down_proj.weight\n",
      "Pass: model.layers.31.input_layernorm.weight\n",
      "Pass: model.layers.31.post_attention_layernorm.weight\n",
      "Pass: model.norm.weight\n",
      "Pass: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for ((for_loop_name, for_loop_grad), (scan_name, scan_grad)) in zip(for_loop_grads, scan_grads):\n",
    "  assert for_loop_name == scan_name\n",
    "  assert torch.allclose(for_loop_grad, scan_grad, atol=1e-3, rtol=1e-3), f\"{for_loop_name} mismatch by: {torch.max(torch.abs(for_loop_grad - scan_grad))}\"\n",
    "  print(f\"Pass: {for_loop_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[ 1.7305e-03, -1.6064e-03, -2.2568e-03,  ...,  2.7648e-03,\n",
       "           -9.1544e-04, -2.4204e-04],\n",
       "          [ 2.0572e-03,  4.0270e-04,  1.7538e-03,  ..., -8.5519e-04,\n",
       "            1.0854e-03, -8.3036e-04],\n",
       "          [ 2.5064e-03,  1.0427e-03, -5.9767e-04,  ...,  1.7253e-03,\n",
       "            3.0151e-03,  5.6954e-03],\n",
       "          ...,\n",
       "          [-7.3633e-03,  4.2043e-03,  7.1877e-04,  ..., -5.1985e-03,\n",
       "           -2.1175e-03,  1.1589e-04],\n",
       "          [-1.7412e-03, -5.6326e-03, -4.5214e-03,  ...,  6.4733e-03,\n",
       "           -1.0346e-03,  3.4541e-03],\n",
       "          [ 7.2834e-04, -2.6041e-03, -9.6229e-05,  ..., -1.0209e-03,\n",
       "           -2.4465e-03, -3.2896e-03]], device='xla:0')),\n",
       " tensor(0.0273, device='xla:0'),\n",
       " tensor(-0.0253, device='xla:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_grads[3], torch.max(for_loop_grads[3][1]), torch.min(for_loop_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[ 0.0017, -0.0016, -0.0023,  ...,  0.0028, -0.0009, -0.0002],\n",
       "          [ 0.0021,  0.0004,  0.0017,  ..., -0.0008,  0.0011, -0.0008],\n",
       "          [ 0.0025,  0.0010, -0.0006,  ...,  0.0017,  0.0030,  0.0057],\n",
       "          ...,\n",
       "          [-0.0073,  0.0042,  0.0007,  ..., -0.0052, -0.0021,  0.0001],\n",
       "          [-0.0018, -0.0056, -0.0045,  ...,  0.0065, -0.0010,  0.0035],\n",
       "          [ 0.0007, -0.0026, -0.0001,  ..., -0.0010, -0.0025, -0.0033]],\n",
       "         device='xla:0')),\n",
       " tensor(0.0273, device='xla:0'),\n",
       " tensor(-0.0253, device='xla:0'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_grads[3], torch.max(scan_grads[3][1]), torch.min(scan_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
