{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `scan` and `apply_layers` in Llama 3\n",
    "\n",
    "Hugging Face usage follows https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    "\n",
    "To test scan, we need to use a custom modification of the transformer repo:\n",
    "https://github.com/tengyifei/transformers/tree/test-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_USE_SPMD=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_USE_SPMD=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:250: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "  torch.random.manual_seed(42)\n",
    "  torch_xla.manual_seed(42)\n",
    "  np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer.bos_token_id = 128000\n",
    "tokenizer.eos_token_id = 128001\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"][1].keys(), lm_datasets[\"validation\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"validation\"])  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 07:21:53.603012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731828113.611602 2174382 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731828113.614233 2174382 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=True,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "2024-11-17 08:01:49.210165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731830509.228689 2204723 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731830509.234155 2204723 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=2500,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    tpu_num_cores=4,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 13:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.202200</td>\n",
       "      <td>6.074329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=6.7848673828125, metrics={'train_runtime': 828.9795, 'train_samples_per_second': 193.008, 'train_steps_per_second': 3.016, 'total_flos': 1.83650746368e+16, 'train_loss': 6.7848673828125, 'epoch': 0.08881941237076775})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 5127\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 69\n",
      "  Accumulator: 01m21s611ms438.649us\n",
      "  ValueRate: 103ms558.488us / second\n",
      "  Rate: 0.0877858 / second\n",
      "  Percentiles: 1%=018ms806.509us; 5%=018ms962.480us; 10%=018ms977.220us; 20%=018ms102.569us; 50%=019ms684.279us; 80%=020ms692.660us; 90%=092ms417.858us; 95%=08s385ms375.178us; 99%=41s494ms927.067us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 5196\n",
      "  Accumulator: 03m23s448ms266.155us\n",
      "  ValueRate: 236ms798.921us / second\n",
      "  Rate: 6.70838 / second\n",
      "  Percentiles: 1%=002ms666.150us; 5%=002ms038.580us; 10%=008ms481.340us; 20%=009ms722.840us; 50%=017ms692.760us; 80%=069ms464.728us; 90%=070ms776.829us; 95%=070ms038.109us; 99%=070ms435.728us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 13063\n",
      "  Accumulator: 01s057ms608.472us\n",
      "  ValueRate: 001ms269.116us / second\n",
      "  Rate: 16.4747 / second\n",
      "  Percentiles: 1%=052.500us; 5%=055.780us; 10%=057.990us; 20%=060.710us; 50%=070.260us; 80%=090.770us; 90%=100.730us; 95%=111.110us; 99%=166.890us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 42\n",
      "  Accumulator: 011ms838.950us\n",
      "  ValueRate: 017.544us / second\n",
      "  Rate: 0.0679798 / second\n",
      "  Percentiles: 1%=067.070us; 5%=074.200us; 10%=076.810us; 20%=082.270us; 50%=101.940us; 80%=160.800us; 90%=175.640us; 95%=232.830us; 99%=003ms499.270us\n",
      "Counter: MarkStep\n",
      "  Value: 7701\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again, this time using scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "# Define model configuration\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,  # Model size\n",
    "    num_hidden_layers=32,  # Number of transformer layers\n",
    "    num_attention_heads=8,  # Number of attention heads\n",
    "    intermediate_size=1024,  # Size of the hidden feedforward layer\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=False,\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "model = LlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 39:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.202200</td>\n",
       "      <td>6.074419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=6.78487939453125, metrics={'train_runtime': 2358.5111, 'train_samples_per_second': 67.839, 'train_steps_per_second': 1.06, 'total_flos': 1.83650746368e+16, 'train_loss': 6.78487939453125, 'epoch': 0.08881941237076775})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the for loop and scan model train to the same loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 5127\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 69\n",
      "  Accumulator: 41s922ms465.724us\n",
      "  ValueRate: 017ms411.805us / second\n",
      "  Rate: 0.0293583 / second\n",
      "  Percentiles: 1%=014ms787.790us; 5%=014ms218.589us; 10%=014ms466.060us; 20%=015ms696.840us; 50%=015ms103.810us; 80%=016ms766.180us; 90%=038ms692.209us; 95%=05s339ms505.876us; 99%=11s852ms781.887us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 5196\n",
      "  Accumulator: 04m12s092ms616.193us\n",
      "  ValueRate: 106ms616.496us / second\n",
      "  Rate: 2.41193 / second\n",
      "  Percentiles: 1%=002ms282.140us; 5%=003ms828.120us; 10%=008ms281.590us; 20%=009ms508.559us; 50%=015ms485.620us; 80%=090ms391.318us; 90%=090ms472.928us; 95%=091ms562.748us; 99%=091ms792.338us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 133889\n",
      "  Accumulator: 15s458ms597.766us\n",
      "  ValueRate: 005ms992.141us / second\n",
      "  Rate: 40.6463 / second\n",
      "  Percentiles: 1%=070.580us; 5%=077.270us; 10%=079.650us; 20%=083.230us; 50%=097.390us; 80%=145.530us; 90%=185.180us; 95%=242.650us; 99%=459.260us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 5101\n",
      "  Accumulator: 47s459ms544.178us\n",
      "  ValueRate: 019ms251.391us / second\n",
      "  Rate: 2.15102 / second\n",
      "  Percentiles: 1%=002ms906.270us; 5%=002ms042.880us; 10%=002ms090.370us; 20%=002ms140.920us; 50%=002ms428.210us; 80%=017ms704.650us; 90%=017ms882.030us; 95%=017ms049.839us; 99%=017ms420.169us\n",
      "Counter: MarkStep\n",
      "  Value: 7701\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the numerical correctness of `apply_layers`\n",
    "\n",
    "Under the same weights, and the same input tokens, both the for loop based\n",
    "implementation and `apply_layers` based implementation should produce the same\n",
    "output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "input_ids = torch.tensor(tokenized_datasets[\"train\"][3][\"input_ids\"]).unsqueeze(0).type(torch.LongTensor) # type:ignore\n",
    "attention_mask = torch.tensor(tokenized_datasets[\"train\"][3][\"attention_mask\"]).unsqueeze(0) # type:ignore\n",
    "input_ids = input_ids.to(torch_xla.device())\n",
    "attention_mask = attention_mask.to(torch_xla.device())\n",
    "torch_xla.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   5476,     73,  56761,    912,  86262,     88,   4298,    220,\n",
       "             18,    551,    366,   3200,     29,  66416,    320,  11002,    551,\n",
       "          50534,     99,  75267,  16144, 115687,  33710, 123283, 104612,     18,\n",
       "           1174,  13318,    662,  86262,     88,   4298,    315,    279,  71735,\n",
       "            220,     18,    883,   1174,  17037,  14183,    311,    439,  86262,\n",
       "             88,   4298,  66416,  14767,   4994,   6457,   1174,    374,    264,\n",
       "          39747,   3560,    571,     12,     31,   5737,   2835,   1847,   8040,\n",
       "            555,  80949,    323,   7972,   5168,   1854,    369,    279,  32365,\n",
       "          42585,    662,  45894,    304,   6186,    220,    679,     16,    304,\n",
       "           6457,   1174,    433,    374,    279,   4948,   1847,    304,    279,\n",
       "          86262,     88,   4298,   4101,    662,  21445,    287,    279,   1890,\n",
       "          37608,    315,  39747,    323,   1972,    571,     12,     31,    892,\n",
       "          27120,    439,   1202,  62540,   1174,    279,   3446,   8640,  15638,\n",
       "            311,    279,   1176,   1847,    323,  11263,    279,    330,   4076,\n",
       "           1752,    330]], device='xla:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-4.4933, -4.3954, -3.4201,  ..., -3.4511, -4.3217, -3.8000],\n",
       "          [-5.2261, -4.5482, -4.1086,  ..., -4.5811, -4.8370, -4.5229],\n",
       "          [-4.9636, -4.4915, -3.7757,  ..., -4.4788, -4.4865, -4.2921],\n",
       "          ...,\n",
       "          [-5.1460, -4.6022, -4.4547,  ..., -5.0204, -4.9061, -4.6020],\n",
       "          [-4.8599, -4.3751, -4.1500,  ..., -4.9346, -4.6417, -4.5007],\n",
       "          [-5.8086, -5.2585, -5.1723,  ..., -5.3529, -5.6692, -4.9910]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_token(logits):\n",
    "  return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  364, 1174,  320,   13,  304, 1174, 1174,   16, 1389,  220, 3200,\n",
       "           29,  720, 1174,  220,  366,  366,  883,  883,  883,  883,  883,  883,\n",
       "          883,  883,  883,  220,  883,  720, 1174, 1174,  883,  279,  220,  315,\n",
       "         1049, 1389,  720,  279,  220,  311,  279,  279,  304, 1174,  662,  662,\n",
       "         1174,  279,  662,  323,  264,  220,  571,  315,   12,   31,  220,  323,\n",
       "          662,  662,  304,  279,  662,  279,  662,   13,  662,  279,  220,  220,\n",
       "          662,  720,  279,  279,  220, 1049,   15, 1174,  220, 1174,  279,  574,\n",
       "          264,  220,  315,  315,  279,  220, 1174, 1174,  662,  662,  720,  574,\n",
       "          279,  220,  220, 1174,  279, 1174,  279,  571,   12,   31,  220, 1174,\n",
       "         1174,  264, 1176, 1174,  323,  220,  574,  304,  311,  279,  220,  220,\n",
       "          662,  279,  279,  220,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pick_token(logits)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = \\', (. in,,1 – unk> \\n,  < < ) ) ) ) ) ) ) ) )  ) \\n,, ) the  of200 – \\n the  to the the in,.., the. and a  @ of-@  and.. in the. the... the  . \\n the the 2000, , the was a  of of the ,,.. \\n was the  , the, the @-@ ,, a first, and  was in to the  . the the  < \" \".'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128000]),\n",
       " tensor([[[-4.4933, -4.3954, -3.4201,  ..., -3.4511, -4.3217, -3.8000],\n",
       "          [-5.2261, -4.5482, -4.1086,  ..., -4.5811, -4.8370, -4.5229],\n",
       "          [-4.9636, -4.4915, -3.7757,  ..., -4.4788, -4.4865, -4.2921],\n",
       "          ...,\n",
       "          [-5.1460, -4.6022, -4.4547,  ..., -5.0204, -4.9061, -4.6020],\n",
       "          [-4.8599, -4.3751, -4.1500,  ..., -4.9346, -4.6417, -4.5007],\n",
       "          [-5.8086, -5.2585, -5.1723,  ..., -5.3529, -5.6692, -4.9910]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "for_loop_logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "for_loop_logits.shape, for_loop_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 284,  364, 1174,  320,   13,  304, 1174, 1174,   16, 1389,  220, 3200,\n",
       "           29,  720, 1174,  220,  366,  366,  883,  883,  883,  883,  883,  883,\n",
       "          883,  883,  883,  220,  883,  720, 1174, 1174,  883,  279,  220,  315,\n",
       "         1049, 1389,  720,  279,  220,  311,  279,  279,  304, 1174,  662,  662,\n",
       "         1174,  279,  662,  323,  264,  220,  571,  315,   12,   31,  220,  323,\n",
       "          662,  662,  304,  279,  662,  279,  662,   13,  662,  279,  220,  220,\n",
       "          662,  720,  279,  279,  220, 1049,   15, 1174,  220, 1174,  279,  574,\n",
       "          264,  220,  315,  315,  279,  220, 1174, 1174,  662,  662,  720,  574,\n",
       "          279,  220,  220, 1174,  279, 1174,  279,  571,   12,   31,  220, 1174,\n",
       "         1174,  264, 1176, 1174,  323,  220,  574,  304,  311,  279,  220,  220,\n",
       "          662,  279,  279,  220,  366,  330,  330,  662]], device='xla:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_tokens = pick_token(for_loop_logits)\n",
    "for_loop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = \\', (. in,,1 – unk> \\n,  < < ) ) ) ) ) ) ) ) )  ) \\n,, ) the  of200 – \\n the  to the the in,.., the. and a  @ of-@  and.. in the. the... the  . \\n the the 2000, , the was a  of of the ,,.. \\n was the  , the, the @-@ ,, a first, and  was in to the  . the the  < \" \".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(for_loop_tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(logits, for_loop_logits, atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the gradients of scan\n",
    "\n",
    "After I run both scan and for loop versions of the model on the same input, their\n",
    "gradients should also be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n",
      "NOTE: Using apply_layers to speed up compilation\n"
     ]
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "\n",
    "input_ids.requires_grad_(False)\n",
    "attention_mask.requires_grad_(False)\n",
    "labels = input_ids[:, :].clone().contiguous()\n",
    "\n",
    "# Run for loop model and collect the gradients\n",
    "reset_seed()\n",
    "for_loop_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  for_loop_grads.append((name, param.grad.clone().detach()))\n",
    "\n",
    "# Run scan model and collect the gradients\n",
    "reset_seed()\n",
    "scan_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  scan_grads.append((name, param.grad.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the gradients\n",
    "assert len(for_loop_grads) == len(scan_grads)\n",
    "assert len(for_loop_grads) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: model.embed_tokens.weight\n",
      "Pass: model.layers.0.self_attn.q_proj.weight\n",
      "Pass: model.layers.0.self_attn.k_proj.weight\n",
      "Pass: model.layers.0.self_attn.v_proj.weight\n",
      "Pass: model.layers.0.self_attn.o_proj.weight\n",
      "Pass: model.layers.0.mlp.gate_proj.weight\n",
      "Pass: model.layers.0.mlp.up_proj.weight\n",
      "Pass: model.layers.0.mlp.down_proj.weight\n",
      "Pass: model.layers.0.input_layernorm.weight\n",
      "Pass: model.layers.0.post_attention_layernorm.weight\n",
      "Pass: model.layers.1.self_attn.q_proj.weight\n",
      "Pass: model.layers.1.self_attn.k_proj.weight\n",
      "Pass: model.layers.1.self_attn.v_proj.weight\n",
      "Pass: model.layers.1.self_attn.o_proj.weight\n",
      "Pass: model.layers.1.mlp.gate_proj.weight\n",
      "Pass: model.layers.1.mlp.up_proj.weight\n",
      "Pass: model.layers.1.mlp.down_proj.weight\n",
      "Pass: model.layers.1.input_layernorm.weight\n",
      "Pass: model.layers.1.post_attention_layernorm.weight\n",
      "Pass: model.layers.2.self_attn.q_proj.weight\n",
      "Pass: model.layers.2.self_attn.k_proj.weight\n",
      "Pass: model.layers.2.self_attn.v_proj.weight\n",
      "Pass: model.layers.2.self_attn.o_proj.weight\n",
      "Pass: model.layers.2.mlp.gate_proj.weight\n",
      "Pass: model.layers.2.mlp.up_proj.weight\n",
      "Pass: model.layers.2.mlp.down_proj.weight\n",
      "Pass: model.layers.2.input_layernorm.weight\n",
      "Pass: model.layers.2.post_attention_layernorm.weight\n",
      "Pass: model.layers.3.self_attn.q_proj.weight\n",
      "Pass: model.layers.3.self_attn.k_proj.weight\n",
      "Pass: model.layers.3.self_attn.v_proj.weight\n",
      "Pass: model.layers.3.self_attn.o_proj.weight\n",
      "Pass: model.layers.3.mlp.gate_proj.weight\n",
      "Pass: model.layers.3.mlp.up_proj.weight\n",
      "Pass: model.layers.3.mlp.down_proj.weight\n",
      "Pass: model.layers.3.input_layernorm.weight\n",
      "Pass: model.layers.3.post_attention_layernorm.weight\n",
      "Pass: model.layers.4.self_attn.q_proj.weight\n",
      "Pass: model.layers.4.self_attn.k_proj.weight\n",
      "Pass: model.layers.4.self_attn.v_proj.weight\n",
      "Pass: model.layers.4.self_attn.o_proj.weight\n",
      "Pass: model.layers.4.mlp.gate_proj.weight\n",
      "Pass: model.layers.4.mlp.up_proj.weight\n",
      "Pass: model.layers.4.mlp.down_proj.weight\n",
      "Pass: model.layers.4.input_layernorm.weight\n",
      "Pass: model.layers.4.post_attention_layernorm.weight\n",
      "Pass: model.layers.5.self_attn.q_proj.weight\n",
      "Pass: model.layers.5.self_attn.k_proj.weight\n",
      "Pass: model.layers.5.self_attn.v_proj.weight\n",
      "Pass: model.layers.5.self_attn.o_proj.weight\n",
      "Pass: model.layers.5.mlp.gate_proj.weight\n",
      "Pass: model.layers.5.mlp.up_proj.weight\n",
      "Pass: model.layers.5.mlp.down_proj.weight\n",
      "Pass: model.layers.5.input_layernorm.weight\n",
      "Pass: model.layers.5.post_attention_layernorm.weight\n",
      "Pass: model.layers.6.self_attn.q_proj.weight\n",
      "Pass: model.layers.6.self_attn.k_proj.weight\n",
      "Pass: model.layers.6.self_attn.v_proj.weight\n",
      "Pass: model.layers.6.self_attn.o_proj.weight\n",
      "Pass: model.layers.6.mlp.gate_proj.weight\n",
      "Pass: model.layers.6.mlp.up_proj.weight\n",
      "Pass: model.layers.6.mlp.down_proj.weight\n",
      "Pass: model.layers.6.input_layernorm.weight\n",
      "Pass: model.layers.6.post_attention_layernorm.weight\n",
      "Pass: model.layers.7.self_attn.q_proj.weight\n",
      "Pass: model.layers.7.self_attn.k_proj.weight\n",
      "Pass: model.layers.7.self_attn.v_proj.weight\n",
      "Pass: model.layers.7.self_attn.o_proj.weight\n",
      "Pass: model.layers.7.mlp.gate_proj.weight\n",
      "Pass: model.layers.7.mlp.up_proj.weight\n",
      "Pass: model.layers.7.mlp.down_proj.weight\n",
      "Pass: model.layers.7.input_layernorm.weight\n",
      "Pass: model.layers.7.post_attention_layernorm.weight\n",
      "Pass: model.layers.8.self_attn.q_proj.weight\n",
      "Pass: model.layers.8.self_attn.k_proj.weight\n",
      "Pass: model.layers.8.self_attn.v_proj.weight\n",
      "Pass: model.layers.8.self_attn.o_proj.weight\n",
      "Pass: model.layers.8.mlp.gate_proj.weight\n",
      "Pass: model.layers.8.mlp.up_proj.weight\n",
      "Pass: model.layers.8.mlp.down_proj.weight\n",
      "Pass: model.layers.8.input_layernorm.weight\n",
      "Pass: model.layers.8.post_attention_layernorm.weight\n",
      "Pass: model.layers.9.self_attn.q_proj.weight\n",
      "Pass: model.layers.9.self_attn.k_proj.weight\n",
      "Pass: model.layers.9.self_attn.v_proj.weight\n",
      "Pass: model.layers.9.self_attn.o_proj.weight\n",
      "Pass: model.layers.9.mlp.gate_proj.weight\n",
      "Pass: model.layers.9.mlp.up_proj.weight\n",
      "Pass: model.layers.9.mlp.down_proj.weight\n",
      "Pass: model.layers.9.input_layernorm.weight\n",
      "Pass: model.layers.9.post_attention_layernorm.weight\n",
      "Pass: model.layers.10.self_attn.q_proj.weight\n",
      "Pass: model.layers.10.self_attn.k_proj.weight\n",
      "Pass: model.layers.10.self_attn.v_proj.weight\n",
      "Pass: model.layers.10.self_attn.o_proj.weight\n",
      "Pass: model.layers.10.mlp.gate_proj.weight\n",
      "Pass: model.layers.10.mlp.up_proj.weight\n",
      "Pass: model.layers.10.mlp.down_proj.weight\n",
      "Pass: model.layers.10.input_layernorm.weight\n",
      "Pass: model.layers.10.post_attention_layernorm.weight\n",
      "Pass: model.layers.11.self_attn.q_proj.weight\n",
      "Pass: model.layers.11.self_attn.k_proj.weight\n",
      "Pass: model.layers.11.self_attn.v_proj.weight\n",
      "Pass: model.layers.11.self_attn.o_proj.weight\n",
      "Pass: model.layers.11.mlp.gate_proj.weight\n",
      "Pass: model.layers.11.mlp.up_proj.weight\n",
      "Pass: model.layers.11.mlp.down_proj.weight\n",
      "Pass: model.layers.11.input_layernorm.weight\n",
      "Pass: model.layers.11.post_attention_layernorm.weight\n",
      "Pass: model.layers.12.self_attn.q_proj.weight\n",
      "Pass: model.layers.12.self_attn.k_proj.weight\n",
      "Pass: model.layers.12.self_attn.v_proj.weight\n",
      "Pass: model.layers.12.self_attn.o_proj.weight\n",
      "Pass: model.layers.12.mlp.gate_proj.weight\n",
      "Pass: model.layers.12.mlp.up_proj.weight\n",
      "Pass: model.layers.12.mlp.down_proj.weight\n",
      "Pass: model.layers.12.input_layernorm.weight\n",
      "Pass: model.layers.12.post_attention_layernorm.weight\n",
      "Pass: model.layers.13.self_attn.q_proj.weight\n",
      "Pass: model.layers.13.self_attn.k_proj.weight\n",
      "Pass: model.layers.13.self_attn.v_proj.weight\n",
      "Pass: model.layers.13.self_attn.o_proj.weight\n",
      "Pass: model.layers.13.mlp.gate_proj.weight\n",
      "Pass: model.layers.13.mlp.up_proj.weight\n",
      "Pass: model.layers.13.mlp.down_proj.weight\n",
      "Pass: model.layers.13.input_layernorm.weight\n",
      "Pass: model.layers.13.post_attention_layernorm.weight\n",
      "Pass: model.layers.14.self_attn.q_proj.weight\n",
      "Pass: model.layers.14.self_attn.k_proj.weight\n",
      "Pass: model.layers.14.self_attn.v_proj.weight\n",
      "Pass: model.layers.14.self_attn.o_proj.weight\n",
      "Pass: model.layers.14.mlp.gate_proj.weight\n",
      "Pass: model.layers.14.mlp.up_proj.weight\n",
      "Pass: model.layers.14.mlp.down_proj.weight\n",
      "Pass: model.layers.14.input_layernorm.weight\n",
      "Pass: model.layers.14.post_attention_layernorm.weight\n",
      "Pass: model.layers.15.self_attn.q_proj.weight\n",
      "Pass: model.layers.15.self_attn.k_proj.weight\n",
      "Pass: model.layers.15.self_attn.v_proj.weight\n",
      "Pass: model.layers.15.self_attn.o_proj.weight\n",
      "Pass: model.layers.15.mlp.gate_proj.weight\n",
      "Pass: model.layers.15.mlp.up_proj.weight\n",
      "Pass: model.layers.15.mlp.down_proj.weight\n",
      "Pass: model.layers.15.input_layernorm.weight\n",
      "Pass: model.layers.15.post_attention_layernorm.weight\n",
      "Pass: model.layers.16.self_attn.q_proj.weight\n",
      "Pass: model.layers.16.self_attn.k_proj.weight\n",
      "Pass: model.layers.16.self_attn.v_proj.weight\n",
      "Pass: model.layers.16.self_attn.o_proj.weight\n",
      "Pass: model.layers.16.mlp.gate_proj.weight\n",
      "Pass: model.layers.16.mlp.up_proj.weight\n",
      "Pass: model.layers.16.mlp.down_proj.weight\n",
      "Pass: model.layers.16.input_layernorm.weight\n",
      "Pass: model.layers.16.post_attention_layernorm.weight\n",
      "Pass: model.layers.17.self_attn.q_proj.weight\n",
      "Pass: model.layers.17.self_attn.k_proj.weight\n",
      "Pass: model.layers.17.self_attn.v_proj.weight\n",
      "Pass: model.layers.17.self_attn.o_proj.weight\n",
      "Pass: model.layers.17.mlp.gate_proj.weight\n",
      "Pass: model.layers.17.mlp.up_proj.weight\n",
      "Pass: model.layers.17.mlp.down_proj.weight\n",
      "Pass: model.layers.17.input_layernorm.weight\n",
      "Pass: model.layers.17.post_attention_layernorm.weight\n",
      "Pass: model.layers.18.self_attn.q_proj.weight\n",
      "Pass: model.layers.18.self_attn.k_proj.weight\n",
      "Pass: model.layers.18.self_attn.v_proj.weight\n",
      "Pass: model.layers.18.self_attn.o_proj.weight\n",
      "Pass: model.layers.18.mlp.gate_proj.weight\n",
      "Pass: model.layers.18.mlp.up_proj.weight\n",
      "Pass: model.layers.18.mlp.down_proj.weight\n",
      "Pass: model.layers.18.input_layernorm.weight\n",
      "Pass: model.layers.18.post_attention_layernorm.weight\n",
      "Pass: model.layers.19.self_attn.q_proj.weight\n",
      "Pass: model.layers.19.self_attn.k_proj.weight\n",
      "Pass: model.layers.19.self_attn.v_proj.weight\n",
      "Pass: model.layers.19.self_attn.o_proj.weight\n",
      "Pass: model.layers.19.mlp.gate_proj.weight\n",
      "Pass: model.layers.19.mlp.up_proj.weight\n",
      "Pass: model.layers.19.mlp.down_proj.weight\n",
      "Pass: model.layers.19.input_layernorm.weight\n",
      "Pass: model.layers.19.post_attention_layernorm.weight\n",
      "Pass: model.layers.20.self_attn.q_proj.weight\n",
      "Pass: model.layers.20.self_attn.k_proj.weight\n",
      "Pass: model.layers.20.self_attn.v_proj.weight\n",
      "Pass: model.layers.20.self_attn.o_proj.weight\n",
      "Pass: model.layers.20.mlp.gate_proj.weight\n",
      "Pass: model.layers.20.mlp.up_proj.weight\n",
      "Pass: model.layers.20.mlp.down_proj.weight\n",
      "Pass: model.layers.20.input_layernorm.weight\n",
      "Pass: model.layers.20.post_attention_layernorm.weight\n",
      "Pass: model.layers.21.self_attn.q_proj.weight\n",
      "Pass: model.layers.21.self_attn.k_proj.weight\n",
      "Pass: model.layers.21.self_attn.v_proj.weight\n",
      "Pass: model.layers.21.self_attn.o_proj.weight\n",
      "Pass: model.layers.21.mlp.gate_proj.weight\n",
      "Pass: model.layers.21.mlp.up_proj.weight\n",
      "Pass: model.layers.21.mlp.down_proj.weight\n",
      "Pass: model.layers.21.input_layernorm.weight\n",
      "Pass: model.layers.21.post_attention_layernorm.weight\n",
      "Pass: model.layers.22.self_attn.q_proj.weight\n",
      "Pass: model.layers.22.self_attn.k_proj.weight\n",
      "Pass: model.layers.22.self_attn.v_proj.weight\n",
      "Pass: model.layers.22.self_attn.o_proj.weight\n",
      "Pass: model.layers.22.mlp.gate_proj.weight\n",
      "Pass: model.layers.22.mlp.up_proj.weight\n",
      "Pass: model.layers.22.mlp.down_proj.weight\n",
      "Pass: model.layers.22.input_layernorm.weight\n",
      "Pass: model.layers.22.post_attention_layernorm.weight\n",
      "Pass: model.layers.23.self_attn.q_proj.weight\n",
      "Pass: model.layers.23.self_attn.k_proj.weight\n",
      "Pass: model.layers.23.self_attn.v_proj.weight\n",
      "Pass: model.layers.23.self_attn.o_proj.weight\n",
      "Pass: model.layers.23.mlp.gate_proj.weight\n",
      "Pass: model.layers.23.mlp.up_proj.weight\n",
      "Pass: model.layers.23.mlp.down_proj.weight\n",
      "Pass: model.layers.23.input_layernorm.weight\n",
      "Pass: model.layers.23.post_attention_layernorm.weight\n",
      "Pass: model.layers.24.self_attn.q_proj.weight\n",
      "Pass: model.layers.24.self_attn.k_proj.weight\n",
      "Pass: model.layers.24.self_attn.v_proj.weight\n",
      "Pass: model.layers.24.self_attn.o_proj.weight\n",
      "Pass: model.layers.24.mlp.gate_proj.weight\n",
      "Pass: model.layers.24.mlp.up_proj.weight\n",
      "Pass: model.layers.24.mlp.down_proj.weight\n",
      "Pass: model.layers.24.input_layernorm.weight\n",
      "Pass: model.layers.24.post_attention_layernorm.weight\n",
      "Pass: model.layers.25.self_attn.q_proj.weight\n",
      "Pass: model.layers.25.self_attn.k_proj.weight\n",
      "Pass: model.layers.25.self_attn.v_proj.weight\n",
      "Pass: model.layers.25.self_attn.o_proj.weight\n",
      "Pass: model.layers.25.mlp.gate_proj.weight\n",
      "Pass: model.layers.25.mlp.up_proj.weight\n",
      "Pass: model.layers.25.mlp.down_proj.weight\n",
      "Pass: model.layers.25.input_layernorm.weight\n",
      "Pass: model.layers.25.post_attention_layernorm.weight\n",
      "Pass: model.layers.26.self_attn.q_proj.weight\n",
      "Pass: model.layers.26.self_attn.k_proj.weight\n",
      "Pass: model.layers.26.self_attn.v_proj.weight\n",
      "Pass: model.layers.26.self_attn.o_proj.weight\n",
      "Pass: model.layers.26.mlp.gate_proj.weight\n",
      "Pass: model.layers.26.mlp.up_proj.weight\n",
      "Pass: model.layers.26.mlp.down_proj.weight\n",
      "Pass: model.layers.26.input_layernorm.weight\n",
      "Pass: model.layers.26.post_attention_layernorm.weight\n",
      "Pass: model.layers.27.self_attn.q_proj.weight\n",
      "Pass: model.layers.27.self_attn.k_proj.weight\n",
      "Pass: model.layers.27.self_attn.v_proj.weight\n",
      "Pass: model.layers.27.self_attn.o_proj.weight\n",
      "Pass: model.layers.27.mlp.gate_proj.weight\n",
      "Pass: model.layers.27.mlp.up_proj.weight\n",
      "Pass: model.layers.27.mlp.down_proj.weight\n",
      "Pass: model.layers.27.input_layernorm.weight\n",
      "Pass: model.layers.27.post_attention_layernorm.weight\n",
      "Pass: model.layers.28.self_attn.q_proj.weight\n",
      "Pass: model.layers.28.self_attn.k_proj.weight\n",
      "Pass: model.layers.28.self_attn.v_proj.weight\n",
      "Pass: model.layers.28.self_attn.o_proj.weight\n",
      "Pass: model.layers.28.mlp.gate_proj.weight\n",
      "Pass: model.layers.28.mlp.up_proj.weight\n",
      "Pass: model.layers.28.mlp.down_proj.weight\n",
      "Pass: model.layers.28.input_layernorm.weight\n",
      "Pass: model.layers.28.post_attention_layernorm.weight\n",
      "Pass: model.layers.29.self_attn.q_proj.weight\n",
      "Pass: model.layers.29.self_attn.k_proj.weight\n",
      "Pass: model.layers.29.self_attn.v_proj.weight\n",
      "Pass: model.layers.29.self_attn.o_proj.weight\n",
      "Pass: model.layers.29.mlp.gate_proj.weight\n",
      "Pass: model.layers.29.mlp.up_proj.weight\n",
      "Pass: model.layers.29.mlp.down_proj.weight\n",
      "Pass: model.layers.29.input_layernorm.weight\n",
      "Pass: model.layers.29.post_attention_layernorm.weight\n",
      "Pass: model.layers.30.self_attn.q_proj.weight\n",
      "Pass: model.layers.30.self_attn.k_proj.weight\n",
      "Pass: model.layers.30.self_attn.v_proj.weight\n",
      "Pass: model.layers.30.self_attn.o_proj.weight\n",
      "Pass: model.layers.30.mlp.gate_proj.weight\n",
      "Pass: model.layers.30.mlp.up_proj.weight\n",
      "Pass: model.layers.30.mlp.down_proj.weight\n",
      "Pass: model.layers.30.input_layernorm.weight\n",
      "Pass: model.layers.30.post_attention_layernorm.weight\n",
      "Pass: model.layers.31.self_attn.q_proj.weight\n",
      "Pass: model.layers.31.self_attn.k_proj.weight\n",
      "Pass: model.layers.31.self_attn.v_proj.weight\n",
      "Pass: model.layers.31.self_attn.o_proj.weight\n",
      "Pass: model.layers.31.mlp.gate_proj.weight\n",
      "Pass: model.layers.31.mlp.up_proj.weight\n",
      "Pass: model.layers.31.mlp.down_proj.weight\n",
      "Pass: model.layers.31.input_layernorm.weight\n",
      "Pass: model.layers.31.post_attention_layernorm.weight\n",
      "Pass: model.norm.weight\n",
      "Pass: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for ((for_loop_name, for_loop_grad), (scan_name, scan_grad)) in zip(for_loop_grads, scan_grads):\n",
    "  assert for_loop_name == scan_name\n",
    "  assert torch.allclose(for_loop_grad, scan_grad, atol=1e-5, rtol=1e-5), f\"{for_loop_name} mismatch by: {torch.max(torch.abs(for_loop_grad - scan_grad))}\"\n",
    "  print(f\"Pass: {for_loop_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[-2.0387e-03,  2.9268e-03,  2.2655e-03,  ..., -8.6698e-05,\n",
       "            2.9253e-03, -2.0951e-03],\n",
       "          [ 1.3585e-03,  6.2552e-03, -3.5560e-03,  ...,  5.9593e-03,\n",
       "            5.8606e-04,  3.1323e-04],\n",
       "          [ 1.1323e-03,  5.4640e-03,  5.5192e-04,  ...,  2.8118e-03,\n",
       "           -2.1846e-03,  1.2042e-03],\n",
       "          ...,\n",
       "          [ 1.7723e-03,  1.7468e-03, -9.1277e-04,  ...,  3.8791e-05,\n",
       "           -3.4969e-04, -1.5257e-03],\n",
       "          [ 2.5581e-03,  7.8343e-03, -3.5407e-03,  ...,  7.5814e-03,\n",
       "            2.6671e-03, -1.4380e-03],\n",
       "          [ 3.7069e-03,  5.2893e-03, -2.6598e-03,  ...,  1.6263e-03,\n",
       "           -2.9556e-03,  1.9197e-03]], device='xla:0')),\n",
       " tensor(0.0229, device='xla:0'),\n",
       " tensor(-0.0295, device='xla:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_grads[3], torch.max(for_loop_grads[3][1]), torch.min(for_loop_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[-2.0387e-03,  2.9268e-03,  2.2655e-03,  ..., -8.6698e-05,\n",
       "            2.9253e-03, -2.0951e-03],\n",
       "          [ 1.3585e-03,  6.2552e-03, -3.5560e-03,  ...,  5.9593e-03,\n",
       "            5.8606e-04,  3.1323e-04],\n",
       "          [ 1.1323e-03,  5.4640e-03,  5.5192e-04,  ...,  2.8118e-03,\n",
       "           -2.1846e-03,  1.2042e-03],\n",
       "          ...,\n",
       "          [ 1.7723e-03,  1.7468e-03, -9.1277e-04,  ...,  3.8791e-05,\n",
       "           -3.4969e-04, -1.5257e-03],\n",
       "          [ 2.5581e-03,  7.8343e-03, -3.5407e-03,  ...,  7.5814e-03,\n",
       "            2.6671e-03, -1.4380e-03],\n",
       "          [ 3.7069e-03,  5.2893e-03, -2.6598e-03,  ...,  1.6263e-03,\n",
       "           -2.9556e-03,  1.9197e-03]], device='xla:0')),\n",
       " tensor(0.0229, device='xla:0'),\n",
       " tensor(-0.0295, device='xla:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_grads[3], torch.max(scan_grads[3][1]), torch.min(scan_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
