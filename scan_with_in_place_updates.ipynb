{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n"
     ]
    }
   ],
   "source": [
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_xla.experimental.scan import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.add.Tensor(tensor([...], device='xla:0', size=(2,)), 1.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     10\u001b[0m xs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]], device\u001b[38;5;241m=\u001b[39mtorch_xla\u001b[38;5;241m.\u001b[39mdevice())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Bug: because we only trace the DAG subgraph rooted at `step_fn` outputs, mutations\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# to `weird_tensor` aren't captured. In PyTorch/XLA, mutations are supported via a\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Copy-on-Write mechanism, where we update the reference inside `weird_tensor` to an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# just like how JAX requires `step_fn` to be a pure function, we also need to prevent\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# side-effects in order to extract a single shared HLO computation.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/scan.py:130\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(fn, init, xs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xs_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`xs` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is an empty PyTree.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m forward, backward \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad_partitioned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m carry, ys \u001b[38;5;241m=\u001b[39m Scan\u001b[38;5;241m.\u001b[39mapply(forward, backward, init, xs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m carry, ys\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/experimental/scan.py:179\u001b[0m, in \u001b[0;36mvalue_and_grad_partitioned\u001b[0;34m(fn, init, xs)\u001b[0m\n\u001b[1;32m    176\u001b[0m   fw_compiler, _ \u001b[38;5;241m=\u001b[39m _make_get_graph_compiler()\n\u001b[1;32m    177\u001b[0m   fn_compiled_no_grad \u001b[38;5;241m=\u001b[39m aot_function(\n\u001b[1;32m    178\u001b[0m       fn, fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler, bw_compiler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m   no_grad_out \u001b[38;5;241m=\u001b[39m \u001b[43mfn_compiled_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_carry_pytree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_x_pytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m   no_grad_out, _ \u001b[38;5;241m=\u001b[39m tree_flatten(no_grad_out)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m     (fake_mode, shape_env) \u001b[38;5;241m=\u001b[39m construct_fake_mode(flat_args, aot_config)\n\u001b[1;32m    884\u001b[0m     fake_flat_args: FakifiedFlatArgs \u001b[38;5;241m=\u001b[39m process_inputs(\n\u001b[1;32m    885\u001b[0m         flat_args, aot_config, fake_mode, shape_env\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 887\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:527\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    520\u001b[0m     flat_fn,\n\u001b[1;32m    521\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:635\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    633\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m--> 635\u001b[0m     fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mrun_functionalized_fw_and_collect_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_input_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_input_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_autograd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m req_subclass_dispatch \u001b[38;5;241m=\u001b[39m requires_subclass_dispatch(\n\u001b[1;32m    645\u001b[0m     fake_flat_args, fw_metadata\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m try_record_chromium_data(\n\u001b[1;32m    648\u001b[0m     requires_subclass_dispatch\u001b[38;5;241m=\u001b[39mreq_subclass_dispatch\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py:197\u001b[0m, in \u001b[0;36mrun_functionalized_fw_and_collect_metadata.<locals>.inner\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m disable_above, mode, suppress_pending:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     flat_f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(_to_fun, flat_args)\n\u001b[0;32m--> 197\u001b[0m     flat_f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_f_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# We didn't do any tracing, so we don't need to process the\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# unbacked symbols, they will just disappear into the ether.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Also, prevent memoization from applying.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fake_mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:184\u001b[0m, in \u001b[0;36mcreate_tree_flattened_fn.<locals>.flat_fn\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m out_spec\n\u001b[1;32m    183\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, tensor_args_spec)\n\u001b[0;32m--> 184\u001b[0m tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m flat_out, spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flat_out:\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mstep_fn\u001b[0;34m(carry, x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m(carry, x):\n\u001b[1;32m      4\u001b[0m   new_carry \u001b[38;5;241m=\u001b[39m carry \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m----> 5\u001b[0m   \u001b[43mweird_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m   y \u001b[38;5;241m=\u001b[39m new_carry \u001b[38;5;241m+\u001b[39m weird_tensor\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m new_carry, y\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:545\u001b[0m, in \u001b[0;36mFunctionalTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     outs_wrapped \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    537\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, wrap, outs_unwrapped\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# When we dispatch to the C++ functionalization kernel, we might need to jump back to the\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# PreDispatch mode stack afterwards, to handle any other PreDispatch modes underneath\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode. If we call func() directly, we would need to exclude PreDispatch\u001b[39;00m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# from the TLS in order to avoid infinite looping, but this would prevent us from coming\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;66;03m# back to PreDispatch later\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     outs_unwrapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_dk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatchKey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunctionalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# We don't allow any mutation on result of dropout or _to_copy\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1271\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1268\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m ), func\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1273\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1813\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1381\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1381\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1911\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         avoiding_device_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Recompute flat_arg_fake_tensors here again in case some of the inputs\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;66;03m# were real tensors and fakified in validate_and_convert_non_fake_tensors\u001b[39;00m\n\u001b[0;32m-> 1911\u001b[0m (flat_args, flat_arg_fake_tensors) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_and_convert_non_fake_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m args, kwargs  \u001b[38;5;66;03m# Invalidated\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# The current constant handling only support tracing systems\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# (aot autograd, torchdynamo) where each operation is run consecutively.\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Because each operation is run in order, we can trace out and support\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m \n\u001b[1;32m   1925\u001b[0m \u001b[38;5;66;03m# We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2378\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors\u001b[0;34m(self, func, converter, flat_args, args_spec)\u001b[0m\n\u001b[1;32m   2375\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2378\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [validate(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2375\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2378\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2366\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors.<locals>.validate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2364\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing fake modes NYI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2365\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, args_spec)\n\u001b[0;32m-> 2366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   2367\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2368\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_non_fake_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_call(func,\u001b[38;5;250m \u001b[39margs,\u001b[38;5;250m \u001b[39mkwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2369\u001b[0m         )\n\u001b[1;32m   2371\u001b[0m     out \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mfrom_real_tensor(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.add.Tensor(tensor([...], device='xla:0', size=(2,)), 1.0)"
     ]
    }
   ],
   "source": [
    "weird_tensor = torch.tensor([0.0, 0.0], device=torch_xla.device())\n",
    "\n",
    "def step_fn(carry, x):\n",
    "  new_carry = carry + x\n",
    "  weird_tensor.add_(1.0)\n",
    "  y = new_carry + weird_tensor\n",
    "  return new_carry, y\n",
    "\n",
    "init = torch.tensor([0.0, 0.0], device=torch_xla.device())\n",
    "xs = torch.tensor([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], device=torch_xla.device())\n",
    "\n",
    "# Bug: because we only trace the DAG subgraph rooted at `step_fn` outputs, mutations\n",
    "# to `weird_tensor` aren't captured. In PyTorch/XLA, mutations are supported via a\n",
    "# Copy-on-Write mechanism, where we update the reference inside `weird_tensor` to an\n",
    "# updated tensor. To fix the bug, we need to verify that the HLO from each step are\n",
    "# the same. This way, the next trace of `step_fn` will use the mutated tensor and\n",
    "# collect a larger and distinct graph, which will catch in-place mutations. In summary,\n",
    "# just like how JAX requires `step_fn` to be a pure function, we also need to prevent\n",
    "# side-effects in order to extract a single shared HLO computation.\n",
    "scan(step_fn, init, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0.], device='xla:0'),\n",
       " tensor([[1., 1.],\n",
       "         [2., 2.],\n",
       "         [3., 3.]], device='xla:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils._pytree import tree_map, tree_flatten, tree_iter, tree_leaves, PyTree\n",
    "\n",
    "\n",
    "def loopy_scan(fn, init, xs):\n",
    "  \"\"\"A simple scan implemented with for loops serving as reference\n",
    "  implementation.\"\"\"\n",
    "  carry = init\n",
    "  ys = []\n",
    "  xs_len = len(next(iter(tree_iter(xs))))\n",
    "  for i in range(xs_len):\n",
    "    carry, y = fn(carry, tree_map(lambda x: x[i], xs))\n",
    "    ys.append(y)\n",
    "  ys = tree_map(lambda *x: torch.stack(x), *ys)\n",
    "  return carry, ys\n",
    "\n",
    "\n",
    "weird_tensor = torch.tensor([0.0, 0.0], device=torch_xla.device())\n",
    "\n",
    "def step_fn(carry, x):\n",
    "  new_carry = carry + x\n",
    "  weird_tensor.add_(1.0)\n",
    "  y = new_carry + weird_tensor\n",
    "  return new_carry, y\n",
    "\n",
    "init = torch.tensor([0.0, 0.0], device=torch_xla.device())\n",
    "xs = torch.tensor([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], device=torch_xla.device())\n",
    "\n",
    "loopy_scan(step_fn, init, xs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
