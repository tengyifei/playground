{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
      "env: LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
    "\n",
    "# MaxText flags except that we disable optimization barrier removal: crashes in native code\n",
    "%env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n",
    "\n",
    "# Still crashes in native code\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100\n",
    "\n",
    "# Removed some more flags, and 1% scheduler shared memory limit: OK\n",
    "# %env LIBTPU_INIT_ARGS=--xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch_xla import runtime as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "      return self.m(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder_only_model\n",
    "from trainer import TrainDecoderOnlyBase\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2D FSDP+TP sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch_xla import runtime as xr\n",
    "from itertools import chain\n",
    "\n",
    "class TrainDecoderOnlyFSDPv2(TrainDecoderOnlyBase):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(decoder_only_model.DecoderOnlyConfig(\n",
    "       hidden_size=4096,\n",
    "       num_hidden_layers=32,\n",
    "       num_attention_heads=16,\n",
    "       num_key_value_heads=8,\n",
    "       intermediate_size=8192,\n",
    "       vocab_size=16384,\n",
    "    ))\n",
    "    # Define the mesh following common SPMD practice\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    tensor_axis = 4\n",
    "    fsdp_axis = num_devices // tensor_axis\n",
    "    mesh_shape = (fsdp_axis, tensor_axis)\n",
    "    print(f\"Single-slice sharding: mesh={mesh_shape}\")\n",
    "    spmd_mesh = xs.Mesh(list(range(num_devices)), mesh_shape, ('fsdp', 'tensor'))\n",
    "    xs.set_global_mesh(spmd_mesh)\n",
    "\n",
    "    model: decoder_only_model.DecoderOnlyModel = self.model  # type:ignore\n",
    "    self.model = model\n",
    "   \n",
    "    # Mark model weights to be sharded\n",
    "    for name, param in chain(model.named_parameters(), model.named_buffers()):\n",
    "      print('> [2D] Sharding tensor', name, param.shape)\n",
    "\n",
    "      # Here we intentionally skip layernorm and moe.gate weights given they are small.\n",
    "      if 'embed_tokens' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'q_proj' in name or 'k_proj' in name or 'v_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "      elif 'o_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'gate_proj' in name or 'up_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "      elif 'down_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'lm_head' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, (('tensor', 'fsdp'), None))\n",
    "\n",
    "      print(f'{name} {torch_xla._XLAC._get_xla_sharding_spec(param)}')\n",
    "\n",
    "    # Shard the input.\n",
    "    # Scale the batch size with num_devices since there will be only one\n",
    "    # process that handles all runtime devices.\n",
    "    self.batch_size *= num_devices\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.randint(\n",
    "            0,\n",
    "            self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "            dtype=torch.int64,\n",
    "            device='cpu'),\n",
    "              torch.randint(\n",
    "                  0,\n",
    "                  self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "                  dtype=torch.int64,\n",
    "                  device='cpu')),\n",
    "        sample_count=self.train_dataset_len // self.batch_size)\n",
    "    self.train_device_loader = pl.MpDeviceLoader(\n",
    "        train_loader,\n",
    "        self.device,\n",
    "        # Shard the input's batch dimension along the `fsdp` axis, no sharding along other dimensions\n",
    "        input_sharding=xs.ShardingSpec(spmd_mesh, ('fsdp', None)))  # type:ignore\n",
    "    \n",
    "    # Apply checkpoint to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = checkpoint_module(block)\n",
    "        \n",
    "    # Apply offloading to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = OffloadingModule(block)\n",
    "\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001)\n",
    "    torch_xla.sync(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-slice sharding: mesh=(2, 4)\n",
      "> [2D] Sharding tensor embed_tokens.weight torch.Size([16384, 4096])\n",
      "embed_tokens.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.0.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.0.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.0.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.0.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.0.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.0.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.0.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.input_layernorm.weight torch.Size([4096])\n",
      "layers.0.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.0.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.1.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.1.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.1.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.1.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.1.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.1.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.1.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.1.input_layernorm.weight torch.Size([4096])\n",
      "layers.1.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.1.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.2.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.2.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.2.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.2.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.2.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.2.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.2.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.2.input_layernorm.weight torch.Size([4096])\n",
      "layers.2.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.2.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.3.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.3.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.3.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.3.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.3.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.3.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.3.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.3.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.3.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.3.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.3.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.3.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.3.input_layernorm.weight torch.Size([4096])\n",
      "layers.3.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.3.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.4.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.4.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.4.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.4.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.4.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.4.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.4.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.4.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.4.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.4.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.4.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.4.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.4.input_layernorm.weight torch.Size([4096])\n",
      "layers.4.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.4.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.5.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.5.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.5.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.5.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.5.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.5.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.5.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.5.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.5.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.5.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.5.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.5.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.5.input_layernorm.weight torch.Size([4096])\n",
      "layers.5.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.5.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.6.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.6.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.6.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.6.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.6.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.6.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.6.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.6.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.6.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.6.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.6.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.6.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.6.input_layernorm.weight torch.Size([4096])\n",
      "layers.6.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.6.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.7.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.7.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.7.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.7.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.7.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.7.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.7.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.7.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.7.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.7.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.7.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.7.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.7.input_layernorm.weight torch.Size([4096])\n",
      "layers.7.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.7.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.8.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.8.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.8.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.8.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.8.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.8.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.8.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.8.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.8.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.8.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.8.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.8.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.8.input_layernorm.weight torch.Size([4096])\n",
      "layers.8.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.8.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.9.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.9.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.9.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.9.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.9.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.9.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.9.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.9.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.9.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.9.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.9.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.9.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.9.input_layernorm.weight torch.Size([4096])\n",
      "layers.9.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.9.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.10.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.10.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.10.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.10.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.10.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.10.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.10.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.10.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.10.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.10.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.10.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.10.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.10.input_layernorm.weight torch.Size([4096])\n",
      "layers.10.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.10.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.11.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.11.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.11.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.11.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.11.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.11.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.11.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.11.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.11.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.11.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.11.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.11.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.11.input_layernorm.weight torch.Size([4096])\n",
      "layers.11.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.11.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.12.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.12.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.12.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.12.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.12.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.12.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.12.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.12.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.12.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.12.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.12.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.12.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.12.input_layernorm.weight torch.Size([4096])\n",
      "layers.12.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.12.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.13.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.13.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.13.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.13.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.13.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.13.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.13.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.13.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.13.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.13.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.13.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.13.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.13.input_layernorm.weight torch.Size([4096])\n",
      "layers.13.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.13.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.14.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.14.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.14.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.14.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.14.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.14.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.14.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.14.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.14.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.14.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.14.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.14.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.14.input_layernorm.weight torch.Size([4096])\n",
      "layers.14.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.14.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.15.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.15.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.15.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.15.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.15.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.15.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.15.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.15.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.15.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.15.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.15.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.15.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.15.input_layernorm.weight torch.Size([4096])\n",
      "layers.15.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.15.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.16.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.16.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.16.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.16.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.16.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.16.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.16.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.16.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.16.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.16.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.16.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.16.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.16.input_layernorm.weight torch.Size([4096])\n",
      "layers.16.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.16.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.17.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.17.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.17.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.17.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.17.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.17.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.17.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.17.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.17.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.17.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.17.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.17.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.17.input_layernorm.weight torch.Size([4096])\n",
      "layers.17.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.17.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.18.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.18.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.18.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.18.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.18.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.18.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.18.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.18.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.18.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.18.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.18.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.18.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.18.input_layernorm.weight torch.Size([4096])\n",
      "layers.18.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.18.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.19.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.19.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.19.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.19.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.19.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.19.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.19.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.19.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.19.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.19.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.19.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.19.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.19.input_layernorm.weight torch.Size([4096])\n",
      "layers.19.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.19.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.20.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.20.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.20.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.20.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.20.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.20.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.20.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.20.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.20.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.20.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.20.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.20.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.20.input_layernorm.weight torch.Size([4096])\n",
      "layers.20.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.20.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.21.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.21.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.21.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.21.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.21.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.21.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.21.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.21.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.21.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.21.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.21.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.21.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.21.input_layernorm.weight torch.Size([4096])\n",
      "layers.21.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.21.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.22.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.22.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.22.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.22.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.22.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.22.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.22.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.22.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.22.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.22.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.22.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.22.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.22.input_layernorm.weight torch.Size([4096])\n",
      "layers.22.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.22.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.23.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.23.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.23.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.23.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.23.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.23.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.23.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.23.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.23.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.23.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.23.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.23.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.23.input_layernorm.weight torch.Size([4096])\n",
      "layers.23.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.23.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.24.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.24.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.24.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.24.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.24.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.24.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.24.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.24.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.24.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.24.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.24.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.24.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.24.input_layernorm.weight torch.Size([4096])\n",
      "layers.24.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.24.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.25.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.25.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.25.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.25.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.25.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.25.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.25.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.25.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.25.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.25.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.25.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.25.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.25.input_layernorm.weight torch.Size([4096])\n",
      "layers.25.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.25.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.26.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.26.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.26.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.26.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.26.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.26.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.26.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.26.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.26.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.26.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.26.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.26.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.26.input_layernorm.weight torch.Size([4096])\n",
      "layers.26.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.26.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.27.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.27.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.27.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.27.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.27.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.27.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.27.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.27.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.27.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.27.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.27.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.27.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.27.input_layernorm.weight torch.Size([4096])\n",
      "layers.27.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.27.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.28.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.28.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.28.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.28.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.28.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.28.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.28.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.28.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.28.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.28.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.28.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.28.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.28.input_layernorm.weight torch.Size([4096])\n",
      "layers.28.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.28.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.29.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.29.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.29.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.29.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.29.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.29.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.29.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.29.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.29.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.29.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.29.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.29.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.29.input_layernorm.weight torch.Size([4096])\n",
      "layers.29.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.29.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.30.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.30.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.30.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.30.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.30.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.30.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.30.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.30.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.30.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.30.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.30.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.30.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.30.input_layernorm.weight torch.Size([4096])\n",
      "layers.30.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.30.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "layers.31.self_attn.q_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.31.self_attn.k_proj.weight torch.Size([2048, 4096])\n",
      "layers.31.self_attn.k_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.31.self_attn.v_proj.weight torch.Size([2048, 4096])\n",
      "layers.31.self_attn.v_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "layers.31.self_attn.o_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.31.mlp.gate_proj.weight torch.Size([8192, 4096])\n",
      "layers.31.mlp.gate_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.31.mlp.up_proj.weight torch.Size([8192, 4096])\n",
      "layers.31.mlp.up_proj.weight {devices=[4,2]0,4,1,5,2,6,3,7}\n",
      "> [2D] Sharding tensor layers.31.mlp.down_proj.weight torch.Size([4096, 8192])\n",
      "layers.31.mlp.down_proj.weight {devices=[2,4]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.31.input_layernorm.weight torch.Size([4096])\n",
      "layers.31.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "layers.31.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor norm.weight torch.Size([4096])\n",
      "norm.weight \n",
      "> [2D] Sharding tensor lm_head.weight torch.Size([16384, 4096])\n",
      "lm_head.weight {devices=[8,1]0,4,1,5,2,6,3,7}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "xr.use_spmd()\n",
    "base = TrainDecoderOnlyFSDPv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n",
      "Epoch 1 train begin  5:37AM UTC on Nov 10, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:184: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n",
      "https://symbolize.corp.google.com/r/?trace=7fd7770fb0ad,7fe93bf48dcf,7fd7770fa2bc,7fd7771020c3,7fd77c08cc11,7fd77c08c678,7fd7770a6ee1,7fd77709ceb0,7fd7797c41b2,7fd77eaf4d12,7fd77eafaed5,7fd77eb03a44,7fd77ec9f602,7fe93bef5ea6&map= \n",
      "*** SIGSEGV (@0x18), see go/stacktraces#s15 received by PID 2069606 (TID 2071168) on cpu 20; stack trace: ***\n",
      "PC: @     0x7fd7770fb0ad  (unknown)  xla::jellyfish::(anonymous namespace)::HasActivationSemantics()\n",
      "    @     0x7fd77edc80e1       1888  FailureSignalHandler()\n",
      "    @     0x7fe93bf48dd0  (unknown)  (unknown)\n",
      "    @     0x7fd7770fa2bd        384  xla::jellyfish::AllReduceDecomposer::TryMatchAllGatherEinsumOrEinsumReduceScatter()\n",
      "    @     0x7fd7771020c4        400  xla::jellyfish::AllReduceDecomposer::Run()\n",
      "    @     0x7fd77c08cc12        416  xla::HloPassPipeline::RunPassesInternal<>()\n",
      "    @     0x7fd77c08c679         80  xla::HloPassPipeline::Run()\n",
      "    @     0x7fd7770a6ee2       1120  xla::jellyfish::(anonymous namespace)::Phase2PreLayoutAssignment()\n",
      "    @     0x7fd77709ceb1       1472  xla::jellyfish::(anonymous namespace)::HloOptimize()::$_0::operator()()\n",
      "    @     0x7fd7797c41b3         32  absl::internal_any_invocable::LocalInvoker<>()\n",
      "    @     0x7fd77eaf4d13        176  thread::Fiber::Body()\n",
      "    @     0x7fd77eafaed6        112  thread::(anonymous namespace)::FutexDomainThread::WorkLoop()\n",
      "    @     0x7fd77eb03a45         32  thread::CommonFiberDomainThread::Run()\n",
      "    @     0x7fd77ec9f603        256  Thread::ThreadBody()\n",
      "    @     0x7fe93bef5ea7  (unknown)  start_thread\n",
      "https://symbolize.corp.google.com/r/?trace=7fd7770fb0ad,7fd77edc80e0,7fe93bf48dcf,7fd7770fa2bc,7fd7771020c3,7fd77c08cc11,7fd77c08c678,7fd7770a6ee1,7fd77709ceb0,7fd7797c41b2,7fd77eaf4d12,7fd77eafaed5,7fd77eb03a44,7fd77ec9f602,7fe93bef5ea6&map= \n",
      "E1110 05:37:37.059751 2071168 coredump_hook.cc:301] RAW: Remote crash data gathering hook invoked.\n",
      "E1110 05:37:37.059765 2071168 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\n",
      "E1110 05:37:37.059768 2071168 coredump_hook.cc:396] RAW: Sending fingerprint to remote end.\n",
      "E1110 05:37:37.059803 2071168 coredump_hook.cc:405] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\n",
      "E1110 05:37:37.059806 2071168 coredump_hook.cc:457] RAW: Dumping core locally.\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling model\")\n",
    "base.num_steps = 3\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "print(\"Profiling model\")\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)\n",
    "xp.trace_detached(\n",
    "    service_addr=\"localhost:9012\", logdir=\"profile/\", duration_ms=15000)\n",
    "base.num_steps = 5\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "del server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                   4096               \n",
       "     \n",
       "                                 \n",
       "      TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "                                 \n",
       "16384\n",
       "                                 \n",
       "      TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> \n",
       "                                 \n",
       "     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                   4096               \n",
       "     \n",
       "                                 \n",
       "      TPU \u001b[1;36m0\u001b[0m  TPU \u001b[1;36m1\u001b[0m  TPU \u001b[1;36m2\u001b[0m  TPU \u001b[1;36m3\u001b[0m \n",
       "                                 \n",
       "16384\n",
       "                                 \n",
       "      TPU \u001b[1;36m4\u001b[0m  TPU \u001b[1;36m5\u001b[0m  TPU \u001b[1;36m6\u001b[0m  TPU \u001b[1;36m7\u001b[0m \n",
       "                                 \n",
       "     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualize import visualize_tensor_sharding\n",
    "_ = visualize_tensor_sharding(base.model.embed_tokens.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">          4096       \n",
       "    \n",
       "                  \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> \n",
       "                  \n",
       "    \n",
       "                  \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> \n",
       "                  \n",
       "4096\n",
       "                  \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> \n",
       "                  \n",
       "    \n",
       "                  \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> \n",
       "                  \n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "          4096       \n",
       "    \n",
       "                  \n",
       "     TPU \u001b[1;36m0\u001b[0m  TPU \u001b[1;36m4\u001b[0m \n",
       "                  \n",
       "    \n",
       "                  \n",
       "     TPU \u001b[1;36m1\u001b[0m  TPU \u001b[1;36m5\u001b[0m \n",
       "                  \n",
       "4096\n",
       "                  \n",
       "     TPU \u001b[1;36m2\u001b[0m  TPU \u001b[1;36m6\u001b[0m \n",
       "                  \n",
       "    \n",
       "                  \n",
       "     TPU \u001b[1;36m3\u001b[0m  TPU \u001b[1;36m7\u001b[0m \n",
       "                  \n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.q_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  4096               \n",
       "    \n",
       "                                \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "                                \n",
       "4096\n",
       "                                \n",
       "     TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>  TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> \n",
       "                                \n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                  4096               \n",
       "    \n",
       "                                \n",
       "     TPU \u001b[1;36m0\u001b[0m  TPU \u001b[1;36m1\u001b[0m  TPU \u001b[1;36m2\u001b[0m  TPU \u001b[1;36m3\u001b[0m \n",
       "                                \n",
       "4096\n",
       "                                \n",
       "     TPU \u001b[1;36m4\u001b[0m  TPU \u001b[1;36m5\u001b[0m  TPU \u001b[1;36m6\u001b[0m  TPU \u001b[1;36m7\u001b[0m \n",
       "                                \n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.o_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
