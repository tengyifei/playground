{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5168f1",
   "metadata": {},
   "source": [
    "# Experimenting converting PyTorch code to Jax code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b9137",
   "metadata": {},
   "source": [
    "Approach 1: let's print the jaxpr back into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f53ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (25.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: JaxDecompiler in /usr/local/lib/python3.10/site-packages (0.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --upgrade JaxDecompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cdd97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import torch\n",
    "import torchax\n",
    "import jax.extend.core as core\n",
    "from JaxDecompiler import decompiler\n",
    "from torchax.interop import jax_view\n",
    "\n",
    "torchax.enable_globally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c6df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example torch function\n",
    "def my_mathy_thing(weight, hidden):\n",
    "  def other_math(a, b):\n",
    "    return (a + b) @ (a * b)\n",
    "\n",
    "  activation = torch.matmul(weight, hidden)\n",
    "  output = torch.sin(activation) + other_math(weight, hidden)\n",
    "  return output + hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a02ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameIterator:\n",
    "  \"\"\"\n",
    "  An iterator that yields strings in lexicographical order:\n",
    "  'a', 'b', ..., 'z', 'aa', 'ab', ..., 'az', 'ba', ...\n",
    "\n",
    "  The sequence starts with single letters, then moves to longer\n",
    "  combinations, following the pattern similar to spreadsheet columns.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    self.current = None\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self.current is None:\n",
    "      self.current = \"a\"\n",
    "      return self.current\n",
    "\n",
    "    # Convert the current string to a list of indices\n",
    "    indices = [self.alphabet.index(c) for c in self.current]\n",
    "\n",
    "    # Try to increment the rightmost index\n",
    "    pos = len(indices) - 1\n",
    "    indices[pos] += 1\n",
    "\n",
    "    # Handle carry, like incrementing digits in a number\n",
    "    while pos >= 0 and indices[pos] == 26:\n",
    "      indices[pos] = 0\n",
    "      pos -= 1\n",
    "\n",
    "      if pos < 0:\n",
    "        # We need to add a new position (like going from 'z' to 'aa')\n",
    "        indices = [0] * (len(indices) + 1)\n",
    "        break\n",
    "\n",
    "      indices[pos] += 1\n",
    "\n",
    "    # Convert back to a string\n",
    "    self.current = \"\".join(self.alphabet[i] for i in indices)\n",
    "    return self.current\n",
    "\n",
    "\n",
    "class LegalNameIterator:\n",
    "  def __init__(self):\n",
    "    self.iterator = NameIterator()\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    while True:\n",
    "      name = next(self.iterator)\n",
    "      if name not in {'and', 'as', 'not', 'or', 'is'}:\n",
    "        return name\n",
    "\n",
    "\n",
    "\n",
    "def decompile_jaxpr(jaxpr: core.ClosedJaxpr) -> str:\n",
    "  names = LegalNameIterator()\n",
    "\n",
    "  class NamedVar(core.Var):\n",
    "    def __init__(self, name, suffix, aval):\n",
    "      self.name = name\n",
    "      super().__init__(suffix, aval)\n",
    "\n",
    "    def __str__(self):\n",
    "      return self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "      if self.suffix:\n",
    "        return f\"var_id_{id(self)}_{self.suffix}\"\n",
    "      return f\"var_id_{id(self)}\"\n",
    "\n",
    "  var_names = {}\n",
    "  def get_name(v: core.Var):\n",
    "    key = v.__repr__()\n",
    "    if key not in var_names:\n",
    "      var_names[key] = next(names)\n",
    "    return var_names[key]\n",
    "\n",
    "  def name(vars):\n",
    "    for i in range(len(vars)):\n",
    "      v = vars[i]\n",
    "      if not isinstance(v, NamedVar) and isinstance(v, core.Var):\n",
    "        count = v.count\n",
    "        var_name = get_name(v)\n",
    "        nv = NamedVar(var_name, v.suffix, v.aval)\n",
    "        setattr(nv, 'count', count)\n",
    "        vars[i] = nv\n",
    "\n",
    "  # Create an in-memory file object\n",
    "  import io\n",
    "  f = io.StringIO()\n",
    "  # Name all the variables\n",
    "  def recursive_name(jaxpr):\n",
    "    name(jaxpr.invars)\n",
    "    for eqn in jaxpr.eqns:\n",
    "        name(eqn.invars)\n",
    "        name(eqn.outvars)\n",
    "        if hasattr(eqn, 'params') and 'jaxpr' in eqn.params:\n",
    "          recursive_name(eqn.params['jaxpr'].jaxpr)\n",
    "    name(jaxpr.outvars)\n",
    "  recursive_name(jaxpr.jaxpr)\n",
    "  python_lines = decompiler.decompiler(jaxpr, python_func_name='decompiled')\n",
    "  decompiler._recursively_write_python_program(f, python_lines)\n",
    "  f.seek(0)\n",
    "  s = f.read()\n",
    "  f.close()\n",
    "  try:\n",
    "    from yapf.yapflib.yapf_api import FormatCode\n",
    "    formatted_code, _ = FormatCode(s)\n",
    "  except Exception as e:\n",
    "    print(\"yapf error\")\n",
    "    print(s)\n",
    "    raise e\n",
    "  return formatted_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1ff555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import jax\n",
      "from jax.numpy import *\n",
      "from jax.experimental import sparse\n",
      "from jax._src import prng\n",
      "from mpi4py import MPI\n",
      "\n",
      "\n",
      "def decompiled(a, b):\n",
      "    c = tensordot(a, b, axes=((1, ), (0, )))\n",
      "    d = sin(c)\n",
      "    e = b * 1.0\n",
      "    f = a + e\n",
      "    g = a * b\n",
      "    h = tensordot(f, g, axes=((1, ), (0, )))\n",
      "    i = h * 1.0\n",
      "    j = d + i\n",
      "    k = b * 1.0\n",
      "    l = j + k\n",
      "    return l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_mathy_thing_jaxpr = jax.make_jaxpr(jax_view(my_mathy_thing))(jax.numpy.ones((4, 4)), jax.numpy.ones((4, 4)))\n",
    "my_mathy_thing_py = decompile_jaxpr(my_mathy_thing_jaxpr)\n",
    "print(my_mathy_thing_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c98d6a",
   "metadata": {},
   "source": [
    "This works but has significant readability drawbacks:\n",
    "\n",
    "- Function invocations are all inlined and flattened.\n",
    "- Variable names don't have meaning.\n",
    "\n",
    "Let's try this on a Llama transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968dc539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= (1, 4, 2048, 64)\n",
      "k= (1, 4, 2048, 64)\n",
      "v= (1, 4, 2048, 64)\n",
      "mask= (1, 1, 2048, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/jax/_src/ops/scatter.py:108: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=bfloat16 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torchax.interop\n",
    "\n",
    "# Adjust path as needed\n",
    "p = Path('/') / 'workspaces' / 'torch' / 'pytorch' / 'xla' / 'torchax' / 'test' / 'llama'\n",
    "assert p.exists()\n",
    "sys.path.append(str(p))\n",
    "\n",
    "\n",
    "def setup_llama():\n",
    "  import llama_model  # type: ignore\n",
    "\n",
    "  model_args = llama_model.ModelArgs(\n",
    "      block_size=2048,\n",
    "      vocab_size=32000,\n",
    "      n_layer=2,\n",
    "      n_head=4,\n",
    "      dim=256,\n",
    "  )\n",
    "  m = llama_model.Transformer(model_args)\n",
    "  m.to(torch.bfloat16)\n",
    "  m.setup_caches(1, 2048)\n",
    "  m = m.to('jax')\n",
    "\n",
    "  # Extract jaxpr\n",
    "  input_pos = torch.arange(0, 2048, device='jax:0')\n",
    "  sample_args = (\n",
    "      torch.rand((1, 2048, 256), device='jax:0'),  # Embedding\n",
    "      input_pos,  # Input pos\n",
    "      m.freqs_cis[input_pos],  # Freqs-cis\n",
    "      m.causal_mask[None, None, input_pos],  # Mask\n",
    "  )\n",
    "  states, jax_func = torchax.extract_jax(m.layers[0])\n",
    "  sample_inputs = jax_view(sample_args)\n",
    "  jaxpr = jax.make_jaxpr(jax_func)(states, sample_inputs)\n",
    "  return jaxpr, sample_inputs\n",
    "\n",
    "\n",
    "jaxpr, sample_inputs = setup_llama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb6261",
   "metadata": {},
   "source": [
    "We also need to add some extra op support to JaxDecompiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JaxDecompiler import primitive_mapping # Prod phase\n",
    "\n",
    "\n",
    "def register(fn):\n",
    "  setattr(primitive_mapping, fn.__name__, fn)\n",
    "\n",
    "\n",
    "@register\n",
    "def rsqrt(input_var, output_var, params):\n",
    "  return f\"{output_var[0]} = jax.lax.rsqrt({input_var[0]})\"\n",
    "\n",
    "\n",
    "@register\n",
    "def scatter(input_var, output_var, params):\n",
    "  operand = input_var[0]\n",
    "  scatter_indices = input_var[1]\n",
    "  updates = input_var[2]\n",
    "\n",
    "  dimension_numbers = params.get(\"dimension_numbers\", None)\n",
    "  indices_are_sorted = params.get(\"indices_are_sorted\", False)\n",
    "  unique_indices = params.get(\"unique_indices\", False)\n",
    "\n",
    "  options = []\n",
    "  if dimension_numbers:\n",
    "    options.append(f\"dimension_numbers=jax.lax.{dimension_numbers}\")\n",
    "  if indices_are_sorted:\n",
    "    options.append(f\"indices_are_sorted={indices_are_sorted}\")\n",
    "  if unique_indices:\n",
    "    options.append(f\"unique_indices={unique_indices}\")\n",
    "\n",
    "  options_str = \", \".join(options)\n",
    "  if options_str:\n",
    "    options_str = \", \" + options_str\n",
    "\n",
    "  return f\"{output_var[0]} = jax.lax.scatter({operand}, {scatter_indices}, {updates}{options_str})\"\n",
    "\n",
    "\n",
    "@register\n",
    "def not__(input_var, output_var, params):\n",
    "  return f\"{output_var[0]} = ~{input_var[0]}\"\n",
    "\n",
    "\n",
    "@register\n",
    "def logistic(input_var, output_var, params):\n",
    "    return f\"{output_var[0]} = 1.0 / (1.0 + exp(-{input_var[0]}))\"\n",
    "\n",
    "\n",
    "@register\n",
    "def pjit(input_var, output_var, params) -> list[str]:\n",
    "  local_f_name = \"local_f\" + str(primitive_mapping._LOCAL_F_COUNT)\n",
    "  primitive_mapping._LOCAL_F_COUNT += 1\n",
    "\n",
    "  lvalue = \", \".join(output_var)\n",
    "  rvalue = \", \".join(input_var)\n",
    "\n",
    "  # pjit specific parameters\n",
    "  in_axis_resources = params.get(\"in_axis_resources\", None)\n",
    "  out_axis_resources = params.get(\"out_axis_resources\", None)\n",
    "  resource_env = params.get(\"resource_env\", None)\n",
    "  donated_invars = params.get(\"donated_invars\", None)\n",
    "\n",
    "  options = []\n",
    "  if in_axis_resources is not None:\n",
    "      options.append(f\"in_axis_resources={in_axis_resources}\")\n",
    "  if out_axis_resources is not None:\n",
    "      options.append(f\"out_axis_resources={out_axis_resources}\")\n",
    "  if resource_env is not None:\n",
    "      options.append(f\"resource_env={resource_env}\")\n",
    "  if donated_invars is not None:\n",
    "      options.append(f\"donated_invars={donated_invars}\")\n",
    "\n",
    "  options_str = \", \".join(options)\n",
    "  if options_str:\n",
    "      options_str = \", \" + options_str\n",
    "\n",
    "  line = f\"{lvalue} = jax.pjit({local_f_name}{options_str})({rvalue})\"\n",
    "\n",
    "  params[\"call_jaxpr\"] = params[\"jaxpr\"].jaxpr\n",
    "  lines = primitive_mapping._recurive_op(params, line, local_f_name)\n",
    "  return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f1f3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import jax\n",
      "from jax.numpy import *\n",
      "from jax.experimental import sparse\n",
      "from jax._src import prng\n",
      "from mpi4py import MPI\n",
      "\n",
      "\n",
      "def decompiled(a, b, c, d, e, f, g, h, i, j, k, l, m):\n",
      "    n = j * j\n",
      "    o = sum(n, axis=(2, ))\n",
      "    tmp_broadcast = array(o) if isinstance(o, ndarray) or isscalar(o) else -1\n",
      "    p = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 1)))\n",
      "    q = p / 256.0\n",
      "    r = q + 9.999999747378752e-06\n",
      "    s = jax.lax.rsqrt(r)\n",
      "    t = j * s\n",
      "    u = array(i).astype(float32)\n",
      "    tmp_broadcast = array(u) if isinstance(u, ndarray) or isscalar(u) else -1\n",
      "    v = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 1, 256)))\n",
      "    w = t * v\n",
      "    x = tensordot(w, a, axes=((2, ), (1, )))\n",
      "    y = x[0:1:][0:2048:][0:256:] if len(\n",
      "        x.shape\n",
      "    ) >= 3 else x  # static slice inputs:[(0, 0, 0), (1, 2048, 256), None]\n",
      "    z = x[0:1:][0:2048:][256:512:] if len(\n",
      "        x.shape\n",
      "    ) >= 3 else x  # static slice inputs:[(0, 0, 256), (1, 2048, 512), None]\n",
      "    aa = x[0:1:][0:2048:][512:768:] if len(\n",
      "        x.shape\n",
      "    ) >= 3 else x  # static slice inputs:[(0, 0, 512), (1, 2048, 768), None]\n",
      "    ab = array(y).reshape((1, 2048, 4, 64))\n",
      "    ac = array(z).reshape((1, 2048, 4, 64))\n",
      "    ad = array(aa).reshape((1, 2048, 4, 64))\n",
      "    ae = array(ab).reshape((1, 2048, 4, 32, 2))\n",
      "    af = array(l).reshape((1, 2048, 1, 32, 2))\n",
      "    ag = ae[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    ah = squeeze(array(ag))\n",
      "    ai = af[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    ah = squeeze(array(ai))\n",
      "    aj = ae[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    ak = squeeze(array(aj))\n",
      "    al = af[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    am = squeeze(array(al))\n",
      "    an = array(am).astype(float32)\n",
      "    ao = ak * an\n",
      "    ap = ae[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    ah = squeeze(array(ap))\n",
      "    aq = af[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    ah = squeeze(array(aq))\n",
      "    ar = ae[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    at = squeeze(array(ar))\n",
      "    au = af[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    av = squeeze(array(au))\n",
      "    aw = array(av).astype(float32)\n",
      "    ax = at * aw\n",
      "    ay = ax * 1.0\n",
      "    az = ao - ay\n",
      "    ba = ae[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    ah = squeeze(array(ba))\n",
      "    bb = af[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    ah = squeeze(array(bb))\n",
      "    bc = ae[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    bd = squeeze(array(bc))\n",
      "    be = af[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    bf = squeeze(array(be))\n",
      "    bg = array(bf).astype(float32)\n",
      "    bh = bd * bg\n",
      "    bi = ae[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    ah = squeeze(array(bi))\n",
      "    bj = af[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    ah = squeeze(array(bj))\n",
      "    bk = ae[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        ae.shape\n",
      "    ) >= 5 else ae  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    bl = squeeze(array(bk))\n",
      "    bm = af[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        af.shape\n",
      "    ) >= 5 else af  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    bn = squeeze(array(bm))\n",
      "    bo = array(bn).astype(float32)\n",
      "    bp = bl * bo\n",
      "    bq = bp * 1.0\n",
      "    br = bh + bq\n",
      "    tmp_broadcast = array(az) if isinstance(az,\n",
      "                                            ndarray) or isscalar(az) else -1\n",
      "    bs = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 4, 32, 1)))\n",
      "    tmp_broadcast = array(br) if isinstance(br,\n",
      "                                            ndarray) or isscalar(br) else -1\n",
      "    bt = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 4, 32, 1)))\n",
      "    bu = concatenate((bs, bt), axis=4)\n",
      "    bv = array(bu).reshape((1, 2048, 4, 64))\n",
      "    bw = array(ac).reshape((1, 2048, 4, 32, 2))\n",
      "    bx = array(l).reshape((1, 2048, 1, 32, 2))\n",
      "    by = bw[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    ah = squeeze(array(by))\n",
      "    bz = bx[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    ah = squeeze(array(bz))\n",
      "    ca = bw[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    cb = squeeze(array(ca))\n",
      "    cc = bx[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    cd = squeeze(array(cc))\n",
      "    ce = array(cd).astype(float32)\n",
      "    cf = cb * ce\n",
      "    cg = bw[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    ah = squeeze(array(cg))\n",
      "    ch = bx[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    ah = squeeze(array(ch))\n",
      "    ci = bw[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    cj = squeeze(array(ci))\n",
      "    ck = bx[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    cl = squeeze(array(ck))\n",
      "    cm = array(cl).astype(float32)\n",
      "    cn = cj * cm\n",
      "    co = cn * 1.0\n",
      "    cp = cf - co\n",
      "    cq = bw[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    ah = squeeze(array(cq))\n",
      "    cr = bx[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    ah = squeeze(array(cr))\n",
      "    cs = bw[0:1:][0:2048:][0:4:][0:32:][1:2:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 4, 32, 2), None]\n",
      "    ct = squeeze(array(cs))\n",
      "    cu = bx[0:1:][0:2048:][0:1:][0:32:][0:1:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 1, 32, 1), None]\n",
      "    cv = squeeze(array(cu))\n",
      "    cw = array(cv).astype(float32)\n",
      "    cx = ct * cw\n",
      "    cy = bw[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    ah = squeeze(array(cy))\n",
      "    cz = bx[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    ah = squeeze(array(cz))\n",
      "    da = bw[0:1:][0:2048:][0:4:][0:32:][0:1:] if len(\n",
      "        bw.shape\n",
      "    ) >= 5 else bw  # static slice inputs:[(0, 0, 0, 0, 0), (1, 2048, 4, 32, 1), None]\n",
      "    db = squeeze(array(da))\n",
      "    dc = bx[0:1:][0:2048:][0:1:][0:32:][1:2:] if len(\n",
      "        bx.shape\n",
      "    ) >= 5 else bx  # static slice inputs:[(0, 0, 0, 0, 1), (1, 2048, 1, 32, 2), None]\n",
      "    dd = squeeze(array(dc))\n",
      "    de = array(dd).astype(float32)\n",
      "    df = db * de\n",
      "    dg = df * 1.0\n",
      "    dh = cx + dg\n",
      "    tmp_broadcast = array(cp) if isinstance(cp,\n",
      "                                            ndarray) or isscalar(cp) else -1\n",
      "    di = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 4, 32, 1)))\n",
      "    tmp_broadcast = array(dh) if isinstance(dh,\n",
      "                                            ndarray) or isscalar(dh) else -1\n",
      "    dj = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 4, 32, 1)))\n",
      "    dk = concatenate((di, dj), axis=4)\n",
      "    dl = array(dk).reshape((1, 2048, 4, 64))\n",
      "    dm = transpose(bv, axes=(0, 2, 1, 3))\n",
      "    dn = transpose(dl, axes=(0, 2, 1, 3))\n",
      "    do = transpose(ad, axes=(0, 2, 1, 3))\n",
      "    dp = k <= 0\n",
      "    dq = k + 2048\n",
      "    dr = select([dp, invert(dp)], [dq, k])\n",
      "    tmp_broadcast = array(dr) if isinstance(dr,\n",
      "                                            ndarray) or isscalar(dr) else -1\n",
      "    ds = array(jax.numpy.broadcast_to(tmp_broadcast, (2048, 1)))\n",
      "    dt = array(c).astype(float32)\n",
      "    du = jax.lax.scatter(dt,\n",
      "                         ds,\n",
      "                         dn,\n",
      "                         dimension_numbers=jax.lax.ScatterDimensionNumbers(\n",
      "                             update_window_dims=(0, 1, 3),\n",
      "                             inserted_window_dims=(2, ),\n",
      "                             scatter_dims_to_operand_dims=(2, ),\n",
      "                             operand_batching_dims=(),\n",
      "                             scatter_indices_batching_dims=()))\n",
      "    dv = array(du).astype(bfloat16)\n",
      "    dw = k <= 0\n",
      "    dx = k + 2048\n",
      "    dy = select([dw, invert(dw)], [dx, k])\n",
      "    tmp_broadcast = array(dy) if isinstance(dy,\n",
      "                                            ndarray) or isscalar(dy) else -1\n",
      "    dz = array(jax.numpy.broadcast_to(tmp_broadcast, (2048, 1)))\n",
      "    ea = array(d).astype(float32)\n",
      "    eb = jax.lax.scatter(ea,\n",
      "                         dz,\n",
      "                         do,\n",
      "                         dimension_numbers=jax.lax.ScatterDimensionNumbers(\n",
      "                             update_window_dims=(0, 1, 3),\n",
      "                             inserted_window_dims=(2, ),\n",
      "                             scatter_dims_to_operand_dims=(2, ),\n",
      "                             operand_batching_dims=(),\n",
      "                             scatter_indices_batching_dims=()))\n",
      "    ec = array(eb).astype(bfloat16)\n",
      "    tmp_broadcast = array(dv) if isinstance(dv,\n",
      "                                            ndarray) or isscalar(dv) else -1\n",
      "    ed = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 4, 1, 2048, 64)))\n",
      "    ee = array(ed).reshape((1, 4, 2048, 64))\n",
      "    tmp_broadcast = array(ec) if isinstance(ec,\n",
      "                                            ndarray) or isscalar(ec) else -1\n",
      "    ef = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 4, 1, 2048, 64)))\n",
      "    eg = array(ef).reshape((1, 4, 2048, 64))\n",
      "    tmp_broadcast = array(0.0) if isinstance(0.0,\n",
      "                                             ndarray) or isscalar(0.0) else -1\n",
      "    eh = array(jax.numpy.broadcast_to(tmp_broadcast, (2048, 2048)))\n",
      "    ei = ~m\n",
      "\n",
      "    def local_f0(ek, el, em):\n",
      "        en = array(el).astype(float32)\n",
      "        tmp_broadcast = array(en) if isinstance(\n",
      "            en, ndarray) or isscalar(en) else -1\n",
      "        eo = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 1, 2048, 2048)))\n",
      "        tmp_broadcast = array(em) if isinstance(\n",
      "            em, ndarray) or isscalar(em) else -1\n",
      "        ep = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 1, 2048, 2048)))\n",
      "        eq = select([ek, invert(ek)], [eo, ep])\n",
      "        return eq\n",
      "\n",
      "    ej = jax.pjit(local_f0, donated_invars=(False, False, False))(ei, -inf, eh)\n",
      "    er = transpose(ee, axes=(0, 1, 3, 2))\n",
      "    es = array(dm).reshape((4, 2048, 64))\n",
      "    et = array(er).reshape((4, 64, 2048))\n",
      "    eu = tensordot(es, et, axes=((2, ), (1, )))\n",
      "    ev = array(eu).reshape((1, 4, 2048, 2048))\n",
      "    ew = ev * 0.125\n",
      "    ex = ej * 1.0\n",
      "    ey = ew + ex\n",
      "    ez = max(ey)\n",
      "    fa = array([max(-inf)])\n",
      "    tmp_broadcast = array(fa) if isinstance(fa,\n",
      "                                            ndarray) or isscalar(fa) else -1\n",
      "    fb = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 4, 2048, 1)))\n",
      "    fc = fb  # stop grad\n",
      "    fd = ey - fc\n",
      "    fe = exp(fd)\n",
      "    ff = sum(fe, axis=(3, ))\n",
      "    tmp_broadcast = array(ff) if isinstance(ff,\n",
      "                                            ndarray) or isscalar(ff) else -1\n",
      "    fg = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 4, 2048, 1)))\n",
      "    fh = fe / fg\n",
      "    fi = array(fh).reshape((4, 2048, 2048))\n",
      "    fj = array(eg).reshape((4, 2048, 64))\n",
      "    fk = tensordot(fi, fj, axes=((2, ), (1, )))\n",
      "    fl = array(fk).reshape((1, 4, 2048, 64))\n",
      "    fm = transpose(fl, axes=(0, 2, 1, 3))\n",
      "    fn = array(fm).reshape((1, 2048, 256))\n",
      "    fo = tensordot(fn, b, axes=((2, ), (1, )))\n",
      "    fp = fo * 1.0\n",
      "    fq = j + fp\n",
      "    fr = fq * fq\n",
      "    fs = sum(fr, axis=(2, ))\n",
      "    tmp_broadcast = array(fs) if isinstance(fs,\n",
      "                                            ndarray) or isscalar(fs) else -1\n",
      "    ft = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 2048, 1)))\n",
      "    fu = ft / 256.0\n",
      "    fv = fu + 9.999999747378752e-06\n",
      "    fw = jax.lax.rsqrt(fv)\n",
      "    fx = fq * fw\n",
      "    fy = array(h).astype(float32)\n",
      "    tmp_broadcast = array(fy) if isinstance(fy,\n",
      "                                            ndarray) or isscalar(fy) else -1\n",
      "    fz = array(jax.numpy.broadcast_to(tmp_broadcast, (1, 1, 256)))\n",
      "    ga = fx * fz\n",
      "    gb = tensordot(ga, e, axes=((2, ), (1, )))\n",
      "\n",
      "    def local_f1(gd):\n",
      "        ge = 1.0 / (1.0 + exp(-gd))\n",
      "        gf = gd * ge\n",
      "        return gf\n",
      "\n",
      "    gc = jax.pjit(local_f1, donated_invars=(False, ))(gb)\n",
      "    gg = tensordot(ga, f, axes=((2, ), (1, )))\n",
      "    gh = gc * gg\n",
      "    gi = tensordot(gh, g, axes=((2, ), (1, )))\n",
      "    gj = gi * 1.0\n",
      "    gk = fq + gj\n",
      "    return gk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_py = decompile_jaxpr(jaxpr)\n",
    "print(llama_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ad4ed",
   "metadata": {},
   "source": [
    "Not very readable. If we're going to check in the converted JAX code, it will\n",
    "probably take a lot of cleaning up. One might as well write JAX by hand or just\n",
    "use `torchax` at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09ab38",
   "metadata": {},
   "source": [
    "# Future ideas\n",
    "\n",
    "- Can we intercept `jax`, `jax.numpy`, and `jax.lax` ops instead of JAX primitives?\n",
    "  - Assumption: these ops are pure.\n",
    "  - Benefit: improves readability.\n",
    "- Can we inspect the stack to find better variable names?\n",
    "  - Approach: find which variable in the stack at the model file corresponds to\n",
    "    which `jaxpr` variable.\n",
    "- Can we recreate the function structure?\n",
    "  - Approach: figure out when the Python interpreter enters another function when\n",
    "    running the model file.\n",
    "  - Translate function by function.\n",
    "- Can we use LLM to clean things up?\n",
    "- Instead of generating JAX code from scratch, what if we created JAX comments\n",
    "  next to each line in the PyTorch model file?\n",
    "\n",
    "This will be a non-trivial project. But the foundation is laid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
