{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we study the LazyTensor IR lowering of a toy decoder layer with\n",
    "or without AOTAutograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-slice sharding: mesh=(4, 2)\n",
      "Building model\n",
      "> [2D] Sharding tensor embed_tokens.weight torch.Size([8192, 1024])\n",
      "embed_tokens.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "layers.0.self_attn.q_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.k_proj.weight torch.Size([512, 1024])\n",
      "layers.0.self_attn.k_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.v_proj.weight torch.Size([512, 1024])\n",
      "layers.0.self_attn.v_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.0.self_attn.o_proj.weight torch.Size([1024, 1024])\n",
      "layers.0.self_attn.o_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "layers.0.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "layers.0.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.0.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "layers.0.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.0.input_layernorm.weight torch.Size([1024])\n",
      "layers.0.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.0.post_attention_layernorm.weight torch.Size([1024])\n",
      "layers.0.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "layers.1.self_attn.q_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.k_proj.weight torch.Size([512, 1024])\n",
      "layers.1.self_attn.k_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.v_proj.weight torch.Size([512, 1024])\n",
      "layers.1.self_attn.v_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.1.self_attn.o_proj.weight torch.Size([1024, 1024])\n",
      "layers.1.self_attn.o_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "layers.1.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "layers.1.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.1.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "layers.1.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.1.input_layernorm.weight torch.Size([1024])\n",
      "layers.1.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.1.post_attention_layernorm.weight torch.Size([1024])\n",
      "layers.1.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "layers.2.self_attn.q_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.k_proj.weight torch.Size([512, 1024])\n",
      "layers.2.self_attn.k_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.v_proj.weight torch.Size([512, 1024])\n",
      "layers.2.self_attn.v_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.2.self_attn.o_proj.weight torch.Size([1024, 1024])\n",
      "layers.2.self_attn.o_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.gate_proj.weight torch.Size([4096, 1024])\n",
      "layers.2.mlp.gate_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.up_proj.weight torch.Size([4096, 1024])\n",
      "layers.2.mlp.up_proj.weight {devices=[2,4]0,2,4,6,1,3,5,7}\n",
      "> [2D] Sharding tensor layers.2.mlp.down_proj.weight torch.Size([1024, 4096])\n",
      "layers.2.mlp.down_proj.weight {devices=[4,2]0,1,2,3,4,5,6,7}\n",
      "> [2D] Sharding tensor layers.2.input_layernorm.weight torch.Size([1024])\n",
      "layers.2.input_layernorm.weight \n",
      "> [2D] Sharding tensor layers.2.post_attention_layernorm.weight torch.Size([1024])\n",
      "layers.2.post_attention_layernorm.weight \n",
      "> [2D] Sharding tensor norm.weight torch.Size([1024])\n",
      "norm.weight \n",
      "> [2D] Sharding tensor lm_head.weight torch.Size([8192, 1024])\n",
      "lm_head.weight {devices=[8,1]0,2,4,6,1,3,5,7}\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "from decoder_only_model import DecoderOnlyConfig, DecoderOnlyModel\n",
    "from aot_flash_attention import flash_attention_2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics\n",
    "import torch\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.debug.profiler as xp\n",
    "from torch_xla import runtime as xr\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Sharding\n",
    "num_devices = xr.global_runtime_device_count()\n",
    "tensor_axis = 2\n",
    "fsdp_axis = num_devices // tensor_axis\n",
    "mesh_shape = (fsdp_axis, tensor_axis)\n",
    "print(f\"Single-slice sharding: mesh={mesh_shape}\")\n",
    "spmd_mesh = xs.Mesh(\n",
    "    list(range(num_devices)), mesh_shape, ('fsdp', 'tensor'))\n",
    "xs.set_global_mesh(spmd_mesh)\n",
    "xr.use_spmd()\n",
    "\n",
    "print(\"Building model\")\n",
    "device = torch_xla.device()\n",
    "config = DecoderOnlyConfig(\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=3,\n",
    "    use_flash_attention=True)\n",
    "config.intermediate_size = 4096\n",
    "config.vocab_size = 8192\n",
    "model = DecoderOnlyModel(config=config).bfloat16().to(device)\n",
    "batch_size = 32\n",
    "sequence_length = 1024\n",
    "\n",
    "model.use_offload_(False)\n",
    "model.use_scan_(True)\n",
    "for layer in model.layers:\n",
    "  layer.self_attn.flash_attention_impl = flash_attention_2  # type: ignore\n",
    "\n",
    "# Mark model weights to be sharded\n",
    "for name, param in chain(model.named_parameters(), model.named_buffers()):\n",
    "  print('> [2D] Sharding tensor', name, param.shape)\n",
    "\n",
    "  # Here we intentionally skip layernorm and moe.gate weights given they are small.\n",
    "  if 'embed_tokens' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "  elif 'q_proj' in name or 'k_proj' in name or 'v_proj' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "  elif 'o_proj' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "  elif 'gate_proj' in name or 'up_proj' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "  elif 'down_proj' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "  elif 'lm_head' in name:\n",
    "    xs.mark_sharding(param, spmd_mesh, (('tensor', 'fsdp'), None))\n",
    "\n",
    "  print(f'{name} {torch_xla._XLAC._get_xla_sharding_spec(param)}')\n",
    "\n",
    "# Generate random input_ids within the range of the vocabulary size\n",
    "input_ids = torch.randint(\n",
    "    0, config.vocab_size, (batch_size, sequence_length), device=device)\n",
    "# Shard the input data too.\n",
    "xs.mark_sharding(input_ids, spmd_mesh, ('fsdp', None))\n",
    "xs.set_global_mesh(spmd_mesh)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "torch_xla.sync(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decoder_only_model.DecoderLayer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_layer = model.layers[0]\n",
    "type(decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([32, 1024, 1024]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "inputs_embeds = model.embed_tokens(input_ids).clone().detach()\n",
    "torch_xla.sync(wait=True)\n",
    "print(type(inputs_embeds), inputs_embeds.shape, inputs_embeds.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower a toy decoder layer into LazyTensor IR directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = bf16[] prim::Constant(), xla_shape=bf16[]\n",
      "  %1 = bf16[1024,4096]{1,0} xla::device_data(), xla_shape=bf16[1024,4096]{1,0}\n",
      "  %2 = bf16[4096,1024]{0,1} aten::permute(%1), xla_shape=bf16[4096,1024]{0,1}\n",
      "  %3 = bf16[4096,1024]{1,0} xla::device_data(), xla_shape=bf16[4096,1024]{1,0}\n",
      "  %4 = bf16[1024,4096]{0,1} aten::permute(%3), xla_shape=bf16[1024,4096]{0,1}\n",
      "  %5 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %6 = f32[] xla::device_data(), xla_shape=f32[]\n",
      "  %7 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %8 = bf16[] prim::Constant(), xla_shape=bf16[]\n",
      "  %9 = bf16[1024,1024]{1,0} xla::device_data(), xla_shape=bf16[1024,1024]{1,0}\n",
      "  %10 = bf16[1024,1024]{0,1} aten::permute(%9), xla_shape=bf16[1024,1024]{0,1}\n",
      "  %11 = bf16[512,1024]{1,0} xla::device_data(), xla_shape=bf16[512,1024]{1,0}\n",
      "  %12 = bf16[1024,512]{0,1} aten::permute(%11), xla_shape=bf16[1024,512]{0,1}\n",
      "  %13 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %14 = f32[] xla::device_data(), xla_shape=f32[]\n",
      "  %15 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %16 = bf16[32,1024,1024]{2,1,0} xla::device_data(), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %17 = f32[32,1024,1024]{2,1,0} xla::cast(%16), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %18 = f32[32,1024,1024]{2,1,0} aten::pow(%17, %15), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %19 = f32[32,1024,1]{2,1,0} aten::mean(%18), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %20 = f32[32,1024,1]{2,1,0} aten::add(%19, %14, %13), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %21 = f32[32,1024,1]{2,1,0} aten::rsqrt(%20), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %22 = f32[32,1024,1024]{2,1,0} aten::mul(%17, %21), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %23 = bf16[32,1024,1024]{2,1,0} xla::cast(%22), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %24 = f32[32,1024,1024]{2,1,0} xla::cast(%23), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %25 = bf16[1024]{0} xla::device_data(), xla_shape=bf16[1024]{0}\n",
      "  %26 = f32[1024]{0} xla::cast(%25), xla_shape=f32[1024]{0}\n",
      "  %27 = f32[32,1024,1024]{2,1,0} aten::mul(%26, %24), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %28 = bf16[32,1024,1024]{2,1,0} xla::cast(%27), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %29 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %30 = bf16[32768,512]{1,0} aten::mm(%29, %12), xla_shape=bf16[32768,512]{1,0}\n",
      "  %31 = bf16[32,1024,512]{2,1,0} aten::view(%30), xla_shape=bf16[32,1024,512]{2,1,0}\n",
      "  %32 = bf16[32,1024,4,128]{3,2,1,0} aten::view(%31), xla_shape=bf16[32,1024,4,128]{3,2,1,0}\n",
      "  %33 = bf16[32,4,1024,128]{3,1,2,0} aten::permute(%32), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %34 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%33), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %35 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%34), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %36 = bf16[32,4,1,1024,128]{4,3,2,1,0} aten::view(%35), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %37 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%36), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %38 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%37), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %39 = bf16[32,4,2,1024,128]{4,3,2,1,0} aten::expand(%38), xla_shape=bf16[32,4,2,1024,128]{4,3,2,1,0}\n",
      "  %40 = bf16[32,8,1024,128]{3,2,1,0} aten::view(%39), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %41 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%40), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %42 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%41), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %43 = bf16[512,1024]{1,0} xla::device_data(), xla_shape=bf16[512,1024]{1,0}\n",
      "  %44 = bf16[1024,512]{0,1} aten::permute(%43), xla_shape=bf16[1024,512]{0,1}\n",
      "  %45 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %46 = bf16[32768,512]{1,0} aten::mm(%45, %44), xla_shape=bf16[32768,512]{1,0}\n",
      "  %47 = bf16[32,1024,512]{2,1,0} aten::view(%46), xla_shape=bf16[32,1024,512]{2,1,0}\n",
      "  %48 = bf16[32,1024,4,128]{3,2,1,0} aten::view(%47), xla_shape=bf16[32,1024,4,128]{3,2,1,0}\n",
      "  %49 = bf16[32,4,1024,128]{3,1,2,0} aten::permute(%48), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %50 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%49), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %51 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%50), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %52 = bf16[32,4,1,1024,128]{4,3,2,1,0} aten::view(%51), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %53 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%52), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %54 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%53), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %55 = bf16[32,4,2,1024,128]{4,3,2,1,0} aten::expand(%54), xla_shape=bf16[32,4,2,1024,128]{4,3,2,1,0}\n",
      "  %56 = bf16[32,8,1024,128]{3,2,1,0} aten::view(%55), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %57 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%56), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %58 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%57), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %59 = bf16[1024,1024]{1,0} xla::device_data(), xla_shape=bf16[1024,1024]{1,0}\n",
      "  %60 = bf16[1024,1024]{0,1} aten::permute(%59), xla_shape=bf16[1024,1024]{0,1}\n",
      "  %61 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %62 = bf16[32768,1024]{1,0} aten::mm(%61, %60), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %63 = bf16[32,1024,1024]{2,1,0} aten::view(%62), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %64 = bf16[32,1024,8,128]{3,2,1,0} aten::view(%63), xla_shape=bf16[32,1024,8,128]{3,2,1,0}\n",
      "  %65 = bf16[32,8,1024,128]{3,1,2,0} aten::permute(%64), xla_shape=bf16[32,8,1024,128]{3,1,2,0}\n",
      "  %66 = bf16[32,8,1024,128]{3,1,2,0} xla::custom_sharding(%65), xla_shape=bf16[32,8,1024,128]{3,1,2,0}\n",
      "  %67 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%66), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %68 = (bf16[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}) xla::tpu_custom_call(%67, %58, %42), num_outputs=3, xla_shape=(bf16[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0})\n",
      "  %69 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%68.0), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %70 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%69), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %71 = bf16[32,1024,8,128]{3,1,2,0} aten::permute(%70), xla_shape=bf16[32,1024,8,128]{3,1,2,0}\n",
      "  %72 = bf16[32,1024,1024]{2,1,0} aten::view(%71), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %73 = bf16[32768,1024]{1,0} aten::view(%72), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %74 = bf16[32768,1024]{1,0} aten::mm(%73, %10), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %75 = bf16[32,1024,1024]{2,1,0} aten::view(%74), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %76 = bf16[32,1024,1024]{2,1,0} aten::add(%16, %75, %8), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %77 = f32[32,1024,1024]{2,1,0} xla::cast(%76), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %78 = f32[32,1024,1024]{2,1,0} aten::pow(%77, %7), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %79 = f32[32,1024,1]{2,1,0} aten::mean(%78), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %80 = f32[32,1024,1]{2,1,0} aten::add(%79, %6, %5), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %81 = f32[32,1024,1]{2,1,0} aten::rsqrt(%80), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %82 = f32[32,1024,1024]{2,1,0} aten::mul(%77, %81), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %83 = bf16[32,1024,1024]{2,1,0} xla::cast(%82), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %84 = f32[32,1024,1024]{2,1,0} xla::cast(%83), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %85 = bf16[1024]{0} xla::device_data(), xla_shape=bf16[1024]{0}\n",
      "  %86 = f32[1024]{0} xla::cast(%85), xla_shape=f32[1024]{0}\n",
      "  %87 = f32[32,1024,1024]{2,1,0} aten::mul(%86, %84), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %88 = bf16[32,1024,1024]{2,1,0} xla::cast(%87), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %89 = bf16[32768,1024]{1,0} aten::view(%88), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %90 = bf16[32768,4096]{1,0} aten::mm(%89, %4), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %91 = bf16[32,1024,4096]{2,1,0} aten::view(%90), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %92 = f32[32,1024,4096]{2,1,0} xla::cast(%91), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %93 = bf16[4096,1024]{1,0} xla::device_data(), xla_shape=bf16[4096,1024]{1,0}\n",
      "  %94 = bf16[1024,4096]{0,1} aten::permute(%93), xla_shape=bf16[1024,4096]{0,1}\n",
      "  %95 = bf16[32768,1024]{1,0} aten::view(%88), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %96 = bf16[32768,4096]{1,0} aten::mm(%95, %94), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %97 = bf16[32,1024,4096]{2,1,0} aten::view(%96), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %98 = bf16[32,1024,4096]{2,1,0} aten::silu(%97), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %99 = f32[32,1024,4096]{2,1,0} xla::cast(%98), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %100 = f32[32,1024,4096]{2,1,0} aten::mul(%99, %92), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %101 = bf16[32,1024,4096]{2,1,0} xla::cast(%100), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %102 = bf16[32768,4096]{1,0} aten::view(%101), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %103 = bf16[32768,1024]{1,0} aten::mm(%102, %2), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %104 = bf16[32,1024,1024]{2,1,0} aten::view(%103), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %105 = bf16[32,1024,1024]{2,1,0} aten::add(%76, %104, %0), xla_shape=bf16[32,1024,1024]{2,1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_out = decoder_layer(inputs_embeds)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([decoder_out]))\n",
    "torch_xla.sync(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: DeviceLockWait\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 034.990us\n",
      "  ValueRate: 05s887ms871.508us / second\n",
      "  Rate: 279330 / second\n",
      "  Percentiles: 1%=008.080us; 5%=008.080us; 10%=008.080us; 20%=008.080us; 50%=026.910us; 80%=026.910us; 90%=026.910us; 95%=026.910us; 99%=026.910us\n",
      "Metric: InputOutputAliasCount\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 0.00\n",
      "  Percentiles: 1%=0.00; 5%=0.00; 10%=0.00; 20%=0.00; 50%=0.00; 80%=0.00; 90%=0.00; 95%=0.00; 99%=0.00\n",
      "Metric: LazyTracing\n",
      "  TotalSamples: 132\n",
      "  Accumulator: 023ms453.661us\n",
      "  ValueRate: 050ms623.296us / second\n",
      "  Rate: 279.286 / second\n",
      "  Percentiles: 1%=007.330us; 5%=008.140us; 10%=023.490us; 20%=058.730us; 50%=089.600us; 80%=253.070us; 90%=360.600us; 95%=583.459us; 99%=001ms129.530us\n",
      "Metric: TensorToData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 990.460us\n",
      "  Percentiles: 1%=990.460us; 5%=990.460us; 10%=990.460us; 20%=990.460us; 50%=990.460us; 80%=990.460us; 90%=990.460us; 95%=990.460us; 99%=990.460us\n",
      "Metric: TensorsGraphSize\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 112.00\n",
      "  Percentiles: 1%=112.00; 5%=112.00; 10%=112.00; 20%=112.00; 50%=112.00; 80%=112.00; 90%=112.00; 95%=112.00; 99%=112.00\n",
      "Metric: UnwrapXlaData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 019.390us\n",
      "  Percentiles: 1%=019.390us; 5%=019.390us; 10%=019.390us; 20%=019.390us; 50%=019.390us; 80%=019.390us; 90%=019.390us; 95%=019.390us; 99%=019.390us\n",
      "Metric: WrapXlaData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 011.320us\n",
      "  Percentiles: 1%=011.320us; 5%=011.320us; 10%=011.320us; 20%=011.320us; 50%=011.320us; 80%=011.320us; 90%=011.320us; 95%=011.320us; 99%=011.320us\n",
      "Counter: CreateOpSharding\n",
      "  Value: 5\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 101\n",
      "Counter: DestroyLtcTensor\n",
      "  Value: 66\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 66\n",
      "Counter: DeviceDataCacheMiss\n",
      "  Value: 1\n",
      "Counter: ExecuteReplicated\n",
      "  Value: 1\n",
      "Counter: SetShardingSpec\n",
      "  Value: 49\n",
      "Counter: UncachedCompile\n",
      "  Value: 1\n",
      "Counter: XlaMarkSharding\n",
      "  Value: 6\n",
      "Counter: xla::_copy_from\n",
      "  Value: 13\n",
      "Counter: xla::_to_copy\n",
      "  Value: 13\n",
      "Counter: xla::_unsafe_view\n",
      "  Value: 7\n",
      "Counter: xla::add\n",
      "  Value: 4\n",
      "Counter: xla::as_strided_copy\n",
      "  Value: 4\n",
      "Counter: xla::clone\n",
      "  Value: 7\n",
      "Counter: xla::detach_copy\n",
      "  Value: 9\n",
      "Counter: xla::empty_symint\n",
      "  Value: 13\n",
      "Counter: xla::expand_copy_symint\n",
      "  Value: 2\n",
      "Counter: xla::mean\n",
      "  Value: 2\n",
      "Counter: xla::mm\n",
      "  Value: 7\n",
      "Counter: xla::mul\n",
      "  Value: 5\n",
      "Counter: xla::pow\n",
      "  Value: 2\n",
      "Counter: xla::rsqrt\n",
      "  Value: 2\n",
      "Counter: xla::silu\n",
      "  Value: 1\n",
      "Counter: xla::slice_copy\n",
      "  Value: 8\n",
      "Counter: xla::t_copy\n",
      "  Value: 7\n",
      "Counter: xla::transpose_copy\n",
      "  Value: 4\n",
      "Counter: xla::unsqueeze_copy\n",
      "  Value: 2\n",
      "Counter: xla::view_copy_symint\n",
      "  Value: 20\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 01s498ms266.831us\n",
      "  Percentiles: 1%=01s498ms266.831us; 5%=01s498ms266.831us; 10%=01s498ms266.831us; 20%=01s498ms266.831us; 50%=01s498ms266.831us; 80%=01s498ms266.831us; 90%=01s498ms266.831us; 95%=01s498ms266.831us; 99%=01s498ms266.831us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 117ms760.890us\n",
      "  Percentiles: 1%=117ms760.890us; 5%=117ms760.890us; 10%=117ms760.890us; 20%=117ms760.890us; 50%=117ms760.890us; 80%=117ms760.890us; 90%=117ms760.890us; 95%=117ms760.890us; 99%=117ms760.890us\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 32.00B\n",
      "  Percentiles: 1%=32.00B; 5%=32.00B; 10%=32.00B; 20%=32.00B; 50%=32.00B; 80%=32.00B; 90%=32.00B; 95%=32.00B; 99%=32.00B\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 288.300us\n",
      "  Percentiles: 1%=288.300us; 5%=288.300us; 10%=288.300us; 20%=288.300us; 50%=288.300us; 80%=288.300us; 90%=288.300us; 95%=288.300us; 99%=288.300us\n",
      "Counter: CreateCompileHandles\n",
      "  Value: 1\n",
      "Counter: CreateDataHandles\n",
      "  Value: 8\n",
      "Counter: MarkStep\n",
      "  Value: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.metrics_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower a toy decoder layer to ATen ops via AOTAutograd first, then into LazyTensor IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOTAutograd graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10):\n",
      "    offload_name = torch.ops.xla.offload_name.default(primals_1, 'decoder_input');  primals_1 = None\n",
      "    _to_copy = torch.ops.aten._to_copy.default(offload_name, dtype = torch.float32)\n",
      "    pow_1 = torch.ops.aten.pow.Tensor_Scalar(_to_copy, 2)\n",
      "    mean = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n",
      "    add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None\n",
      "    rsqrt = torch.ops.aten.rsqrt.default(add);  add = None\n",
      "    mul = torch.ops.aten.mul.Tensor(_to_copy, rsqrt);  _to_copy = rsqrt = None\n",
      "    _to_copy_1 = torch.ops.aten._to_copy.default(mul, dtype = torch.bfloat16);  mul = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(primals_9, _to_copy_1);  primals_9 = None\n",
      "    t = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    view = torch.ops.aten.view.default(mul_1, [32768, 1024])\n",
      "    mm = torch.ops.aten.mm.default(view, t)\n",
      "    _unsafe_view = torch.ops.aten._unsafe_view.default(mm, [32, 1024, 1024]);  mm = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_3);  primals_3 = None\n",
      "    view_1 = torch.ops.aten.view.default(mul_1, [32768, 1024])\n",
      "    mm_1 = torch.ops.aten.mm.default(view_1, t_1)\n",
      "    _unsafe_view_1 = torch.ops.aten._unsafe_view.default(mm_1, [32, 1024, 512]);  mm_1 = None\n",
      "    t_2 = torch.ops.aten.t.default(primals_4);  primals_4 = None\n",
      "    view_2 = torch.ops.aten.view.default(mul_1, [32768, 1024]);  mul_1 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(view_2, t_2)\n",
      "    _unsafe_view_2 = torch.ops.aten._unsafe_view.default(mm_2, [32, 1024, 512]);  mm_2 = None\n",
      "    view_3 = torch.ops.aten.view.default(_unsafe_view, [32, 1024, 8, 128]);  _unsafe_view = None\n",
      "    transpose = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None\n",
      "    view_4 = torch.ops.aten.view.default(_unsafe_view_1, [32, 1024, 4, 128]);  _unsafe_view_1 = None\n",
      "    transpose_1 = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
      "    view_5 = torch.ops.aten.view.default(_unsafe_view_2, [32, 1024, 4, 128]);  _unsafe_view_2 = None\n",
      "    transpose_2 = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
      "    slice_1 = torch.ops.aten.slice.Tensor(transpose_1, 0, 0, 9223372036854775807);  transpose_1 = None\n",
      "    slice_2 = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None\n",
      "    unsqueeze = torch.ops.aten.unsqueeze.default(slice_2, 2);  slice_2 = None\n",
      "    slice_3 = torch.ops.aten.slice.Tensor(unsqueeze, 3, 0, 9223372036854775807);  unsqueeze = None\n",
      "    slice_4 = torch.ops.aten.slice.Tensor(slice_3, 4, 0, 9223372036854775807);  slice_3 = None\n",
      "    expand = torch.ops.aten.expand.default(slice_4, [32, 4, 2, 1024, 128]);  slice_4 = None\n",
      "    clone = torch.ops.aten.clone.default(expand, memory_format = torch.contiguous_format);  expand = None\n",
      "    _unsafe_view_3 = torch.ops.aten._unsafe_view.default(clone, [32, 8, 1024, 128]);  clone = None\n",
      "    slice_5 = torch.ops.aten.slice.Tensor(transpose_2, 0, 0, 9223372036854775807);  transpose_2 = None\n",
      "    slice_6 = torch.ops.aten.slice.Tensor(slice_5, 1, 0, 9223372036854775807);  slice_5 = None\n",
      "    unsqueeze_1 = torch.ops.aten.unsqueeze.default(slice_6, 2);  slice_6 = None\n",
      "    slice_7 = torch.ops.aten.slice.Tensor(unsqueeze_1, 3, 0, 9223372036854775807);  unsqueeze_1 = None\n",
      "    slice_8 = torch.ops.aten.slice.Tensor(slice_7, 4, 0, 9223372036854775807);  slice_7 = None\n",
      "    expand_1 = torch.ops.aten.expand.default(slice_8, [32, 4, 2, 1024, 128]);  slice_8 = None\n",
      "    clone_1 = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None\n",
      "    _unsafe_view_4 = torch.ops.aten._unsafe_view.default(clone_1, [32, 8, 1024, 128]);  clone_1 = None\n",
      "    clone_2 = torch.ops.aten.clone.default(transpose);  transpose = None\n",
      "    detach = torch.ops.aten.detach.default(clone_2);  clone_2 = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    clone_3 = torch.ops.aten.clone.default(_unsafe_view_3);  _unsafe_view_3 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(clone_3);  clone_3 = None\n",
      "    detach_4 = torch.ops.aten.detach.default(detach_3);  detach_3 = None\n",
      "    detach_5 = torch.ops.aten.detach.default(detach_4);  detach_4 = None\n",
      "    clone_4 = torch.ops.aten.clone.default(_unsafe_view_4);  _unsafe_view_4 = None\n",
      "    detach_6 = torch.ops.aten.detach.default(clone_4);  clone_4 = None\n",
      "    detach_7 = torch.ops.aten.detach.default(detach_6);  detach_6 = None\n",
      "    detach_8 = torch.ops.aten.detach.default(detach_7);  detach_7 = None\n",
      "    fa_custom_forward = torch.ops.xla.fa_custom_forward.default(detach_2, detach_5, detach_8);  detach_2 = detach_5 = detach_8 = None\n",
      "    getitem = fa_custom_forward[0]\n",
      "    getitem_1 = fa_custom_forward[1]\n",
      "    getitem_2 = fa_custom_forward[2]\n",
      "    getitem_3 = fa_custom_forward[3]\n",
      "    getitem_4 = fa_custom_forward[4]\n",
      "    getitem_5 = fa_custom_forward[5];  fa_custom_forward = None\n",
      "    transpose_3 = torch.ops.aten.transpose.int(getitem, 1, 2)\n",
      "    clone_5 = torch.ops.aten.clone.default(transpose_3, memory_format = torch.contiguous_format);  transpose_3 = None\n",
      "    view_6 = torch.ops.aten.view.default(clone_5, [32, 1024, 1024]);  clone_5 = None\n",
      "    t_3 = torch.ops.aten.t.default(primals_5);  primals_5 = None\n",
      "    view_7 = torch.ops.aten.view.default(view_6, [32768, 1024]);  view_6 = None\n",
      "    mm_3 = torch.ops.aten.mm.default(view_7, t_3)\n",
      "    _unsafe_view_5 = torch.ops.aten._unsafe_view.default(mm_3, [32, 1024, 1024]);  mm_3 = None\n",
      "    add_1 = torch.ops.aten.add.Tensor(offload_name, _unsafe_view_5);  offload_name = _unsafe_view_5 = None\n",
      "    _to_copy_2 = torch.ops.aten._to_copy.default(add_1, dtype = torch.float32)\n",
      "    pow_2 = torch.ops.aten.pow.Tensor_Scalar(_to_copy_2, 2)\n",
      "    mean_1 = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None\n",
      "    add_2 = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None\n",
      "    rsqrt_1 = torch.ops.aten.rsqrt.default(add_2);  add_2 = None\n",
      "    mul_2 = torch.ops.aten.mul.Tensor(_to_copy_2, rsqrt_1)\n",
      "    _to_copy_3 = torch.ops.aten._to_copy.default(mul_2, dtype = torch.bfloat16);  mul_2 = None\n",
      "    mul_3 = torch.ops.aten.mul.Tensor(primals_10, _to_copy_3)\n",
      "    t_4 = torch.ops.aten.t.default(primals_7);  primals_7 = None\n",
      "    view_8 = torch.ops.aten.view.default(mul_3, [32768, 1024])\n",
      "    mm_4 = torch.ops.aten.mm.default(view_8, t_4)\n",
      "    _unsafe_view_6 = torch.ops.aten._unsafe_view.default(mm_4, [32, 1024, 4096]);  mm_4 = None\n",
      "    t_5 = torch.ops.aten.t.default(primals_6);  primals_6 = None\n",
      "    view_9 = torch.ops.aten.view.default(mul_3, [32768, 1024]);  mul_3 = None\n",
      "    mm_5 = torch.ops.aten.mm.default(view_9, t_5)\n",
      "    _unsafe_view_7 = torch.ops.aten._unsafe_view.default(mm_5, [32, 1024, 4096]);  mm_5 = None\n",
      "    silu = torch.ops.aten.silu.default(_unsafe_view_7)\n",
      "    mul_4 = torch.ops.aten.mul.Tensor(silu, _unsafe_view_6)\n",
      "    t_6 = torch.ops.aten.t.default(primals_8);  primals_8 = None\n",
      "    view_10 = torch.ops.aten.view.default(mul_4, [32768, 4096]);  mul_4 = None\n",
      "    mm_6 = torch.ops.aten.mm.default(view_10, t_6)\n",
      "    _unsafe_view_8 = torch.ops.aten._unsafe_view.default(mm_6, [32, 1024, 1024]);  mm_6 = None\n",
      "    add_3 = torch.ops.aten.add.Tensor(add_1, _unsafe_view_8);  add_1 = _unsafe_view_8 = None\n",
      "    return (add_3, primals_10, _to_copy_1, t, view, t_1, view_1, t_2, view_2, getitem, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5, t_3, view_7, _to_copy_2, rsqrt_1, _to_copy_3, t_4, view_8, _unsafe_view_6, t_5, view_9, _unsafe_view_7, silu, t_6, view_10)\n",
      "    \n",
      "IR {\n",
      "  %0 = bf16[] prim::Constant(), xla_shape=bf16[]\n",
      "  %1 = bf16[1024,4096]{1,0} xla::device_data(), xla_shape=bf16[1024,4096]{1,0}\n",
      "  %2 = bf16[4096,1024]{0,1} aten::permute(%1), xla_shape=bf16[4096,1024]{0,1}\n",
      "  %3 = bf16[4096,1024]{1,0} xla::device_data(), xla_shape=bf16[4096,1024]{1,0}\n",
      "  %4 = bf16[1024,4096]{0,1} aten::permute(%3), xla_shape=bf16[1024,4096]{0,1}\n",
      "  %5 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %6 = f32[] xla::device_data(), xla_shape=f32[]\n",
      "  %7 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %8 = bf16[] prim::Constant(), xla_shape=bf16[]\n",
      "  %9 = bf16[1024,1024]{1,0} xla::device_data(), xla_shape=bf16[1024,1024]{1,0}\n",
      "  %10 = bf16[1024,1024]{0,1} aten::permute(%9), xla_shape=bf16[1024,1024]{0,1}\n",
      "  %11 = bf16[512,1024]{1,0} xla::device_data(), xla_shape=bf16[512,1024]{1,0}\n",
      "  %12 = bf16[1024,512]{0,1} aten::permute(%11), xla_shape=bf16[1024,512]{0,1}\n",
      "  %13 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %14 = f32[] xla::device_data(), xla_shape=f32[]\n",
      "  %15 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %16 = bf16[32,1024,1024]{2,1,0} xla::device_data(), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %17 = f32[32,1024,1024]{2,1,0} xla::cast(%16), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %18 = f32[32,1024,1024]{2,1,0} aten::pow(%17, %15), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %19 = f32[32,1024,1]{2,1,0} aten::mean(%18), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %20 = f32[32,1024,1]{2,1,0} aten::add(%19, %14, %13), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %21 = f32[32,1024,1]{2,1,0} aten::rsqrt(%20), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %22 = f32[32,1024,1024]{2,1,0} aten::mul(%17, %21), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %23 = bf16[32,1024,1024]{2,1,0} xla::cast(%22), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %24 = f32[32,1024,1024]{2,1,0} xla::cast(%23), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %25 = bf16[1024]{0} xla::device_data(), xla_shape=bf16[1024]{0}\n",
      "  %26 = f32[1024]{0} xla::cast(%25), xla_shape=f32[1024]{0}\n",
      "  %27 = f32[32,1024,1024]{2,1,0} aten::mul(%26, %24), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %28 = bf16[32,1024,1024]{2,1,0} xla::cast(%27), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %29 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %30 = bf16[32768,512]{1,0} aten::mm(%29, %12), xla_shape=bf16[32768,512]{1,0}\n",
      "  %31 = bf16[32,1024,512]{2,1,0} aten::view(%30), xla_shape=bf16[32,1024,512]{2,1,0}\n",
      "  %32 = bf16[32,1024,4,128]{3,2,1,0} aten::view(%31), xla_shape=bf16[32,1024,4,128]{3,2,1,0}\n",
      "  %33 = bf16[32,4,1024,128]{3,1,2,0} aten::permute(%32), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %34 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%33), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %35 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%34), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %36 = bf16[32,4,1,1024,128]{4,3,2,1,0} aten::view(%35), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %37 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%36), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %38 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%37), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %39 = bf16[32,4,2,1024,128]{4,3,2,1,0} aten::expand(%38), xla_shape=bf16[32,4,2,1024,128]{4,3,2,1,0}\n",
      "  %40 = bf16[32,8,1024,128]{3,2,1,0} aten::view(%39), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %41 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%40), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %42 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%41), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %43 = bf16[512,1024]{1,0} xla::device_data(), xla_shape=bf16[512,1024]{1,0}\n",
      "  %44 = bf16[1024,512]{0,1} aten::permute(%43), xla_shape=bf16[1024,512]{0,1}\n",
      "  %45 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %46 = bf16[32768,512]{1,0} aten::mm(%45, %44), xla_shape=bf16[32768,512]{1,0}\n",
      "  %47 = bf16[32,1024,512]{2,1,0} aten::view(%46), xla_shape=bf16[32,1024,512]{2,1,0}\n",
      "  %48 = bf16[32,1024,4,128]{3,2,1,0} aten::view(%47), xla_shape=bf16[32,1024,4,128]{3,2,1,0}\n",
      "  %49 = bf16[32,4,1024,128]{3,1,2,0} aten::permute(%48), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %50 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%49), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %51 = bf16[32,4,1024,128]{3,1,2,0} xla::select(%50), xla_shape=bf16[32,4,1024,128]{3,1,2,0}\n",
      "  %52 = bf16[32,4,1,1024,128]{4,3,2,1,0} aten::view(%51), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %53 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%52), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %54 = bf16[32,4,1,1024,128]{4,3,2,1,0} xla::select(%53), xla_shape=bf16[32,4,1,1024,128]{4,3,2,1,0}\n",
      "  %55 = bf16[32,4,2,1024,128]{4,3,2,1,0} aten::expand(%54), xla_shape=bf16[32,4,2,1024,128]{4,3,2,1,0}\n",
      "  %56 = bf16[32,8,1024,128]{3,2,1,0} aten::view(%55), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %57 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%56), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %58 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%57), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %59 = bf16[1024,1024]{1,0} xla::device_data(), xla_shape=bf16[1024,1024]{1,0}\n",
      "  %60 = bf16[1024,1024]{0,1} aten::permute(%59), xla_shape=bf16[1024,1024]{0,1}\n",
      "  %61 = bf16[32768,1024]{1,0} aten::view(%28), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %62 = bf16[32768,1024]{1,0} aten::mm(%61, %60), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %63 = bf16[32,1024,1024]{2,1,0} aten::view(%62), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %64 = bf16[32,1024,8,128]{3,2,1,0} aten::view(%63), xla_shape=bf16[32,1024,8,128]{3,2,1,0}\n",
      "  %65 = bf16[32,8,1024,128]{3,1,2,0} aten::permute(%64), xla_shape=bf16[32,8,1024,128]{3,1,2,0}\n",
      "  %66 = bf16[32,8,1024,128]{3,1,2,0} xla::custom_sharding(%65), xla_shape=bf16[32,8,1024,128]{3,1,2,0}\n",
      "  %67 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%66), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %68 = (bf16[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}) xla::tpu_custom_call(%67, %58, %42), num_outputs=3, xla_shape=(bf16[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0}, f32[8,4,1024,128]{3,2,1,0})\n",
      "  %69 = bf16[8,4,1024,128]{3,2,1,0} xla::custom_sharding(%68.0), xla_shape=bf16[8,4,1024,128]{3,2,1,0}\n",
      "  %70 = bf16[32,8,1024,128]{3,2,1,0} xla::custom_sharding(%69), xla_shape=bf16[32,8,1024,128]{3,2,1,0}\n",
      "  %71 = bf16[32,1024,8,128]{3,1,2,0} aten::permute(%70), xla_shape=bf16[32,1024,8,128]{3,1,2,0}\n",
      "  %72 = bf16[32,1024,1024]{2,1,0} aten::view(%71), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %73 = bf16[32768,1024]{1,0} aten::view(%72), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %74 = bf16[32768,1024]{1,0} aten::mm(%73, %10), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %75 = bf16[32,1024,1024]{2,1,0} aten::view(%74), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %76 = bf16[32,1024,1024]{2,1,0} aten::add(%16, %75, %8), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %77 = f32[32,1024,1024]{2,1,0} xla::cast(%76), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %78 = f32[32,1024,1024]{2,1,0} aten::pow(%77, %7), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %79 = f32[32,1024,1]{2,1,0} aten::mean(%78), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %80 = f32[32,1024,1]{2,1,0} aten::add(%79, %6, %5), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %81 = f32[32,1024,1]{2,1,0} aten::rsqrt(%80), xla_shape=f32[32,1024,1]{2,1,0}\n",
      "  %82 = f32[32,1024,1024]{2,1,0} aten::mul(%77, %81), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %83 = bf16[32,1024,1024]{2,1,0} xla::cast(%82), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %84 = f32[32,1024,1024]{2,1,0} xla::cast(%83), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %85 = bf16[1024]{0} xla::device_data(), xla_shape=bf16[1024]{0}\n",
      "  %86 = f32[1024]{0} xla::cast(%85), xla_shape=f32[1024]{0}\n",
      "  %87 = f32[32,1024,1024]{2,1,0} aten::mul(%86, %84), xla_shape=f32[32,1024,1024]{2,1,0}\n",
      "  %88 = bf16[32,1024,1024]{2,1,0} xla::cast(%87), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %89 = bf16[32768,1024]{1,0} aten::view(%88), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %90 = bf16[32768,4096]{1,0} aten::mm(%89, %4), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %91 = bf16[32,1024,4096]{2,1,0} aten::view(%90), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %92 = f32[32,1024,4096]{2,1,0} xla::cast(%91), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %93 = bf16[4096,1024]{1,0} xla::device_data(), xla_shape=bf16[4096,1024]{1,0}\n",
      "  %94 = bf16[1024,4096]{0,1} aten::permute(%93), xla_shape=bf16[1024,4096]{0,1}\n",
      "  %95 = bf16[32768,1024]{1,0} aten::view(%88), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %96 = bf16[32768,4096]{1,0} aten::mm(%95, %94), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %97 = bf16[32,1024,4096]{2,1,0} aten::view(%96), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %98 = bf16[32,1024,4096]{2,1,0} aten::silu(%97), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %99 = f32[32,1024,4096]{2,1,0} xla::cast(%98), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %100 = f32[32,1024,4096]{2,1,0} aten::mul(%99, %92), xla_shape=f32[32,1024,4096]{2,1,0}\n",
      "  %101 = bf16[32,1024,4096]{2,1,0} xla::cast(%100), xla_shape=bf16[32,1024,4096]{2,1,0}\n",
      "  %102 = bf16[32768,4096]{1,0} aten::view(%101), xla_shape=bf16[32768,4096]{1,0}\n",
      "  %103 = bf16[32768,1024]{1,0} aten::mm(%102, %2), xla_shape=bf16[32768,1024]{1,0}\n",
      "  %104 = bf16[32,1024,1024]{2,1,0} aten::view(%103), xla_shape=bf16[32,1024,1024]{2,1,0}\n",
      "  %105 = bf16[32,1024,1024]{2,1,0} aten::add(%76, %104, %0), xla_shape=bf16[32,1024,1024]{2,1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functorch.compile import aot_function, make_boxed_func  # type: ignore\n",
    "from torch.func import functional_call  # type: ignore\n",
    "\n",
    "def print_graph_compiler(gm, _):\n",
    "  print(\"AOTAutograd graph:\")\n",
    "  print(gm.code)\n",
    "  return make_boxed_func(gm)\n",
    "\n",
    "compiled_layer = aot_function(lambda x, params: functional_call(decoder_layer, params, x), print_graph_compiler)\n",
    "decoder_out = compiled_layer(inputs_embeds, dict(decoder_layer.named_parameters()))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([decoder_out]))\n",
    "torch_xla.sync(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: DeviceLockWait\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 035.420us\n",
      "  ValueRate: 05s940ms027.894us / second\n",
      "  Rate: 278940 / second\n",
      "  Percentiles: 1%=008.380us; 5%=008.380us; 10%=008.380us; 20%=008.380us; 50%=027.040us; 80%=027.040us; 90%=027.040us; 95%=027.040us; 99%=027.040us\n",
      "Metric: InputOutputAliasCount\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 0.00\n",
      "  Percentiles: 1%=0.00; 5%=0.00; 10%=0.00; 20%=0.00; 50%=0.00; 80%=0.00; 90%=0.00; 95%=0.00; 99%=0.00\n",
      "Metric: LazyTracing\n",
      "  TotalSamples: 151\n",
      "  Accumulator: 017ms099.420us\n",
      "  ValueRate: 705ms210.694us / second\n",
      "  Rate: 6227.51 / second\n",
      "  Percentiles: 1%=006.940us; 5%=007.240us; 10%=013.440us; 20%=024.140us; 50%=083.870us; 80%=121.610us; 90%=239.510us; 95%=318.030us; 99%=001ms002.750us\n",
      "Metric: TensorsGraphSize\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 112.00\n",
      "  Percentiles: 1%=112.00; 5%=112.00; 10%=112.00; 20%=112.00; 50%=112.00; 80%=112.00; 90%=112.00; 95%=112.00; 99%=112.00\n",
      "Metric: UnwrapXlaData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 018.730us\n",
      "  Percentiles: 1%=018.730us; 5%=018.730us; 10%=018.730us; 20%=018.730us; 50%=018.730us; 80%=018.730us; 90%=018.730us; 95%=018.730us; 99%=018.730us\n",
      "Metric: WrapXlaData\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 011.780us\n",
      "  Percentiles: 1%=011.780us; 5%=011.780us; 10%=011.780us; 20%=011.780us; 50%=011.780us; 80%=011.780us; 90%=011.780us; 95%=011.780us; 99%=011.780us\n",
      "Counter: CreateOpSharding\n",
      "  Value: 3\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 104\n",
      "Counter: DestroyLtcTensor\n",
      "  Value: 105\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 105\n",
      "Counter: ExecuteReplicated\n",
      "  Value: 1\n",
      "Counter: SetShardingSpec\n",
      "  Value: 50\n",
      "Counter: UncachedCompile\n",
      "  Value: 1\n",
      "Counter: XlaMarkSharding\n",
      "  Value: 6\n",
      "Counter: trace_pallas_cache_hit\n",
      "  Value: 1\n",
      "Counter: xla::_copy_from\n",
      "  Value: 13\n",
      "Counter: xla::_to_copy\n",
      "  Value: 13\n",
      "Counter: xla::_unsafe_view\n",
      "  Value: 9\n",
      "Counter: xla::add\n",
      "  Value: 4\n",
      "Counter: xla::as_strided_copy\n",
      "  Value: 4\n",
      "Counter: xla::clone\n",
      "  Value: 10\n",
      "Counter: xla::detach_copy\n",
      "  Value: 23\n",
      "Counter: xla::empty_symint\n",
      "  Value: 13\n",
      "Counter: xla::expand_copy_symint\n",
      "  Value: 2\n",
      "Counter: xla::mean\n",
      "  Value: 2\n",
      "Counter: xla::mm\n",
      "  Value: 7\n",
      "Counter: xla::mul\n",
      "  Value: 5\n",
      "Counter: xla::pow\n",
      "  Value: 2\n",
      "Counter: xla::rsqrt\n",
      "  Value: 2\n",
      "Counter: xla::silu\n",
      "  Value: 1\n",
      "Counter: xla::slice_copy\n",
      "  Value: 8\n",
      "Counter: xla::t_copy\n",
      "  Value: 7\n",
      "Counter: xla::transpose_copy\n",
      "  Value: 4\n",
      "Counter: xla::unsqueeze_copy\n",
      "  Value: 2\n",
      "Counter: xla::view_copy_symint\n",
      "  Value: 20\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 01s463ms311.202us\n",
      "  Percentiles: 1%=01s463ms311.202us; 5%=01s463ms311.202us; 10%=01s463ms311.202us; 20%=01s463ms311.202us; 50%=01s463ms311.202us; 80%=01s463ms311.202us; 90%=01s463ms311.202us; 95%=01s463ms311.202us; 99%=01s463ms311.202us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 118ms027.029us\n",
      "  Percentiles: 1%=118ms027.029us; 5%=118ms027.029us; 10%=118ms027.029us; 20%=118ms027.029us; 50%=118ms027.029us; 80%=118ms027.029us; 90%=118ms027.029us; 95%=118ms027.029us; 99%=118ms027.029us\n",
      "Counter: CreateCompileHandles\n",
      "  Value: 1\n",
      "Counter: MarkStep\n",
      "  Value: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.metrics_report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
