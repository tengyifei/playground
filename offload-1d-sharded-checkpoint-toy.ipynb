{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates 1d sharded checkpoint offload.\n",
    "\n",
    "Profile (v6e-8): http://shortn/_AVBaAqnFNd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
      "env: LIBTPU_INIT_ARGS=--xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
    "\n",
    "# MaxText flags except that we disable optimization barrier removal: crashes in native code\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n",
    "\n",
    "# Still crashes in native code\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100\n",
    "\n",
    "# Removed some more flags, and 1% scheduler shared memory limit: OK\n",
    "%env LIBTPU_INIT_ARGS=--xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch_xla import runtime as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "      return self.m(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder_only_model\n",
    "from trainer import TrainDecoderOnlyBase\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1D FSDP sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch_xla.experimental.spmd_fully_sharded_data_parallel import SpmdFullyShardedDataParallel as FSDPv2\n",
    "from torch_xla import runtime as xr\n",
    "from torch_xla.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "# checkout our doc at https://github.com/pytorch/xla/blob/master/docs/fsdpv2.md\n",
    "class TrainDecoderOnlyFSDPv2(TrainDecoderOnlyBase):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(decoder_only_model.DecoderOnlyConfig(\n",
    "       hidden_size=4096,\n",
    "       num_hidden_layers=32,\n",
    "       num_attention_heads=16,\n",
    "       num_key_value_heads=8,\n",
    "       intermediate_size=8192,\n",
    "       vocab_size=16384,\n",
    "    ))\n",
    "    # Define the mesh following common SPMD practice\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    mesh_shape = (num_devices, 1)\n",
    "    device_ids = np.array(range(num_devices))\n",
    "    # To be noted, the mesh must have an axis named 'fsdp', which the weights and activations will be sharded on.\n",
    "    mesh = xs.Mesh(device_ids, mesh_shape, ('fsdp', 'model'))\n",
    "    xs.set_global_mesh(mesh)\n",
    "\n",
    "    # Shard the input(data parallel).\n",
    "    # Scale the batch size with num_devices since there will be only one\n",
    "    # process that handles all runtime devices.\n",
    "    self.batch_size *= num_devices\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.randint(\n",
    "            0,\n",
    "            self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "            dtype=torch.int64,\n",
    "            device='cpu'),\n",
    "              torch.randint(\n",
    "                  0,\n",
    "                  self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "                  dtype=torch.int64,\n",
    "                  device='cpu')),\n",
    "        sample_count=self.train_dataset_len // self.batch_size)\n",
    "    self.train_device_loader = pl.MpDeviceLoader(\n",
    "        train_loader,\n",
    "        self.device,\n",
    "        # Shard the input's batch dimension along the `fsdp` axis, no sharding along other dimensions\n",
    "        input_sharding=xs.ShardingSpec(mesh, ('fsdp', None)))  # type:ignore\n",
    "    \n",
    "    model: decoder_only_model.DecoderOnlyModel = self.model  # type:ignore\n",
    "    self.model = model\n",
    "\n",
    "    # Apply checkpoint to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = checkpoint_module(block)\n",
    "        \n",
    "    # Apply offloading to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = OffloadingModule(block)\n",
    "\n",
    "    # Apply FSDP sharding on each DecoderLayer layer.\n",
    "    auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            OffloadingModule\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # FSDPv2 will use the global mesh set above\n",
    "    self.model: torch.nn.Module = self.model\n",
    "    self.model = FSDPv2(\n",
    "        self.model, auto_wrap_policy=auto_wrap_policy)\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.use_spmd()\n",
    "base = TrainDecoderOnlyFSDPv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n",
      "Epoch 1 train begin  6:27AM UTC on Nov 10, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:184: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: 9.870162010192871, rate: 3.319816552357878\n",
      "epoch: 1, step: 1, loss: 9.870155334472656, rate: 46.719624509538505\n",
      "Epoch 1 train end  6:30AM UTC on Nov 10, 2024\n",
      "epoch: 1, step: 2, loss: 9.870158195495605, rate: 64.20032894730252\n",
      "Profiling model\n",
      "Epoch 1 train begin  6:30AM UTC on Nov 10, 2024\n",
      "Starting to trace for 15000 ms. Remaining attempt(s): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 06:30:27.324573: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 4520830 nanoseconds and will start immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: 9.870157241821289, rate: 73.35901042365177\n",
      "epoch: 1, step: 1, loss: 9.870158195495605, rate: 74.85404335119196\n",
      "epoch: 1, step: 2, loss: 9.870161056518555, rate: 75.44193497289696\n",
      "epoch: 1, step: 3, loss: 9.870157241821289, rate: 75.70407848861927\n",
      "Epoch 1 train end  6:30AM UTC on Nov 10, 2024\n",
      "epoch: 1, step: 4, loss: 9.87015438079834, rate: 75.80861577492679\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling model\")\n",
    "base.num_steps = 3\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "print(\"Profiling model\")\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)\n",
    "xp.trace_detached(\n",
    "    service_addr=\"localhost:9012\", logdir=f\"profile\", duration_ms=15000)\n",
    "base.num_steps = 5\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "del server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">       4096   \n",
       "     ┌───────┐\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "     │       │\n",
       "16384├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "     │       │\n",
       "     └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "       4096   \n",
       "     ┌───────┐\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m0\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m1\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m2\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m3\u001b[0m │\n",
       "     │       │\n",
       "16384├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m4\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m5\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m6\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m7\u001b[0m │\n",
       "     │       │\n",
       "     └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualize import visualize_tensor_sharding\n",
    "_ = visualize_tensor_sharding(base.model.embed_tokens.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "    │       │\n",
       "    └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m0\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m1\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m2\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m3\u001b[0m │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m4\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m5\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m6\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m7\u001b[0m │\n",
       "    │       │\n",
       "    └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.q_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "    │       │\n",
       "    └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m0\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m1\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m2\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m3\u001b[0m │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m4\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m5\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m6\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m7\u001b[0m │\n",
       "    │       │\n",
       "    └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.o_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
