{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates 1d sharded checkpoint offload.\n",
    "\n",
    "Profile (v6e-8): http://shortn/_AVBaAqnFNd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
      "env: LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
    "\n",
    "# MaxText flags except that we disable optimization barrier removal: crashes in native code\n",
    "%env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n",
    "\n",
    "# Still crashes in native code\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100\n",
    "\n",
    "# Removed some more flags, and 1% scheduler shared memory limit: OK\n",
    "# %env LIBTPU_INIT_ARGS=--xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch_xla import runtime as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "      return self.m(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder_only_model\n",
    "from trainer import TrainDecoderOnlyBase\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1D FSDP sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch_xla.experimental.spmd_fully_sharded_data_parallel import SpmdFullyShardedDataParallel as FSDPv2\n",
    "from torch_xla import runtime as xr\n",
    "from torch_xla.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "# checkout our doc at https://github.com/pytorch/xla/blob/master/docs/fsdpv2.md\n",
    "class TrainDecoderOnlyFSDPv2(TrainDecoderOnlyBase):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(decoder_only_model.DecoderOnlyConfig(\n",
    "       hidden_size=4096,\n",
    "       num_hidden_layers=32,\n",
    "       num_attention_heads=16,\n",
    "       num_key_value_heads=8,\n",
    "       intermediate_size=8192,\n",
    "       vocab_size=16384,\n",
    "    ))\n",
    "    # Define the mesh following common SPMD practice\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    mesh_shape = (num_devices, 1)\n",
    "    device_ids = np.array(range(num_devices))\n",
    "    # To be noted, the mesh must have an axis named 'fsdp', which the weights and activations will be sharded on.\n",
    "    mesh = xs.Mesh(device_ids, mesh_shape, ('fsdp', 'model'))\n",
    "    xs.set_global_mesh(mesh)\n",
    "\n",
    "    # Shard the input(data parallel).\n",
    "    # Scale the batch size with num_devices since there will be only one\n",
    "    # process that handles all runtime devices.\n",
    "    self.batch_size *= num_devices\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.randint(\n",
    "            0,\n",
    "            self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "            dtype=torch.int64,\n",
    "            device='cpu'),\n",
    "              torch.randint(\n",
    "                  0,\n",
    "                  self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "                  dtype=torch.int64,\n",
    "                  device='cpu')),\n",
    "        sample_count=self.train_dataset_len // self.batch_size)\n",
    "    self.train_device_loader = pl.MpDeviceLoader(\n",
    "        train_loader,\n",
    "        self.device,\n",
    "        # Shard the input's batch dimension along the `fsdp` axis, no sharding along other dimensions\n",
    "        input_sharding=xs.ShardingSpec(mesh, ('fsdp', None)))  # type:ignore\n",
    "    \n",
    "    model: decoder_only_model.DecoderOnlyModel = self.model  # type:ignore\n",
    "    self.model = model\n",
    "\n",
    "    # Apply checkpoint to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = checkpoint_module(block)\n",
    "        \n",
    "    # Apply offloading to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = OffloadingModule(block)\n",
    "\n",
    "    # Apply FSDP sharding on each DecoderLayer layer.\n",
    "    auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            OffloadingModule\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # FSDPv2 will use the global mesh set above\n",
    "    self.model: torch.nn.Module = self.model\n",
    "    self.model = FSDPv2(\n",
    "        self.model, auto_wrap_policy=auto_wrap_policy)\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "xr.use_spmd()\n",
    "base = TrainDecoderOnlyFSDPv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n",
      "Epoch 1 train begin  5:41AM UTC on Nov 10, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:184: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n",
      "https://symbolize.corp.google.com/r/?trace=79fe4f0fb0ad,7a0fb32a8dcf,79fe4f0fa2bc,79fe4f1020c3,79fe5408cc11,79fe5408c678,79fe4f0a6ee1,79fe4f09ceb0,79fe517c41b2,79fe56af4d12,79fe56afaed5,79fe56b03a44,79fe56c9f602,7a0fb3255ea6&map= \n",
      "*** SIGSEGV (@0x18), see go/stacktraces#s15 received by PID 2076220 (TID 2077780) on cpu 5; stack trace: ***\n",
      "PC: @     0x79fe4f0fb0ad  (unknown)  xla::jellyfish::(anonymous namespace)::HasActivationSemantics()\n",
      "    @     0x79fe56dc80e1       1888  FailureSignalHandler()\n",
      "    @     0x7a0fb32a8dd0  (unknown)  (unknown)\n",
      "    @     0x79fe4f0fa2bd        384  xla::jellyfish::AllReduceDecomposer::TryMatchAllGatherEinsumOrEinsumReduceScatter()\n",
      "    @     0x79fe4f1020c4        400  xla::jellyfish::AllReduceDecomposer::Run()\n",
      "    @     0x79fe5408cc12        416  xla::HloPassPipeline::RunPassesInternal<>()\n",
      "    @     0x79fe5408c679         80  xla::HloPassPipeline::Run()\n",
      "    @     0x79fe4f0a6ee2       1120  xla::jellyfish::(anonymous namespace)::Phase2PreLayoutAssignment()\n",
      "    @     0x79fe4f09ceb1       1472  xla::jellyfish::(anonymous namespace)::HloOptimize()::$_0::operator()()\n",
      "    @     0x79fe517c41b3         32  absl::internal_any_invocable::LocalInvoker<>()\n",
      "    @     0x79fe56af4d13        176  thread::Fiber::Body()\n",
      "    @     0x79fe56afaed6        112  thread::(anonymous namespace)::FutexDomainThread::WorkLoop()\n",
      "    @     0x79fe56b03a45         32  thread::CommonFiberDomainThread::Run()\n",
      "    @     0x79fe56c9f603        256  Thread::ThreadBody()\n",
      "    @     0x7a0fb3255ea7  (unknown)  start_thread\n",
      "https://symbolize.corp.google.com/r/?trace=79fe4f0fb0ad,79fe56dc80e0,7a0fb32a8dcf,79fe4f0fa2bc,79fe4f1020c3,79fe5408cc11,79fe5408c678,79fe4f0a6ee1,79fe4f09ceb0,79fe517c41b2,79fe56af4d12,79fe56afaed5,79fe56b03a44,79fe56c9f602,7a0fb3255ea6&map= \n",
      "E1110 05:41:18.090926 2077780 coredump_hook.cc:301] RAW: Remote crash data gathering hook invoked.\n",
      "E1110 05:41:18.090937 2077780 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\n",
      "E1110 05:41:18.090939 2077780 coredump_hook.cc:396] RAW: Sending fingerprint to remote end.\n",
      "E1110 05:41:18.090959 2077780 coredump_hook.cc:405] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\n",
      "E1110 05:41:18.090962 2077780 coredump_hook.cc:457] RAW: Dumping core locally.\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling model\")\n",
    "base.num_steps = 3\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "print(\"Profiling model\")\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)\n",
    "xp.trace_detached(\n",
    "    service_addr=\"localhost:9012\", logdir=f\"profile\", duration_ms=15000)\n",
    "base.num_steps = 5\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "del server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">       4096   \n",
       "     ┌───────┐\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "     │       │\n",
       "16384├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "     │       │\n",
       "     └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "       4096   \n",
       "     ┌───────┐\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m0\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m1\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m2\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m3\u001b[0m │\n",
       "     │       │\n",
       "16384├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m4\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m5\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m6\u001b[0m │\n",
       "     │       │\n",
       "     ├───────┤\n",
       "     │       │\n",
       "     │ TPU \u001b[1;36m7\u001b[0m │\n",
       "     │       │\n",
       "     └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualize import visualize_tensor_sharding\n",
    "_ = visualize_tensor_sharding(base.model.embed_tokens.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "    │       │\n",
       "    └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m0\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m1\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m2\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m3\u001b[0m │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m4\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m5\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m6\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m7\u001b[0m │\n",
       "    │       │\n",
       "    └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.q_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> │\n",
       "    │       │\n",
       "    └───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      4096   \n",
       "    ┌───────┐\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m0\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m1\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m2\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m3\u001b[0m │\n",
       "    │       │\n",
       "4096├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m4\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m5\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m6\u001b[0m │\n",
       "    │       │\n",
       "    ├───────┤\n",
       "    │       │\n",
       "    │ TPU \u001b[1;36m7\u001b[0m │\n",
       "    │       │\n",
       "    └───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualize_tensor_sharding(base.model.layers[0].m.self_attn.o_proj.weight, use_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
