{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "from torch.library import impl, custom_op\n",
    "\n",
    "# Custom forward op: uses einsum internally\n",
    "@custom_op(\"xla::custom_linear_forward\", schema=\"(Tensor input, Tensor weight, Tensor? bias) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch.einsum('...n,mn->...m', input, weight)\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "  \n",
    "@custom_linear_forward.register_fake\n",
    "def custom_linear_forward_fake(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch.einsum('...n,mn->...m', input, weight)\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "\n",
    "@custom_op(\"xla::custom_linear_backward\", schema=\"(Tensor grad_output, Tensor input, Tensor weight, Tensor? bias, bool needs_input_grad_input, bool needs_input_grad_weight, bool needs_input_grad_bias) -> (Tensor, Tensor, Tensor)\", mutates_args=())\n",
    "def custom_linear_backward(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "    \n",
    "    if needs_input_grad_input:\n",
    "        grad_input = torch.einsum('...m,mn->...n', grad_output, weight)\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "    \n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch.einsum('...m,...n->mn', grad_output, input)\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "    \n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch.einsum('...m->m', grad_output)\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "@custom_linear_backward.register_fake\n",
    "def custom_linear_backward_fake(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "    \n",
    "    if needs_input_grad_input:\n",
    "        grad_input = torch.einsum('...m,mn->...n', grad_output, weight)\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "    \n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch.einsum('...m,...n->mn', grad_output, input)\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "    \n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch.einsum('...m->m', grad_output)\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "# Now define the XLAPatchedLinear function that uses the custom ops\n",
    "class XLAPatchedLinear(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A patched version of `torch.nn.functional.linear` that uses einsum via custom ops.\n",
    "    By wrapping these calls in custom ops, AOTAutograd won't decompose einsum.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        # Call our custom forward op\n",
    "        return torch.ops.xla.custom_linear_forward(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        needs_input_grad_input = ctx.needs_input_grad[0]\n",
    "        needs_input_grad_weight = ctx.needs_input_grad[1]\n",
    "        needs_input_grad_bias = False\n",
    "        if bias is not None:\n",
    "            needs_input_grad_bias = ctx.needs_input_grad[2]\n",
    "\n",
    "        # Call our custom backward op with the boolean flags\n",
    "        grad_input, grad_weight, grad_bias = torch.ops.xla.custom_linear_backward(\n",
    "            grad_output, \n",
    "            input, \n",
    "            weight, \n",
    "            bias, \n",
    "            needs_input_grad_input, \n",
    "            needs_input_grad_weight, \n",
    "            needs_input_grad_bias\n",
    "        )\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4687, -2.7780,  0.8191],\n",
      "        [-0.4687, -2.7780,  0.8191]]) tensor([[ 1.8054, -0.7140, -0.1606],\n",
      "        [ 1.8054, -0.7140, -0.1606],\n",
      "        [ 1.8054, -0.7140, -0.1606],\n",
      "        [ 1.8054, -0.7140, -0.1606]]) tensor([2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "w = torch.randn(4, 3, requires_grad=True)\n",
    "b = torch.randn(4, requires_grad=True)\n",
    "\n",
    "# Run forward\n",
    "y = XLAPatchedLinear.apply(x, w, b)\n",
    "loss = y.sum()\n",
    "# Run backward\n",
    "loss.backward()\n",
    "print(x.grad, w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Graph ===\n",
      "<lambda>()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    custom_linear_forward = torch.ops.xla.custom_linear_forward.default(arg0_1, arg1_1, None);  arg0_1 = arg1_1 = None\n",
      "    return (custom_linear_forward,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "=== Output ===\n",
      "tensor([[-3.4562e-01, -6.5829e-01, -2.4595e+00],\n",
      "        [ 9.8354e-01, -4.9411e-01, -3.9749e+00],\n",
      "        [-1.0354e+00, -1.8021e+00, -8.4746e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "# A custom compiler function that prints the graph.\n",
    "def print_graph(gm, sample_inputs):\n",
    "    # Print the FX Graph to observe the operations after decomposition\n",
    "    print(\"=== Generated Graph ===\")\n",
    "    print(gm)\n",
    "    return gm.forward\n",
    "\n",
    "def my_einsum_func(x, y):\n",
    "    # A simple einsum expression to test decomposition\n",
    "    return XLAPatchedLinear.apply(x, y)\n",
    "\n",
    "# Wrap the function with aot_function, using our custom compilers that print the graph\n",
    "compiled_func = aot_function(\n",
    "    my_einsum_func,\n",
    "    fw_compiler=print_graph,\n",
    "    bw_compiler=print_graph\n",
    ")\n",
    "\n",
    "# Run the compiled function with sample inputs\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "out = compiled_func(x, y)\n",
    "\n",
    "print(\"=== Output ===\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
