{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an AOTAutograd friendly version of einsum that won't be decomposed into\n",
    "views and transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "from torch.library import impl, custom_op\n",
    "\n",
    "# Custom forward op: uses einsum internally\n",
    "@custom_op(\"xla::custom_linear_forward\", schema=\"(Tensor input, Tensor weight, Tensor? bias) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch_xla._XLAC._xla_einsum('...n,mn->...m', (input, weight))\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "  \n",
    "@custom_linear_forward.register_fake\n",
    "def custom_linear_forward_fake(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch.einsum('...n,mn->...m', input, weight)\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "\n",
    "@custom_op(\"xla::custom_linear_backward\", schema=\"(Tensor grad_output, Tensor input, Tensor weight, Tensor? bias, bool needs_input_grad_input, bool needs_input_grad_weight, bool needs_input_grad_bias) -> (Tensor, Tensor, Tensor)\", mutates_args=())\n",
    "def custom_linear_backward(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "    \n",
    "    if needs_input_grad_input:\n",
    "        grad_input = torch_xla._XLAC._xla_einsum('...m,mn->...n', (grad_output, weight))\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "    \n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch_xla._XLAC._xla_einsum('...m,...n->mn', (grad_output, input))\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "    \n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch_xla._XLAC._xla_einsum('...m->m', (grad_output, ))\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "@custom_linear_backward.register_fake\n",
    "def custom_linear_backward_fake(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "    \n",
    "    if needs_input_grad_input:\n",
    "        grad_input = torch.einsum('...m,mn->...n', grad_output, weight)\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "    \n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch.einsum('...m,...n->mn', grad_output, input)\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "    \n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch.einsum('...m->m', grad_output)\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "# Now define the XLAPatchedLinear function that uses the custom ops\n",
    "class XLAPatchedLinear(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A patched version of `torch.nn.functional.linear` that uses einsum via custom ops.\n",
    "    By wrapping these calls in custom ops, AOTAutograd won't decompose einsum.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        # Call our custom forward op\n",
    "        return torch.ops.xla.custom_linear_forward(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        needs_input_grad_input = ctx.needs_input_grad[0]\n",
    "        needs_input_grad_weight = ctx.needs_input_grad[1]\n",
    "        needs_input_grad_bias = False\n",
    "        if bias is not None:\n",
    "            needs_input_grad_bias = ctx.needs_input_grad[2]\n",
    "\n",
    "        # Call our custom backward op with the boolean flags\n",
    "        grad_input, grad_weight, grad_bias = torch.ops.xla.custom_linear_backward(\n",
    "            grad_output, \n",
    "            input, \n",
    "            weight, \n",
    "            bias, \n",
    "            needs_input_grad_input, \n",
    "            needs_input_grad_weight, \n",
    "            needs_input_grad_bias\n",
    "        )\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1934, -2.8848,  0.7188],\n",
      "        [-1.1934, -2.8848,  0.7188]], device='xla:0') tensor([[ 1.3555, -1.8750, -0.3125],\n",
      "        [ 1.3555, -1.8750, -0.3125],\n",
      "        [ 1.3555, -1.8750, -0.3125],\n",
      "        [ 1.3555, -1.8750, -0.3125]], device='xla:0') tensor([2., 2., 2., 2.], device='xla:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Einsum: ...n,mn->...m\n",
      "Building Einsum: ...m,mn->...n\n",
      "Building Einsum: ...m,...n->mn\n",
      "Building Einsum: ...m->m\n"
     ]
    }
   ],
   "source": [
    "with torch_xla.runtime.xla_device():\n",
    "  x = torch.randn(2, 3, requires_grad=True)\n",
    "  w = torch.randn(4, 3, requires_grad=True)\n",
    "  b = torch.randn(4, requires_grad=True)\n",
    "\n",
    "  # Run forward\n",
    "  y = XLAPatchedLinear.apply(x, w, b)\n",
    "  loss = y.sum()\n",
    "  # Run backward\n",
    "  loss.backward()\n",
    "  print(x.grad, w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Graph ===\n",
      "<lambda>()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    custom_linear_forward = torch.ops.xla.custom_linear_forward.default(arg0_1, arg1_1, None);  arg0_1 = arg1_1 = None\n",
      "    return (custom_linear_forward,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "=== Output ===\n",
      "tensor([[ 0.6927, -1.0702,  0.1617],\n",
      "        [ 3.4787, -0.7434,  0.0236],\n",
      "        [ 1.6258,  0.4528, -0.1720]], device='xla:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Einsum: ...n,mn->...m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "# A custom compiler function that prints the graph.\n",
    "def print_graph(gm, sample_inputs):\n",
    "    # Print the FX Graph to observe the operations after decomposition\n",
    "    print(\"=== Generated Graph ===\")\n",
    "    print(gm)\n",
    "    return gm.forward\n",
    "\n",
    "def my_einsum_func(x, y):\n",
    "    # A simple einsum expression to test decomposition\n",
    "    return XLAPatchedLinear.apply(x, y)\n",
    "\n",
    "# Wrap the function with aot_function, using our custom compilers that print the graph\n",
    "compiled_func = aot_function(\n",
    "    my_einsum_func,\n",
    "    fw_compiler=print_graph,\n",
    "    bw_compiler=print_graph\n",
    ")\n",
    "\n",
    "# Run the compiled function with sample inputs\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x = torch.randn(3, 3)\n",
    "  y = torch.randn(3, 3)\n",
    "  out = compiled_func(x, y)\n",
    "\n",
    "print(\"=== Output ===\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the HLO lowering of einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[3,3]{1,0}, f32[3,3]{1,0})->(f32[3,3]{1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[3,3], p1.2: f32[3,3]) -> (f32[3,3]) {\n",
      "  %p1.2 = f32[3,3]{1,0} parameter(1)\n",
      "  %p0.1 = f32[3,3]{1,0} parameter(0)\n",
      "  %dot.3 = f32[3,3]{1,0} dot(f32[3,3]{1,0} %p1.2, f32[3,3]{1,0} %p0.1), lhs_contracting_dims={1}, rhs_contracting_dims={1}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[3,3]{1,0} transpose(f32[3,3]{1,0} %dot.3), dimensions={0,1}\n",
      "  ROOT %tuple.5 = (f32[3,3]{1,0}) tuple(f32[3,3]{1,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Einsum: ...n,mn->...m\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.runtime\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x = x.to('xla')\n",
    "  y = y.to('xla')\n",
    "  out = compiled_func(x, y)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,2,1} aten::permute(%1), xla_shape=f32[1,3,3]{0,2,1}\n",
      "  %3 = f32[1,3,1]{2,1,0} aten::sum(%2), xla_shape=f32[1,3,1]{2,1,0}\n",
      "  %4 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %5 = f32[3,3,1]{2,1,0} aten::view(%4), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::permute(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,3,1]{2,1,0} aten::mul(%6, %3), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %8 = f32[3,3]{1,0} aten::view(%7), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.20, entry_computation_layout={(f32[3,3]{1,0}, f32[3,3]{1,0})->(f32[3,3]{1,0})}\n",
      "\n",
      "%AddComputation.6 (x.7: f32[], y.8: f32[]) -> f32[] {\n",
      "  %x.7 = f32[] parameter(0)\n",
      "  %y.8 = f32[] parameter(1)\n",
      "  ROOT %add.9 = f32[] add(f32[] %x.7, f32[] %y.8)\n",
      "}\n",
      "\n",
      "ENTRY %IrToHlo.20 (p0.1: f32[3,3], p1.12: f32[3,3]) -> (f32[3,3]) {\n",
      "  %constant.5 = s32[] constant(3)\n",
      "  %p1.12 = f32[3,3]{1,0} parameter(1)\n",
      "  %reshape.13 = f32[3,3,1]{2,1,0} reshape(f32[3,3]{1,0} %p1.12)\n",
      "  %transpose.14 = f32[3,3,1]{2,1,0} transpose(f32[3,3,1]{2,1,0} %reshape.13), dimensions={0,1,2}\n",
      "  %p0.1 = f32[3,3]{1,0} parameter(0)\n",
      "  %reshape.2 = f32[3,3,1]{2,1,0} reshape(f32[3,3]{1,0} %p0.1)\n",
      "  %transpose.3 = f32[1,3,3]{0,2,1} transpose(f32[3,3,1]{2,1,0} %reshape.2), dimensions={2,0,1}\n",
      "  %constant.4 = f32[] constant(0)\n",
      "  %reduce.10 = f32[1,3]{1,0} reduce(f32[1,3,3]{0,2,1} %transpose.3, f32[] %constant.4), dimensions={2}, to_apply=%AddComputation.6\n",
      "  %reshape.11 = f32[1,3,1]{2,1,0} reshape(f32[1,3]{1,0} %reduce.10)\n",
      "  %reshape.15 = f32[3,1]{1,0} reshape(f32[1,3,1]{2,1,0} %reshape.11)\n",
      "  %broadcast.16 = f32[3,3,1]{2,1,0} broadcast(f32[3,1]{1,0} %reshape.15), dimensions={1,2}\n",
      "  %multiply.17 = f32[3,3,1]{2,1,0} multiply(f32[3,3,1]{2,1,0} %transpose.14, f32[3,3,1]{2,1,0} %broadcast.16)\n",
      "  %reshape.18 = f32[3,3]{1,0} reshape(f32[3,3,1]{2,1,0} %multiply.17)\n",
      "  ROOT %tuple.19 = (f32[3,3]{1,0}) tuple(f32[3,3]{1,0} %reshape.18)\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch_xla.runtime\n",
    "\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "y = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    x = x.to('xla').requires_grad_()\n",
    "    y = y.to('xla').requires_grad_()\n",
    "    out = torch.einsum('ab,bc->ab', x, y)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
