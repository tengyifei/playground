{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `scan` and `scan_layers` in Mixtral-8x7B\n",
    "\n",
    "Hugging Face usage follows https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    "\n",
    "To test scan, we need to use a custom modification of the transformer repo:\n",
    "https://github.com/pytorch-tpu/transformers/commit/24c90b41fc04ad3769b633b41988dc7f33c1175d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_USE_SPMD=1\n",
      "env: XLA_USE_BF16=0\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_USE_SPMD=1\n",
    "%env XLA_USE_BF16=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1801350/1801350 [02:00<00:00, 14937.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "tokenizer.bos_token_id = 1\n",
    "tokenizer.eos_token_id = 2\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1801350/1801350 [19:42<00:00, 1522.77 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:02<00:00, 1587.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " dict_keys(['input_ids', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"][1].keys(), lm_datasets[\"validation\"][1].keys()  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets[\"validation\"])  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, MixtralForCausalLM\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "\n",
    "# Define model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    vocab_size=len(tokenizer),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=8,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=128,\n",
    "    num_local_experts=2,\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=True,\n",
    ")\n",
    "config.flash_attention = True\n",
    "config.static = False\n",
    "config.gmm = True\n",
    "config.gmm_stack = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = MixtralForCausalLM(config)\n",
    "model = model.to(torch_xla.device())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=2500,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"no\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    tpu_num_cores=4,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling server started: <_XLAC.profiler.ProfilerServer object at 0x7faf401a31f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:02:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=5.98215859375, metrics={'train_runtime': 3742.6639, 'train_samples_per_second': 42.75, 'train_steps_per_second': 0.668, 'total_flos': 826461388800000.0, 'train_loss': 5.98215859375, 'epoch': 0.08881941237076775})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 2497\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 3\n",
      "  Accumulator: 07m37s533ms809.052us\n",
      "  ValueRate: 128ms971.758us / second\n",
      "  Rate: 0.00096818 / second\n",
      "  Percentiles: 1%=02m01s577ms772.680us; 5%=02m01s577ms772.680us; 10%=02m01s577ms772.680us; 20%=02m01s577ms772.680us; 50%=02m16s523ms340.934us; 80%=02m20s433ms695.438us; 90%=02m20s433ms695.438us; 95%=02m20s433ms695.438us; 99%=02m20s433ms695.438us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 2500\n",
      "  Accumulator: 03m40s865ms660.447us\n",
      "  ValueRate: 042ms255.951us / second\n",
      "  Rate: 0.669445 / second\n",
      "  Percentiles: 1%=058ms277.354us; 5%=059ms902.614us; 10%=059ms296.924us; 20%=060ms820.644us; 50%=062ms345.244us; 80%=065ms723.073us; 90%=066ms381.143us; 95%=069ms702.473us; 99%=076ms956.492us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 4372771\n",
      "  Accumulator: 06m38s665ms198.653us\n",
      "  ValueRate: 177ms609.091us / second\n",
      "  Rate: 2337.01 / second\n",
      "  Percentiles: 1%=056.960us; 5%=063.530us; 10%=066.170us; 20%=068.850us; 50%=074.140us; 80%=081.640us; 90%=085.640us; 95%=090.630us; 99%=107.710us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 1\n",
      "  Accumulator: 006ms408.589us\n",
      "  Percentiles: 1%=006ms408.589us; 5%=006ms408.589us; 10%=006ms408.589us; 20%=006ms408.589us; 50%=006ms408.589us; 80%=006ms408.589us; 90%=006ms408.589us; 95%=006ms408.589us; 99%=006ms408.589us\n",
      "Counter: MarkStep\n",
      "  Value: 2501\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again, this time using scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# Define model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    vocab_size=len(tokenizer),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=8,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=128,\n",
    "    num_local_experts=2,\n",
    "    max_position_embeddings=128,  # Max tokens in a sequence\n",
    "    use_cache=False,\n",
    "    unroll_decoders=False,\n",
    ")\n",
    "config.flash_attention = True\n",
    "config.static = False\n",
    "config.gmm = True\n",
    "config.gmm_stack = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = MixtralForCausalLM(config)\n",
    "model = model.to(torch_xla.device())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 48:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=5.97009375, metrics={'train_runtime': 2902.9153, 'train_samples_per_second': 55.117, 'train_steps_per_second': 0.861, 'total_flos': 826461388800000.0, 'train_loss': 5.97009375, 'epoch': 0.08881941237076775})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"].shuffle(seed=42),  # type:ignore\n",
    "    eval_dataset=lm_datasets[\"validation\"].shuffle(seed=42),  # type:ignore\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the for loop and scan model train to the same loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: CachedCompile\n",
      "  Value: 2497\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 3\n",
      "  Accumulator: 01m20s375ms273.037us\n",
      "  ValueRate: 033ms672.670us / second\n",
      "  Rate: 0.0012195 / second\n",
      "  Percentiles: 1%=26s984ms557.394us; 5%=26s984ms557.394us; 10%=26s984ms557.394us; 20%=26s984ms557.394us; 50%=26s301ms178.710us; 80%=28s091ms536.933us; 90%=28s091ms536.933us; 95%=28s091ms536.933us; 99%=28s091ms536.933us\n",
      "Metric: ExecuteReplicatedTime\n",
      "  TotalSamples: 2500\n",
      "  Accumulator: 04m53s907ms346.435us\n",
      "  ValueRate: 082ms882.578us / second\n",
      "  Rate: 0.87776 / second\n",
      "  Percentiles: 1%=091ms976.401us; 5%=091ms457.350us; 10%=092ms846.801us; 20%=092ms303.480us; 50%=093ms262.800us; 80%=094ms094.020us; 90%=095ms686.330us; 95%=095ms262.760us; 99%=096ms311.770us\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 1532752\n",
      "  Accumulator: 02m46s218ms925.740us\n",
      "  ValueRate: 044ms886.994us / second\n",
      "  Rate: 672.615 / second\n",
      "  Percentiles: 1%=040.400us; 5%=043.700us; 10%=045.700us; 20%=049.451us; 50%=065.170us; 80%=075.520us; 90%=080.750us; 95%=089.200us; 99%=141.510us\n",
      "Metric: TransferFromDeviceTime\n",
      "  TotalSamples: 10001\n",
      "  Accumulator: 02m02s527ms167.287us\n",
      "  ValueRate: 043ms371.712us / second\n",
      "  Rate: 3.57066 / second\n",
      "  Percentiles: 1%=005ms419.689us; 5%=006ms936.899us; 10%=006ms246.819us; 20%=007ms603.970us; 50%=014ms045.358us; 80%=018ms792.858us; 90%=019ms571.118us; 95%=019ms954.499us; 99%=020ms736.468us\n",
      "Counter: MarkStep\n",
      "  Value: 2501\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.debug.metrics as met\n",
    "print(met.short_metrics_report())\n",
    "met.clear_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the numerical correctness of `scan_layers`\n",
    "\n",
    "Under the same weights, and the same input tokens, both the for loop based\n",
    "implementation and `scan_layers` based implementation should produce the same\n",
    "output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "input_ids = torch.tensor(tokenized_datasets[\"train\"][3][\"input_ids\"]).unsqueeze(0).type(torch.LongTensor) # type:ignore\n",
    "attention_mask = torch.tensor(tokenized_datasets[\"train\"][3][\"attention_mask\"]).unsqueeze(0) # type:ignore\n",
    "input_ids = input_ids.to(torch_xla.device())\n",
    "attention_mask = attention_mask.to(torch_xla.device())\n",
    "torch_xla.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 28705,  5355, 28768, 28934,   708,   550,  1093, 28724,  3931,\n",
       "         28705, 28770,   714, 28705,     0, 28705, 23967,  4992,   325,  8092,\n",
       "           714, 28705, 30842, 30016, 28993, 31428, 30000, 29182, 29753, 30051,\n",
       "         29306, 29322, 28770,  1200,  8724,   842,   550,  1093, 28724,  3931,\n",
       "           302,   272, 13711,  2222, 28705, 28770,  1143,  1200, 14473, 11449,\n",
       "           298,   390,   550,  1093, 28724,  3931, 23967,  4992,  6950,  3536,\n",
       "          4720,  1200,   349,   264, 12529,   745,  3905,   802, 28733, 28818,\n",
       "          4543,  3798,  2039,  6202,   486,   318,  4770,   304,  9347, 28723,\n",
       "         28790,  1522,   354,   272,  6879, 23558,  4194,   522,   842,  1298,\n",
       "         22246,   297,  4624, 28705, 28750, 28734, 28740, 28740,   297,  4720,\n",
       "          1200,   378,   349,   272,  4008,  2039,   297,   272,   550,  1093,\n",
       "         28724,  3931,  3518,   842,  2929,  2193,   288,   272,  1348, 22104,\n",
       "           302, 12529,   745,   304,  1353,   802, 28733, 28818]],\n",
       "       device='xla:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 32000]),\n",
       " tensor([[[ 1.9352,  2.4326,  4.1603,  ..., -2.3325, -2.4542, -2.3538],\n",
       "          [ 2.2733,  2.7671,  4.7023,  ..., -2.7263, -2.6115, -2.7480],\n",
       "          [ 2.2035,  2.7508,  4.6773,  ..., -2.7347, -2.6657, -2.7148],\n",
       "          ...,\n",
       "          [ 2.2157,  2.8167,  4.7999,  ..., -2.8423, -2.6625, -2.7821],\n",
       "          [ 2.2212,  2.7764,  4.7414,  ..., -2.8068, -2.6568, -2.7357],\n",
       "          [ 2.1995,  2.7361,  4.6238,  ..., -2.7696, -2.6364, -2.6633]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_token(logits):\n",
    "  return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28705,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2]],\n",
       "       device='xla:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pick_token(logits)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 32000]),\n",
       " tensor([[[ 1.9352,  2.4326,  4.1603,  ..., -2.3325, -2.4542, -2.3538],\n",
       "          [ 2.2744,  2.7641,  4.7005,  ..., -2.7256, -2.6111, -2.7477],\n",
       "          [ 2.2037,  2.7484,  4.6760,  ..., -2.7333, -2.6671, -2.7150],\n",
       "          ...,\n",
       "          [ 2.2164,  2.8177,  4.8028,  ..., -2.8431, -2.6634, -2.7836],\n",
       "          [ 2.2204,  2.7747,  4.7416,  ..., -2.8071, -2.6574, -2.7342],\n",
       "          [ 2.1988,  2.7359,  4.6236,  ..., -2.7689, -2.6370, -2.6630]]],\n",
       "        device='xla:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "for_loop_logits = model.forward(input_ids, attention_mask).logits  # type:ignore\n",
    "for_loop_logits.shape, for_loop_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28705,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2]],\n",
       "       device='xla:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_tokens = pick_token(for_loop_logits)\n",
    "for_loop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(for_loop_tokens[0].detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be accurate to within 1%\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shouldn't be completely the same\n",
    "torch.allclose(logits, for_loop_logits, atol=1e-6, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the gradients of scan\n",
    "\n",
    "After I run both scan and for loop versions of the model on the same input, their\n",
    "gradients should also be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using for loop to run decoder layers\n",
      "NOTE: Using scan_layers to speed up compilation\n"
     ]
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "\n",
    "input_ids.requires_grad_(False)\n",
    "attention_mask.requires_grad_(False)\n",
    "labels = input_ids[:, :].clone().contiguous()\n",
    "\n",
    "# Run for loop model and collect the gradients\n",
    "torch.manual_seed(42)\n",
    "for_loop_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = True\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  for_loop_grads.append((name, param.grad.clone().detach()))\n",
    "\n",
    "# Run scan model and collect the gradients\n",
    "torch.manual_seed(42)\n",
    "scan_grads = []\n",
    "model.zero_grad()\n",
    "model.model.zero_grad()\n",
    "torch_xla.sync()\n",
    "model.model.unroll_decoders = False\n",
    "model.model.logged_messages = set()\n",
    "with torch.enable_grad():\n",
    "  model(input_ids, attention_mask, labels=labels).loss.backward()  # type: ignore\n",
    "torch_xla.sync()\n",
    "for (name, param) in model.named_parameters():\n",
    "  assert param.grad is not None\n",
    "  scan_grads.append((name, param.grad.clone().detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the gradients\n",
    "assert len(for_loop_grads) == len(scan_grads)\n",
    "assert len(for_loop_grads) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: model.embed_tokens.weight\n",
      "Pass: model.layers.0.self_attn.q_proj.weight\n",
      "Pass: model.layers.0.self_attn.k_proj.weight\n",
      "Pass: model.layers.0.self_attn.v_proj.weight\n",
      "Pass: model.layers.0.self_attn.o_proj.weight\n",
      "Pass: model.layers.0.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.0.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.0.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.0.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.0.input_layernorm.weight\n",
      "Pass: model.layers.0.post_attention_layernorm.weight\n",
      "Pass: model.layers.1.self_attn.q_proj.weight\n",
      "Pass: model.layers.1.self_attn.k_proj.weight\n",
      "Pass: model.layers.1.self_attn.v_proj.weight\n",
      "Pass: model.layers.1.self_attn.o_proj.weight\n",
      "Pass: model.layers.1.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.1.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.1.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.1.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.1.input_layernorm.weight\n",
      "Pass: model.layers.1.post_attention_layernorm.weight\n",
      "Pass: model.layers.2.self_attn.q_proj.weight\n",
      "Pass: model.layers.2.self_attn.k_proj.weight\n",
      "Pass: model.layers.2.self_attn.v_proj.weight\n",
      "Pass: model.layers.2.self_attn.o_proj.weight\n",
      "Pass: model.layers.2.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.2.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.2.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.2.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.2.input_layernorm.weight\n",
      "Pass: model.layers.2.post_attention_layernorm.weight\n",
      "Pass: model.layers.3.self_attn.q_proj.weight\n",
      "Pass: model.layers.3.self_attn.k_proj.weight\n",
      "Pass: model.layers.3.self_attn.v_proj.weight\n",
      "Pass: model.layers.3.self_attn.o_proj.weight\n",
      "Pass: model.layers.3.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.3.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.3.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.3.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.3.input_layernorm.weight\n",
      "Pass: model.layers.3.post_attention_layernorm.weight\n",
      "Pass: model.layers.4.self_attn.q_proj.weight\n",
      "Pass: model.layers.4.self_attn.k_proj.weight\n",
      "Pass: model.layers.4.self_attn.v_proj.weight\n",
      "Pass: model.layers.4.self_attn.o_proj.weight\n",
      "Pass: model.layers.4.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.4.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.4.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.4.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.4.input_layernorm.weight\n",
      "Pass: model.layers.4.post_attention_layernorm.weight\n",
      "Pass: model.layers.5.self_attn.q_proj.weight\n",
      "Pass: model.layers.5.self_attn.k_proj.weight\n",
      "Pass: model.layers.5.self_attn.v_proj.weight\n",
      "Pass: model.layers.5.self_attn.o_proj.weight\n",
      "Pass: model.layers.5.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.5.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.5.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.5.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.5.input_layernorm.weight\n",
      "Pass: model.layers.5.post_attention_layernorm.weight\n",
      "Pass: model.layers.6.self_attn.q_proj.weight\n",
      "Pass: model.layers.6.self_attn.k_proj.weight\n",
      "Pass: model.layers.6.self_attn.v_proj.weight\n",
      "Pass: model.layers.6.self_attn.o_proj.weight\n",
      "Pass: model.layers.6.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.6.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.6.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.6.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.6.input_layernorm.weight\n",
      "Pass: model.layers.6.post_attention_layernorm.weight\n",
      "Pass: model.layers.7.self_attn.q_proj.weight\n",
      "Pass: model.layers.7.self_attn.k_proj.weight\n",
      "Pass: model.layers.7.self_attn.v_proj.weight\n",
      "Pass: model.layers.7.self_attn.o_proj.weight\n",
      "Pass: model.layers.7.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.7.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.7.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.7.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.7.input_layernorm.weight\n",
      "Pass: model.layers.7.post_attention_layernorm.weight\n",
      "Pass: model.layers.8.self_attn.q_proj.weight\n",
      "Pass: model.layers.8.self_attn.k_proj.weight\n",
      "Pass: model.layers.8.self_attn.v_proj.weight\n",
      "Pass: model.layers.8.self_attn.o_proj.weight\n",
      "Pass: model.layers.8.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.8.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.8.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.8.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.8.input_layernorm.weight\n",
      "Pass: model.layers.8.post_attention_layernorm.weight\n",
      "Pass: model.layers.9.self_attn.q_proj.weight\n",
      "Pass: model.layers.9.self_attn.k_proj.weight\n",
      "Pass: model.layers.9.self_attn.v_proj.weight\n",
      "Pass: model.layers.9.self_attn.o_proj.weight\n",
      "Pass: model.layers.9.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.9.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.9.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.9.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.9.input_layernorm.weight\n",
      "Pass: model.layers.9.post_attention_layernorm.weight\n",
      "Pass: model.layers.10.self_attn.q_proj.weight\n",
      "Pass: model.layers.10.self_attn.k_proj.weight\n",
      "Pass: model.layers.10.self_attn.v_proj.weight\n",
      "Pass: model.layers.10.self_attn.o_proj.weight\n",
      "Pass: model.layers.10.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.10.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.10.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.10.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.10.input_layernorm.weight\n",
      "Pass: model.layers.10.post_attention_layernorm.weight\n",
      "Pass: model.layers.11.self_attn.q_proj.weight\n",
      "Pass: model.layers.11.self_attn.k_proj.weight\n",
      "Pass: model.layers.11.self_attn.v_proj.weight\n",
      "Pass: model.layers.11.self_attn.o_proj.weight\n",
      "Pass: model.layers.11.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.11.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.11.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.11.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.11.input_layernorm.weight\n",
      "Pass: model.layers.11.post_attention_layernorm.weight\n",
      "Pass: model.layers.12.self_attn.q_proj.weight\n",
      "Pass: model.layers.12.self_attn.k_proj.weight\n",
      "Pass: model.layers.12.self_attn.v_proj.weight\n",
      "Pass: model.layers.12.self_attn.o_proj.weight\n",
      "Pass: model.layers.12.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.12.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.12.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.12.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.12.input_layernorm.weight\n",
      "Pass: model.layers.12.post_attention_layernorm.weight\n",
      "Pass: model.layers.13.self_attn.q_proj.weight\n",
      "Pass: model.layers.13.self_attn.k_proj.weight\n",
      "Pass: model.layers.13.self_attn.v_proj.weight\n",
      "Pass: model.layers.13.self_attn.o_proj.weight\n",
      "Pass: model.layers.13.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.13.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.13.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.13.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.13.input_layernorm.weight\n",
      "Pass: model.layers.13.post_attention_layernorm.weight\n",
      "Pass: model.layers.14.self_attn.q_proj.weight\n",
      "Pass: model.layers.14.self_attn.k_proj.weight\n",
      "Pass: model.layers.14.self_attn.v_proj.weight\n",
      "Pass: model.layers.14.self_attn.o_proj.weight\n",
      "Pass: model.layers.14.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.14.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.14.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.14.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.14.input_layernorm.weight\n",
      "Pass: model.layers.14.post_attention_layernorm.weight\n",
      "Pass: model.layers.15.self_attn.q_proj.weight\n",
      "Pass: model.layers.15.self_attn.k_proj.weight\n",
      "Pass: model.layers.15.self_attn.v_proj.weight\n",
      "Pass: model.layers.15.self_attn.o_proj.weight\n",
      "Pass: model.layers.15.block_sparse_moe.gate.weight\n",
      "Pass: model.layers.15.block_sparse_moe.experts.w1\n",
      "Pass: model.layers.15.block_sparse_moe.experts.w2\n",
      "Pass: model.layers.15.block_sparse_moe.experts.w3\n",
      "Pass: model.layers.15.input_layernorm.weight\n",
      "Pass: model.layers.15.post_attention_layernorm.weight\n",
      "Pass: model.norm.weight\n",
      "Pass: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for ((for_loop_name, for_loop_grad), (scan_name, scan_grad)) in zip(for_loop_grads, scan_grads):\n",
    "  assert for_loop_name == scan_name\n",
    "  assert torch.allclose(for_loop_grad, scan_grad, atol=1e-3, rtol=1e-3), f\"{for_loop_name} mismatch by: {torch.max(torch.abs(for_loop_grad - scan_grad))}\"\n",
    "  print(f\"Pass: {for_loop_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[-6.0332e-04,  5.3868e-04, -7.1599e-04,  ..., -2.9187e-05,\n",
       "           -6.4175e-05, -3.4787e-04],\n",
       "          [-1.0868e-03,  1.7916e-03, -2.0205e-04,  ..., -2.7523e-04,\n",
       "           -4.3805e-04, -1.5460e-03],\n",
       "          [-1.2247e-03,  5.8597e-04, -1.6040e-03,  ...,  2.2637e-04,\n",
       "            9.9657e-04, -1.5635e-04],\n",
       "          ...,\n",
       "          [ 4.8003e-04, -2.9207e-04,  2.3917e-04,  ...,  1.4600e-04,\n",
       "           -1.7820e-04,  3.6919e-04],\n",
       "          [ 1.9950e-03, -1.5000e-03,  1.9700e-03,  ...,  4.3291e-05,\n",
       "           -8.0997e-04,  1.0016e-03],\n",
       "          [-3.6173e-03,  1.8253e-03, -2.9197e-03,  ..., -4.3047e-04,\n",
       "            1.6450e-03, -1.8373e-03]], device='xla:0')),\n",
       " tensor(0.0125, device='xla:0'),\n",
       " tensor(-0.0125, device='xla:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_loop_grads[3], torch.max(for_loop_grads[3][1]), torch.min(for_loop_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('model.layers.0.self_attn.v_proj.weight',\n",
       "  tensor([[-5.8174e-04,  5.2028e-04, -6.6052e-04,  ..., -1.5826e-06,\n",
       "           -4.5698e-05, -3.6250e-04],\n",
       "          [-1.0702e-03,  1.7425e-03, -2.0061e-04,  ..., -2.7088e-04,\n",
       "           -4.2271e-04, -1.5163e-03],\n",
       "          [-1.2026e-03,  5.9066e-04, -1.5438e-03,  ...,  2.6712e-04,\n",
       "            1.0029e-03, -1.5864e-04],\n",
       "          ...,\n",
       "          [ 4.9042e-04, -3.0399e-04,  2.6145e-04,  ...,  1.7872e-04,\n",
       "           -1.5594e-04,  3.4986e-04],\n",
       "          [ 2.0169e-03, -1.5067e-03,  1.9870e-03,  ...,  5.5200e-05,\n",
       "           -8.0024e-04,  1.0151e-03],\n",
       "          [-3.6087e-03,  1.8497e-03, -2.9272e-03,  ..., -4.1953e-04,\n",
       "            1.7206e-03, -1.8574e-03]], device='xla:0')),\n",
       " tensor(0.0126, device='xla:0'),\n",
       " tensor(-0.0127, device='xla:0'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_grads[3], torch.max(scan_grads[3][1]), torch.min(scan_grads[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
