{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to prototype a version of FlashAttention kernel\n",
    "that can be captured by AOTAutograd and Dynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from typing import List\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(42)\n",
    "torch_xla._XLAC._xla_set_mat_mul_precision('highest')\n",
    "\n",
    "\n",
    "def serialize(t):\n",
    "  if t is None:\n",
    "    # TODO: this code path causes FunctionalStorageImpl assertion error at https://github.com/pytorch/pytorch/blob/9933e59c2b16c1b0475189eef63b0ff405dfd091/aten/src/ATen/FunctionalStorageImpl.cpp#L111\n",
    "    return torch.tensor(False, dtype=torch.bool, device='xla')\n",
    "  else:\n",
    "    return t.clone()\n",
    "  \n",
    "\n",
    "def deserialize(t):\n",
    "  if t.dtype == torch.bool:\n",
    "    return None\n",
    "  else:\n",
    "    return t.clone()\n",
    "\n",
    "\n",
    "class ProxyCtx:\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    self.to_save = []\n",
    "\n",
    "  def save_for_backward(self, *tensors: torch.Tensor):\n",
    "    self.to_save = tensors\n",
    "\n",
    "  @property\n",
    "  def saved_tensors(self):\n",
    "    return self.to_save\n",
    "\n",
    "  def serialize(self) -> List[torch.Tensor]:\n",
    "    lst = []\n",
    "    for t in self.to_save:\n",
    "      lst.append(serialize(t))\n",
    "    return lst\n",
    "\n",
    "  @staticmethod\n",
    "  def deserialize(saved: List[torch.Tensor]) -> \"ProxyCtx\":\n",
    "    ctx = ProxyCtx()\n",
    "    lst = []\n",
    "    for t in saved:\n",
    "      lst.append(deserialize(t))\n",
    "    ctx.to_save = lst\n",
    "    return ctx\n",
    "  \n",
    "\n",
    "def describe_value(v):\n",
    "    if v is not None and isinstance(v, torch.Tensor):\n",
    "      print(f\"{type(v)}({v.shape}, dtype={v.dtype}, device={v.device})\")\n",
    "    elif isinstance(v, list):\n",
    "      print(f\"list({len(v)})\")\n",
    "    elif v is None:\n",
    "      print(\"None\")\n",
    "    else:\n",
    "      print(type(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.debug.metrics as met\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Tuple, Dict\n",
    "from torch.library import impl\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "\n",
    "_XLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\") == \"1\"\n",
    "\n",
    "\n",
    "def _extract_backend_config(\n",
    "    module: \"jaxlib.mlir._mlir_libs._mlir.ir.Module\") -> Optional[str]:\n",
    "  \"\"\"\n",
    "  This algorithm intends to extract the backend config from the compiler IR like the following,\n",
    "  and it is not designed to traverse any generic MLIR module.\n",
    "\n",
    "  module @jit_add_vectors attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
    "    func.func public @main(%arg0: tensor<8xi32> {mhlo.layout_mode = \"default\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<8xi32> {mhlo.layout_mode = \"default\", mhlo.sharding = \"{replicated}\"}) -> (tensor<8xi32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
    "      %0 = call @add_vectors(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @add_vectors(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = call @wrapped(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @wrapped(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = call @apply_kernel(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @apply_kernel(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = stablehlo.custom_call @tpu_custom_call(%arg0, %arg1) {backend_config = \"{\\22custom_call_config\\22: {\\22body\\22: \\22TUzvUgFNTElSMTkuMC4wZ2l0AAErCwEDBQcJAQMLAwUDDQcFDxEJBRMVA3lZDQFVBwsPEw8PCw8PMwsLCwtlCwsLCwsPCw8PFw8LFw8PCxcPCxcTCw8LDxcLBQNhBwNZAQ0bBxMPGw8CagMfBRcdKy0DAycpHVMREQsBBRkVMzkVTw8DCxUXGRsfCyELIyUFGwEBBR0NCWFmZmluZV9tYXA8KGQwKSAtPiAoZDApPgAFHwUhBSMFJQUnEQMBBSkVLw8dDTEXA8IfAR01NwUrFwPWHwEVO0EdPT8FLRcD9h8BHUNFBS8XA3InAQMDSVcFMR1NEQUzHQ1RFwPGHwEFNSN0cHUubWVtb3J5X3NwYWNlPHZtZW0+ACNhcml0aC5vdmVyZmxvdzxub25lPgAXVQMhBx0DJwMhBwECAgUHAQEBAQECBASpBQEQAQcDAQUDEQETBwMVJwcBAQEBAQEHAwUHAwMLBgUDBQUBBwcDBQcDAwsGBQMFBQMLCQdLRwMFBQkNBwMJBwMDCwYJAwUFBRENBAkHDwURBQABBgMBBQEAxgg32wsdE2EZ2Q0LEyMhHSknaw0LCxMPDw8NCQsRYnVpbHRpbgBmdW5jAHRwdQBhcml0aAB2ZWN0b3IAbW9kdWxlAHJldHVybgBjb25zdGFudABhZGRpAGxvYWQAc3RvcmUAL3dvcmtzcGFjZXMvd29yay9weXRvcmNoL3hsYS90ZXN0L3Rlc3Rfb3BlcmF0aW9ucy5weQBhZGRfdmVjdG9yc19rZXJuZWwAZGltZW5zaW9uX3NlbWFudGljcwBmdW5jdGlvbl90eXBlAHNjYWxhcl9wcmVmZXRjaABzY3JhdGNoX29wZXJhbmRzAHN5bV9uYW1lAG1haW4AdmFsdWUAL2dldFt0cmVlPVB5VHJlZURlZigoQ3VzdG9tTm9kZShOREluZGV4ZXJbKFB5VHJlZURlZigoQ3VzdG9tTm9kZShTbGljZVsoMCwgOCldLCBbXSksKSksICg4LCksICgpKV0sIFtdKSwpKV0AYWRkX3ZlY3RvcnMAdGVzdF90cHVfY3VzdG9tX2NhbGxfcGFsbGFzX2V4dHJhY3RfYWRkX3BheWxvYWQAPG1vZHVsZT4Ab3ZlcmZsb3dGbGFncwAvYWRkAC9zd2FwW3RyZWU9UHlUcmVlRGVmKChDdXN0b21Ob2RlKE5ESW5kZXhlclsoUHlUcmVlRGVmKChDdXN0b21Ob2RlKFNsaWNlWygwLCA4KV0sIFtdKSwpKSwgKDgsKSwgKCkpXSwgW10pLCkpXQA=\\22, \\22needs_layout_passes\\22: true}}\", kernel_name = \"add_vectors_kernel\", operand_layouts = [dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>], result_layouts = [dense<0> : tensor<1xindex>]} : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "  }\n",
    "\n",
    "  Basically, what we are looking for is a two level of operations, and the tpu_custom_call operation in the inner level. It will return None if the payload is not found.\n",
    "  \"\"\"\n",
    "  for operation in module.body.operations:\n",
    "    assert len(\n",
    "        operation.body.blocks) == 1, \"The passing module is not compatible.\"\n",
    "    for op in operation.body.blocks[0].operations:\n",
    "      if op.name == \"stablehlo.custom_call\":\n",
    "        return op.backend_config.value\n",
    "  return None\n",
    "\n",
    "\n",
    "def jax_import_guard():\n",
    "  # Somehow, we need to grab the TPU before JAX locks it. Otherwise, any pt-xla TPU operations will hang.\n",
    "  torch_xla._XLAC._init_computation_client()\n",
    "\n",
    "\n",
    "def convert_torch_dtype_to_jax(dtype: torch.dtype) -> \"jnp.dtype\":\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax.numpy as jnp\n",
    "  if _XLA_USE_BF16:\n",
    "    raise RuntimeError(\n",
    "        \"Pallas kernel does not support XLA_USE_BF16, please unset the env var\")\n",
    "  if dtype == torch.float32:\n",
    "    return jnp.float32\n",
    "  elif dtype == torch.float64:\n",
    "    return jnp.float64\n",
    "  elif dtype == torch.float16:\n",
    "    return jnp.float16\n",
    "  elif dtype == torch.bfloat16:\n",
    "    return jnp.bfloat16\n",
    "  elif dtype == torch.int32:\n",
    "    return jnp.int32\n",
    "  elif dtype == torch.int64:\n",
    "    return jnp.int64\n",
    "  elif dtype == torch.int16:\n",
    "    return jnp.int16\n",
    "  elif dtype == torch.int8:\n",
    "    return jnp.int8\n",
    "  elif dtype == torch.uint8:\n",
    "    return jnp.uint8\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def to_jax_shape_dtype_struct(tensor: torch.Tensor) -> \"jax.ShapeDtypeStruct\":\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "\n",
    "  return jax.ShapeDtypeStruct(tensor.shape,\n",
    "                              convert_torch_dtype_to_jax(tensor.dtype))\n",
    "\n",
    "\n",
    "trace_pallas_arg_to_payload: Dict[Tuple[Any], str] = {}\n",
    "\n",
    "\n",
    "def trace_pallas(kernel: Callable,\n",
    "                 *args,\n",
    "                 static_argnums=None,\n",
    "                 static_argnames=None,\n",
    "                 use_cache=False,\n",
    "                 **kwargs):\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "  import jax._src.pallas.mosaic.pallas_call_registration\n",
    "\n",
    "  jax_args = []  # for tracing\n",
    "  tensor_args = []  # for execution\n",
    "  for i, arg in enumerate(args):\n",
    "    # TODO: Could the args be a tuple of tensors or a list of tensors? Flattern them?\n",
    "    if torch.is_tensor(arg):\n",
    "      # ShapeDtypeStruct doesn't have any storage and thus is very suitable for generating the payload.\n",
    "      jax_meta_tensor = to_jax_shape_dtype_struct(arg)\n",
    "      jax_args.append(jax_meta_tensor)\n",
    "      tensor_args.append(arg)\n",
    "    else:\n",
    "      jax_args.append(arg)\n",
    "\n",
    "  hash_key = ()\n",
    "  if use_cache:\n",
    "    global trace_pallas_arg_to_payload\n",
    "    # implcit assumption here that everything in kwargs is hashable and not a tensor,\n",
    "    # which is true for the gmm and tgmm.\n",
    "    hash_key = (jax.config.jax_default_matmul_precision, kernel, static_argnums,\n",
    "                tuple(static_argnames)\n",
    "                if static_argnames is not None else static_argnames,\n",
    "                tuple(jax_args), repr(sorted(kwargs.items())).encode())\n",
    "    if hash_key in trace_pallas_arg_to_payload:\n",
    "      torch_xla._XLAC._xla_increment_counter('trace_pallas_cache_hit', 1)\n",
    "      return trace_pallas_arg_to_payload[hash_key], tensor_args\n",
    "\n",
    "  # Here we ignore the kwargs for execution as most of the time, the kwargs is only used in traced code.\n",
    "  ir = jax.jit(\n",
    "      kernel, static_argnums=static_argnums,\n",
    "      static_argnames=static_argnames).lower(*jax_args, **kwargs).compiler_ir()\n",
    "  payload = _extract_backend_config(ir)\n",
    "\n",
    "  if use_cache:\n",
    "    # if we reach here it means we have a cache miss.\n",
    "    trace_pallas_arg_to_payload[hash_key] = payload\n",
    "\n",
    "  return payload, tensor_args\n",
    "\n",
    "\n",
    "def make_kernel_from_pallas(kernel: Callable, output_shape_dtype_fn: Callable):\n",
    "  # TODO: Maybe we can cache the payload for the same input.\n",
    "  def wrapped_kernel(kernel: Callable,\n",
    "                     output_shape_dtype_fn: Callable,\n",
    "                     *args,\n",
    "                     static_argnums=None,\n",
    "                     static_argnames=None,\n",
    "                     **kwargs) -> Callable:\n",
    "    payload, tensor_args = trace_pallas(\n",
    "        kernel,\n",
    "        *args,\n",
    "        static_argnums=static_argnums,\n",
    "        static_argnames=static_argnames,\n",
    "        **kwargs)\n",
    "    output_shape_dtype = output_shape_dtype_fn(*args)\n",
    "    assert isinstance(output_shape_dtype,\n",
    "                      list), \"The output_shape_dtype_fn should return a list.\"\n",
    "    output_shapes = [shape for shape, _ in output_shape_dtype]\n",
    "    output_dtypes = [dtype for _, dtype in output_shape_dtype]\n",
    "    outputs = torch_xla._XLAC._xla_tpu_custom_call(tensor_args, payload,\n",
    "                                                   output_shapes, output_dtypes)\n",
    "\n",
    "    # Make the output easier to use.\n",
    "    if len(outputs) == 1:\n",
    "      return outputs[0]\n",
    "    return tuple(outputs)\n",
    "\n",
    "  return functools.partial(wrapped_kernel, kernel, output_shape_dtype_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttention(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  This is a simplified wrapper on top of https://github.com/google/jax/blob/b2058d72b7e1693a41303d5411572aabf99b7981/jax/experimental/pallas/ops/tpu/flash_attention.py#L139\n",
    "  where we only takes q, k, v and causal as input and set block_sizes for the users.\n",
    "  \"\"\"\n",
    "\n",
    "  MIN_BLOCK_SIZE = 128\n",
    "  DEFAULT_MASK_VALUE = -0.7 * float(torch.finfo(torch.float32).max)\n",
    "  # The block_sizes configuration is copied from https://github.com/google/maxtext/blob/0fee320451738166c8e596dc63a57a4673671576/MaxText/layers/attentions.py#L215-L240\n",
    "  # It yields much better performance than the default block_sizes.\n",
    "  DEFAULT_BLOCK_SIZES = {\n",
    "      \"block_q\": 512,\n",
    "      \"block_k_major\": 512,\n",
    "      \"block_k\": 512,\n",
    "      \"block_b\": 2,\n",
    "      \"block_q_major_dkv\": 512,\n",
    "      \"block_k_major_dkv\": 512,\n",
    "      \"block_q_dkv\": 512,\n",
    "      \"block_k_dkv\": 512,\n",
    "      \"block_q_dq\": 1024,\n",
    "      \"block_k_dq\": 256,\n",
    "      \"block_k_major_dq\": 512,\n",
    "  }\n",
    "  NUM_LANES = 128\n",
    "  NUM_SUBLANES = 8\n",
    "\n",
    "  @staticmethod\n",
    "  def prepare_segment_ids(q_segment_ids, kv_segment_ids):\n",
    "    from jax.experimental.pallas.ops.tpu.flash_attention import SegmentIds\n",
    "    if q_segment_ids is None or kv_segment_ids is None:\n",
    "      return None, None, None\n",
    "\n",
    "    assert q_segment_ids is not None and kv_segment_ids is not None, \"Both q_segment_ids and kv_segment_ids should be provided.\"\n",
    "    segment_ids = SegmentIds(\n",
    "        to_jax_shape_dtype_struct(q_segment_ids),\n",
    "        to_jax_shape_dtype_struct(kv_segment_ids))\n",
    "    q_segment_ids = q_segment_ids.unsqueeze(-1).expand(\n",
    "        [-1 for _ in q_segment_ids.shape] + [FlashAttention.NUM_LANES])\n",
    "    kv_segment_ids = kv_segment_ids.unsqueeze(1).expand([\n",
    "        kv_segment_ids.shape[0], FlashAttention.NUM_SUBLANES,\n",
    "        kv_segment_ids.shape[1]\n",
    "    ])\n",
    "    return segment_ids, q_segment_ids, kv_segment_ids\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, q, k, v, causal, q_segment_ids, kv_segment_ids, sm_scale, ab,\n",
    "              partition_spec, mesh):\n",
    "    # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "    # in the global scope which could cause problems for xmp.spawn.\n",
    "    jax_import_guard()\n",
    "    import jax\n",
    "    from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_impl\n",
    "\n",
    "    ctx.causal = causal\n",
    "    ctx.sm_scale = sm_scale\n",
    "    ctx.partition_spec = partition_spec\n",
    "    ctx.mesh = mesh\n",
    "    ctx.q_full_shape = None\n",
    "    ctx.kv_full_shape = None\n",
    "    save_residuals = True\n",
    "\n",
    "    # SPMD integration.\n",
    "    # mark_sharding is in-placed, and therefore save the full q, k, v for the backward.\n",
    "    full_q = q\n",
    "    full_k = k\n",
    "    full_v = v\n",
    "    full_ab = ab\n",
    "    if partition_spec is not None:\n",
    "      ctx.q_full_shape = q.shape\n",
    "      ctx.kv_full_shape = k.shape\n",
    "      q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "      k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "      v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "      if ab:\n",
    "        ab = xs.enable_manual_sharding(\n",
    "            ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "    # It computes the shape and type of o, l, m.\n",
    "    shapes = [q.shape]\n",
    "    dtypes = [q.dtype]\n",
    "    if save_residuals:\n",
    "      res_shape = list(q.shape)\n",
    "      res_shape[-1] = FlashAttention.MIN_BLOCK_SIZE\n",
    "      for _ in range(2):\n",
    "        shapes.append(res_shape)\n",
    "        dtypes.append(torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      if partition_spec is not None and q_segment_ids is not None and kv_segment_ids is not None:\n",
    "        # partition_spec is for q,k,v with shape [batch, num_head, seq_len, head_dim], segment id\n",
    "        # is of shape [batch, seq_len], hence we need to tweak it a bit\n",
    "        segment_id_partition_spec = (partition_spec[0], partition_spec[2])\n",
    "        q_segment_ids = xs.enable_manual_sharding(\n",
    "            q_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "        kv_segment_ids = xs.enable_manual_sharding(\n",
    "            kv_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "      segment_ids, q_segment_ids_fa, kv_segment_ids_fa = FlashAttention.prepare_segment_ids(\n",
    "          q_segment_ids, kv_segment_ids)\n",
    "      ctx.segment_ids = segment_ids\n",
    "\n",
    "      # We can't directly use flash_attention as we need to override the save_residuals flag which returns\n",
    "      # l and m that is needed for the backward. Then we lose all the shape checks.\n",
    "      # TODO: replicate the shape checks on flash_attention.\n",
    "      # Here we seperate the tracing and execution part just to support SegmentIds.\n",
    "      payload, _ = trace_pallas(\n",
    "          _flash_attention_impl,\n",
    "          q,\n",
    "          k,\n",
    "          v,\n",
    "          ab,\n",
    "          segment_ids,\n",
    "          save_residuals,\n",
    "          causal,\n",
    "          sm_scale,\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_b\"], q.shape[0]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q\"], q.shape[2]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major\"], k.shape[2]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k\"], k.shape[2]),\n",
    "          False,\n",
    "          static_argnums=range(5, 13),\n",
    "          use_cache=True,\n",
    "      )\n",
    "\n",
    "      args = [q, k, v]\n",
    "      if ab is not None:\n",
    "        args += [ab]\n",
    "      if segment_ids is not None:\n",
    "        args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "      o = torch_xla._XLAC._xla_tpu_custom_call(args, payload, shapes, dtypes)\n",
    "\n",
    "      if not save_residuals:\n",
    "        o = o[0]\n",
    "        # SPMD integration\n",
    "        if partition_spec is not None:\n",
    "          o = xs.disable_manual_sharding(\n",
    "              o, partition_spec, ctx.q_full_shape, mesh=mesh).global_tensor\n",
    "        return o\n",
    "      o, *aux = o\n",
    "      l, m = (v[..., 0] for v in aux[-2:])\n",
    "\n",
    "    # SPMD integration\n",
    "    if partition_spec is not None:\n",
    "      o = xs.disable_manual_sharding(\n",
    "          o, partition_spec, ctx.q_full_shape, mesh=mesh).global_tensor\n",
    "      l = xs.disable_manual_sharding(\n",
    "          l, partition_spec[0:3], ctx.q_full_shape[0:3],\n",
    "          mesh=mesh).global_tensor\n",
    "      m = xs.disable_manual_sharding(\n",
    "          m, partition_spec[0:3], ctx.q_full_shape[0:3],\n",
    "          mesh=mesh).global_tensor\n",
    "\n",
    "    # q_segment_ids and kv_segment_ids are sharded here if partition_spec is provided\n",
    "    # but it should be OK as the backward will use the same partition_spec\n",
    "    ctx.save_for_backward(full_q, full_k, full_v, o, l, m, q_segment_ids_fa,\n",
    "                          kv_segment_ids_fa, full_ab)\n",
    "    return o\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_bwd_dq, _flash_attention_bwd_dkv\n",
    "\n",
    "    q, k, v, o, l, m, q_segment_ids_fa, kv_segment_ids_fa, ab = ctx.saved_tensors\n",
    "    causal = ctx.causal\n",
    "    sm_scale = ctx.sm_scale\n",
    "    partition_spec = ctx.partition_spec\n",
    "    mesh = ctx.mesh\n",
    "    q_full_shape = ctx.q_full_shape\n",
    "    kv_full_shape = ctx.kv_full_shape\n",
    "    # this segment_ids only reflects the local shape of segment_ids\n",
    "    segment_ids = ctx.segment_ids\n",
    "    grad_q = grad_k = grad_v = grad_ab = None\n",
    "\n",
    "    grad_i = torch.sum(\n",
    "        o.to(torch.float32) * grad_output.to(torch.float32),\n",
    "        axis=-1)  # [batch_size, num_heads, q_seq_len]\n",
    "\n",
    "    expanded_l = l.unsqueeze(-1).expand([-1 for _ in l.shape] +\n",
    "                                        [FlashAttention.MIN_BLOCK_SIZE])\n",
    "    expanded_m = m.unsqueeze(-1).expand([-1 for _ in m.shape] +\n",
    "                                        [FlashAttention.MIN_BLOCK_SIZE])\n",
    "    expanded_grad_i = grad_i.unsqueeze(-1).expand(\n",
    "        [-1 for _ in grad_i.shape] + [FlashAttention.MIN_BLOCK_SIZE])\n",
    "\n",
    "    # SPMD integration\n",
    "    if partition_spec is not None:\n",
    "      q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "      k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "      v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "      expanded_l = xs.enable_manual_sharding(\n",
    "          expanded_l, partition_spec, mesh=mesh).global_tensor\n",
    "      expanded_m = xs.enable_manual_sharding(\n",
    "          expanded_m, partition_spec, mesh=mesh).global_tensor\n",
    "      grad_output = xs.enable_manual_sharding(\n",
    "          grad_output, partition_spec, mesh=mesh).global_tensor\n",
    "      expanded_grad_i = xs.enable_manual_sharding(\n",
    "          expanded_grad_i, partition_spec, mesh=mesh).global_tensor\n",
    "      if ab:\n",
    "        ab = xs.enable_manual_sharding(\n",
    "            ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "    if ctx.needs_input_grad[0]:\n",
    "      payload, _ = trace_pallas(\n",
    "          _flash_attention_bwd_dq,\n",
    "          q,\n",
    "          k,\n",
    "          v,\n",
    "          ab,\n",
    "          segment_ids,\n",
    "          l,\n",
    "          m,\n",
    "          grad_output,\n",
    "          grad_i,\n",
    "          block_q_major=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dq\"],\n",
    "                            q.shape[2]),\n",
    "          block_k_major=min(\n",
    "              FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dq\"],\n",
    "              k.shape[2]),\n",
    "          block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dq\"],\n",
    "                      k.shape[2]),\n",
    "          sm_scale=sm_scale,\n",
    "          causal=causal,\n",
    "          mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "          debug=False,\n",
    "          static_argnames=[\n",
    "              \"block_q_major\", \"block_k_major\", \"block_k\", \"sm_scale\", \"causal\",\n",
    "              \"mask_value\", \"debug\"\n",
    "          ],\n",
    "          use_cache=True,\n",
    "      )\n",
    "\n",
    "      args = [q, k, v]\n",
    "      if ab is not None:\n",
    "        args += [ab]\n",
    "      if segment_ids is not None:\n",
    "        args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "      args += [expanded_l, expanded_m, grad_output, expanded_grad_i]\n",
    "\n",
    "      outputs = [q]\n",
    "      if ab is not None:\n",
    "        outputs += [ab]\n",
    "      grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                   [i.shape for i in outputs],\n",
    "                                                   [i.dtype for i in outputs])\n",
    "      if ctx.needs_input_grad[0]:\n",
    "        grad_q = grads[0]\n",
    "      if ctx.needs_input_grad[-3]:\n",
    "        grad_ab = grads[1]\n",
    "\n",
    "    if ctx.needs_input_grad[1] or ctx.needs_input_grad[2]:\n",
    "      payload, _ = trace_pallas(\n",
    "          _flash_attention_bwd_dkv,\n",
    "          q,\n",
    "          k,\n",
    "          v,\n",
    "          ab,\n",
    "          segment_ids,\n",
    "          l,\n",
    "          m,\n",
    "          grad_output,\n",
    "          grad_i,\n",
    "          block_q_major=min(\n",
    "              FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_major_dkv\"],\n",
    "              q.shape[2]),\n",
    "          block_k_major=min(\n",
    "              FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dkv\"],\n",
    "              k.shape[2]),\n",
    "          block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dkv\"],\n",
    "                      k.shape[2]),\n",
    "          block_q=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dkv\"],\n",
    "                      q.shape[2]),\n",
    "          sm_scale=sm_scale,\n",
    "          causal=causal,\n",
    "          mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "          debug=False,\n",
    "          static_argnames=[\n",
    "              \"block_q_major\", \"block_k_major\", \"block_k\", \"block_q\",\n",
    "              \"sm_scale\", \"causal\", \"mask_value\", \"debug\"\n",
    "          ],\n",
    "          use_cache=True)\n",
    "\n",
    "      grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                   [k.shape, v.shape],\n",
    "                                                   [k.dtype, v.dtype])\n",
    "\n",
    "    if ctx.needs_input_grad[1]:\n",
    "      grad_k = grads[0]\n",
    "    if ctx.needs_input_grad[2]:\n",
    "      grad_v = grads[1]\n",
    "\n",
    "    # SPMD integration\n",
    "    if partition_spec is not None:\n",
    "      grad_q = xs.disable_manual_sharding(\n",
    "          grad_q, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "      grad_k = xs.disable_manual_sharding(\n",
    "          grad_k, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "      grad_v = xs.disable_manual_sharding(\n",
    "          grad_v, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "\n",
    "    return grad_q, grad_k, grad_v, None, None, None, None, grad_ab, None, None\n",
    "\n",
    "\n",
    "def flash_attention(\n",
    "    q,  # [batch_size, num_heads, q_seq_len, d_model]\n",
    "    k,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    v,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    causal=False,\n",
    "    q_segment_ids=None,  # [batch_size, q_seq_len]\n",
    "    kv_segment_ids=None,  # [batch_size, kv_seq_len]\n",
    "    sm_scale=1.0,\n",
    "    *,\n",
    "    ab=None,  # [batch_size, num_heads, q_seq_len, kv_seq_len]\n",
    "    partition_spec=None,\n",
    "    mesh=None,\n",
    "):\n",
    "  # TODO: support SPMD and Dynamo with segment_ids.\n",
    "  return FlashAttention.apply(q, k, v, causal, q_segment_ids, kv_segment_ids,\n",
    "                              sm_scale, ab, partition_spec, mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_import_guard()\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _attention(q, k, v, *, attn_mask=None, ab=None):\n",
    "  attn_weight = q @ k.transpose(-2, -1)\n",
    "  if attn_mask is not None:\n",
    "    # Masked out the unrelevant parts.\n",
    "    attn_weight = attn_weight.masked_fill(attn_mask,\n",
    "                                          torch.finfo(attn_weight.dtype).min)\n",
    "  if ab is not None:\n",
    "    attn_weight = attn_weight + ab\n",
    "  attn_weight = nn.functional.softmax(attn_weight, dim=-1)\n",
    "  attn_output = attn_weight @ v\n",
    "  return attn_output\n",
    "\n",
    "\n",
    "def do_test(attn_fn):\n",
    "  from functorch.compile import aot_function\n",
    "  from torch_xla.experimental.custom_kernel import flash_attention\n",
    "\n",
    "  def flash_attention_wrapper(q, k, v):\n",
    "    return attn_fn(q, k, v)\n",
    "\n",
    "  q = torch.randn(3, 2, 128, 4).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "  k = torch.randn(3, 2, 128, 4).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "  v = torch.randn(3, 2, 128, 4).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "\n",
    "  q_clone = q.clone().detach().requires_grad_(True)\n",
    "  k_clone = k.clone().detach().requires_grad_(True)\n",
    "  v_clone = v.clone().detach().requires_grad_(True)\n",
    "  \n",
    "  def compiler(gm, _):\n",
    "    print(\"Got graph:\")\n",
    "    print(gm.code)\n",
    "    return gm\n",
    "\n",
    "  compiled_flash_attention = aot_function(\n",
    "      flash_attention_wrapper, fw_compiler=compiler)\n",
    "  o_actual = compiled_flash_attention(q, k, v)\n",
    "  o_actual.sum().backward()\n",
    "\n",
    "  expected_o = _attention(q_clone, k_clone, v_clone)\n",
    "  expected_o.sum().backward()\n",
    "\n",
    "  torch.testing.assert_close(o_actual.cpu(), expected_o.cpu())\n",
    "  assert q.grad is not None and q_clone.grad is not None, f\"{q.grad}, {q_clone.grad}\"\n",
    "  torch.testing.assert_close(q.grad.cpu(), q_clone.grad.cpu())\n",
    "  assert k.grad is not None and k_clone.grad is not None, f\"{k.grad}, {k_clone.grad}\"\n",
    "  torch.testing.assert_close(k.grad.cpu(), k_clone.grad.cpu())\n",
    "  assert v.grad is not None and v_clone.grad is not None, f\"{v.grad}, {v_clone.grad}\"\n",
    "  torch.testing.assert_close(v.grad.cpu(), v_clone.grad.cpu())\n",
    "\n",
    "\n",
    "def test_flash_attention_wrapper_with_aot_autograd(attn_fn):\n",
    "  jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "  try:\n",
    "    do_test(attn_fn)\n",
    "  finally:\n",
    "    jax.config.update(\"jax_default_matmul_precision\", \"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: without registering a backward pass, the custom op can't be used during\n",
    "backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.library import custom_op\n",
    "\n",
    "\n",
    "@custom_op(\"xla::flash_attention_xla_v2\", mutates_args=())\n",
    "def flash_attention_xla_v2(q: torch.Tensor,\n",
    "                           k: torch.Tensor,\n",
    "                           v: torch.Tensor,\n",
    "                           causal: bool = False) -> torch.Tensor:\n",
    "  return flash_attention(q, k, v, causal=causal)\n",
    "\n",
    "\n",
    "@flash_attention_xla_v2.register_fake\n",
    "def _(q, k, v, causal=False):\n",
    "  assert q.shape == k.shape\n",
    "  assert k.shape == v.shape\n",
    "  return torch.empty_like(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to backward through xla.flash_attention_xla_v2.default but no autograd formula was registered. Please use register_autograd to add one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Error detected in GeneratedBackwardFor_xla_flash_attention_xla_v2_defaultBackward. Traceback of forward call that caused the error:\n",
      " (Triggered internally at /workspaces/torch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  test_flash_attention_wrapper_with_aot_autograd(\n",
    "      torch.ops.xla.flash_attention_xla_v2)\n",
    "except RuntimeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: replace the forward/backward of FlashAttention with custom ops that\n",
    "don't have autograd themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@custom_op(\"xla::fa_custom_forward\", mutates_args=())\n",
    "def fa_custom_forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "  q = q.clone()\n",
    "  k = k.clone()\n",
    "  v = v.clone()\n",
    "\n",
    "  print(\"Inside fa_custom_forward\")\n",
    "  for t in [q, k, v]:\n",
    "    describe_value(t)\n",
    "\n",
    "  ctx = ProxyCtx()\n",
    "  q_segment_ids = kv_segment_ids = ab = partition_spec = mesh = None\n",
    "  sm_scale = 1.0\n",
    "  causal = False\n",
    "\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "  from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_impl\n",
    "\n",
    "  ctx.causal = causal\n",
    "  ctx.sm_scale = sm_scale\n",
    "  ctx.partition_spec = partition_spec\n",
    "  ctx.mesh = mesh\n",
    "  ctx.q_full_shape = None\n",
    "  ctx.kv_full_shape = None\n",
    "  save_residuals = True\n",
    "\n",
    "  # SPMD integration.\n",
    "  # mark_sharding is in-placed, and therefore save the full q, k, v for the backward.\n",
    "  full_q = q\n",
    "  full_k = k\n",
    "  full_v = v\n",
    "  full_ab = ab\n",
    "  if partition_spec is not None:\n",
    "    ctx.q_full_shape = q.shape\n",
    "    ctx.kv_full_shape = k.shape\n",
    "    q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "    k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "    v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "    if ab:\n",
    "      ab = xs.enable_manual_sharding(\n",
    "          ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "  # It computes the shape and type of o, l, m.\n",
    "  shapes = [q.shape]\n",
    "  dtypes = [q.dtype]\n",
    "  if save_residuals:\n",
    "    res_shape = list(q.shape)\n",
    "    res_shape[-1] = FlashAttention.MIN_BLOCK_SIZE\n",
    "    for _ in range(2):\n",
    "      shapes.append(res_shape)\n",
    "      dtypes.append(torch.float32)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    if partition_spec is not None and q_segment_ids is not None and kv_segment_ids is not None:\n",
    "      # partition_spec is for q,k,v with shape [batch, num_head, seq_len, head_dim], segment id\n",
    "      # is of shape [batch, seq_len], hence we need to tweak it a bit\n",
    "      segment_id_partition_spec = (partition_spec[0], partition_spec[2])\n",
    "      q_segment_ids = xs.enable_manual_sharding(\n",
    "          q_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "      kv_segment_ids = xs.enable_manual_sharding(\n",
    "          kv_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "    segment_ids, q_segment_ids_fa, kv_segment_ids_fa = FlashAttention.prepare_segment_ids(\n",
    "        q_segment_ids, kv_segment_ids)\n",
    "    ctx.segment_ids = segment_ids\n",
    "\n",
    "    # We can't directly use flash_attention as we need to override the save_residuals flag which returns\n",
    "    # l and m that is needed for the backward. Then we lose all the shape checks.\n",
    "    # TODO: replicate the shape checks on flash_attention.\n",
    "    # Here we seperate the tracing and execution part just to support SegmentIds.\n",
    "    payload, _ = trace_pallas(\n",
    "        _flash_attention_impl,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        ab,\n",
    "        segment_ids,\n",
    "        save_residuals,\n",
    "        causal,\n",
    "        sm_scale,\n",
    "        min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_b\"], q.shape[0]),\n",
    "        min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q\"], q.shape[2]),\n",
    "        min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major\"], k.shape[2]),\n",
    "        min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k\"], k.shape[2]),\n",
    "        False,\n",
    "        static_argnums=range(5, 13),\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    args = [q, k, v]\n",
    "    if ab is not None:\n",
    "      args += [ab]\n",
    "    if segment_ids is not None:\n",
    "      args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "    o = torch_xla._XLAC._xla_tpu_custom_call(args, payload, shapes, dtypes)\n",
    "\n",
    "    if not save_residuals:\n",
    "      o = o[0]\n",
    "      # SPMD integration\n",
    "      if partition_spec is not None:\n",
    "        o = xs.disable_manual_sharding(\n",
    "            o, partition_spec, ctx.q_full_shape, mesh=mesh).global_tensor\n",
    "      return o\n",
    "    o, *aux = o\n",
    "    l, m = (v[..., 0] for v in aux[-2:])\n",
    "\n",
    "  # SPMD integration\n",
    "  if partition_spec is not None:\n",
    "    o = xs.disable_manual_sharding(\n",
    "        o, partition_spec, ctx.q_full_shape, mesh=mesh).global_tensor\n",
    "    l = xs.disable_manual_sharding(\n",
    "        l, partition_spec[0:3], ctx.q_full_shape[0:3],\n",
    "        mesh=mesh).global_tensor\n",
    "    m = xs.disable_manual_sharding(\n",
    "        m, partition_spec[0:3], ctx.q_full_shape[0:3],\n",
    "        mesh=mesh).global_tensor\n",
    "\n",
    "  # q_segment_ids and kv_segment_ids are sharded here if partition_spec is provided\n",
    "  # but it should be OK as the backward will use the same partition_spec\n",
    "  ctx.save_for_backward(full_q, full_k, full_v, o, l, m)\n",
    "\n",
    "  outs = [o] + ctx.serialize()\n",
    "  print(\"Outs\")\n",
    "  for t in outs:\n",
    "    describe_value(t)\n",
    "  return tuple(outs)\n",
    "\n",
    "\n",
    "@fa_custom_forward.register_fake\n",
    "def _(q, k, v):\n",
    "  print(\"Inside fake fa_custom_forward\")\n",
    "\n",
    "  assert q.shape == k.shape\n",
    "  assert k.shape == v.shape\n",
    "  ctx = ProxyCtx()\n",
    "\n",
    "  # full_q, full_k, full_v, o, l, m, q_segment_ids_fa, kv_segment_ids_fa, full_ab\n",
    "  full_q = torch.empty_like(q)\n",
    "  full_k = torch.empty_like(k)\n",
    "  full_v = torch.empty_like(v)\n",
    "  o = torch.empty_like(v)\n",
    "  l = torch.empty_like(v)[..., 0]\n",
    "  m = torch.empty_like(v)[..., 0]\n",
    "  q_segment_ids_fa = None\n",
    "  kv_segment_ids_fa = None\n",
    "  full_ab = None\n",
    "  ctx.save_for_backward(\n",
    "      full_q,\n",
    "      full_k,\n",
    "      full_v,\n",
    "      o,\n",
    "      l,\n",
    "      m,\n",
    "  )\n",
    "\n",
    "  return tuple([torch.empty_like(o)] + [torch.empty_like(t) for t in ctx.serialize()])\n",
    "\n",
    "\n",
    "@custom_op(\"xla::fa_custom_backward\", mutates_args=())\n",
    "def fa_custom_backward(grad_output: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, o: torch.Tensor, l: torch.Tensor, m: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "  q_segment_ids_fa = kv_segment_ids_fa = ab = None\n",
    "  grad_output = grad_output.clone()\n",
    "\n",
    "  print(\"Inside fa_custom_backward\")\n",
    "\n",
    "  from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_bwd_dq, _flash_attention_bwd_dkv\n",
    "\n",
    "  saved_tensors = (q, k, v, o, l, m)\n",
    "  q, k, v, o, l, m = (deserialize(t.clone()) for t in saved_tensors)\n",
    "  causal = False\n",
    "  sm_scale = 1.0\n",
    "  partition_spec = None\n",
    "  mesh = None\n",
    "  q_full_shape = None\n",
    "  kv_full_shape = None\n",
    "  # this segment_ids only reflects the local shape of segment_ids\n",
    "  segment_ids = None\n",
    "  grad_q = grad_k = grad_v = grad_ab = None\n",
    "  needs_input_grad = [True, True, True]\n",
    "\n",
    "  grad_i = torch.sum(\n",
    "      o.to(torch.float32) * grad_output.to(torch.float32),\n",
    "      axis=-1)  # [batch_size, num_heads, q_seq_len]\n",
    "\n",
    "  expanded_l = l.unsqueeze(-1).expand([-1 for _ in l.shape] +\n",
    "                                      [FlashAttention.MIN_BLOCK_SIZE])\n",
    "  expanded_m = m.unsqueeze(-1).expand([-1 for _ in m.shape] +\n",
    "                                      [FlashAttention.MIN_BLOCK_SIZE])\n",
    "  expanded_grad_i = grad_i.unsqueeze(-1).expand(\n",
    "      [-1 for _ in grad_i.shape] + [FlashAttention.MIN_BLOCK_SIZE])\n",
    "\n",
    "  # SPMD integration\n",
    "  if partition_spec is not None:\n",
    "    q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "    k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "    v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_l = xs.enable_manual_sharding(\n",
    "        expanded_l, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_m = xs.enable_manual_sharding(\n",
    "        expanded_m, partition_spec, mesh=mesh).global_tensor\n",
    "    grad_output = xs.enable_manual_sharding(\n",
    "        grad_output, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_grad_i = xs.enable_manual_sharding(\n",
    "        expanded_grad_i, partition_spec, mesh=mesh).global_tensor\n",
    "    if ab:\n",
    "      ab = xs.enable_manual_sharding(\n",
    "          ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "  if needs_input_grad[0]:\n",
    "    payload, _ = trace_pallas(\n",
    "        _flash_attention_bwd_dq,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        ab,\n",
    "        segment_ids,\n",
    "        l,\n",
    "        m,\n",
    "        grad_output,\n",
    "        grad_i,\n",
    "        block_q_major=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dq\"],\n",
    "                          q.shape[2]),\n",
    "        block_k_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dq\"],\n",
    "            k.shape[2]),\n",
    "        block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dq\"],\n",
    "                    k.shape[2]),\n",
    "        sm_scale=sm_scale,\n",
    "        causal=causal,\n",
    "        mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "        debug=False,\n",
    "        static_argnames=[\n",
    "            \"block_q_major\", \"block_k_major\", \"block_k\", \"sm_scale\", \"causal\",\n",
    "            \"mask_value\", \"debug\"\n",
    "        ],\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    args = [q, k, v]\n",
    "    if ab is not None:\n",
    "      args += [ab]\n",
    "    if segment_ids is not None:\n",
    "      args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "    args += [expanded_l, expanded_m, grad_output, expanded_grad_i]\n",
    "\n",
    "    outputs = [q]\n",
    "    if ab is not None:\n",
    "      outputs += [ab]\n",
    "    grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                 [i.shape for i in outputs],\n",
    "                                                 [i.dtype for i in outputs])\n",
    "    if needs_input_grad[0]:\n",
    "      grad_q = grads[0]\n",
    "\n",
    "  if needs_input_grad[1] or needs_input_grad[2]:\n",
    "    payload, _ = trace_pallas(\n",
    "        _flash_attention_bwd_dkv,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        ab,\n",
    "        segment_ids,\n",
    "        l,\n",
    "        m,\n",
    "        grad_output,\n",
    "        grad_i,\n",
    "        block_q_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_major_dkv\"],\n",
    "            q.shape[2]),\n",
    "        block_k_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dkv\"],\n",
    "            k.shape[2]),\n",
    "        block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dkv\"],\n",
    "                    k.shape[2]),\n",
    "        block_q=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dkv\"],\n",
    "                    q.shape[2]),\n",
    "        sm_scale=sm_scale,\n",
    "        causal=causal,\n",
    "        mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "        debug=False,\n",
    "        static_argnames=[\n",
    "            \"block_q_major\", \"block_k_major\", \"block_k\", \"block_q\",\n",
    "            \"sm_scale\", \"causal\", \"mask_value\", \"debug\"\n",
    "        ],\n",
    "        use_cache=True)\n",
    "\n",
    "    grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                 [k.shape, v.shape],\n",
    "                                                 [k.dtype, v.dtype])\n",
    "\n",
    "  if needs_input_grad[1]:\n",
    "    grad_k = grads[0]\n",
    "  if needs_input_grad[2]:\n",
    "    grad_v = grads[1]\n",
    "\n",
    "  # SPMD integration\n",
    "  if partition_spec is not None:\n",
    "    grad_q = xs.disable_manual_sharding(\n",
    "        grad_q, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "    grad_k = xs.disable_manual_sharding(\n",
    "        grad_k, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "    grad_v = xs.disable_manual_sharding(\n",
    "        grad_v, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "\n",
    "  return grad_q, grad_k, grad_v\n",
    "\n",
    "\n",
    "@fa_custom_backward.register_fake\n",
    "def _(grad_o, q, k, v, o, l, m):\n",
    "  print(\"Inside fake fa_custom_backward\")\n",
    "  return torch.empty_like(grad_o), torch.empty_like(grad_o), torch.empty_like(grad_o)\n",
    "\n",
    "\n",
    "class FlashAttention2(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, q, k, v, causal, q_segment_ids, kv_segment_ids, sm_scale, ab,\n",
    "              partition_spec, mesh):\n",
    "    outs = fa_custom_forward(q, k, v)\n",
    "    print(\"forward done with fa_custom_forward\")\n",
    "    \n",
    "    o = outs[0]\n",
    "    saved = outs[1:]\n",
    "    proxy_ctx = ProxyCtx.deserialize(saved)\n",
    "    full_q, full_k, full_v, o2, l, m = proxy_ctx.saved_tensors\n",
    "\n",
    "    # q_segment_ids and kv_segment_ids are sharded here if partition_spec is provided\n",
    "    # but it should be OK as the backward will use the same partition_spec\n",
    "    ctx.save_for_backward(full_q, full_k, full_v, o2, l, m)\n",
    "    return o\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    grad_ab = None\n",
    "    print(\"Inside backward\")\n",
    "    \n",
    "    saved = [serialize(v) for v in ctx.saved_tensors]\n",
    "    for t in [grad_output] + saved:\n",
    "      describe_value(t)\n",
    "\n",
    "    return fa_custom_backward(grad_output, *saved) + (None, None, None, None, grad_ab, None, None)\n",
    "\n",
    "\n",
    "def flash_attention_2(\n",
    "    q,  # [batch_size, num_heads, q_seq_len, d_model]\n",
    "    k,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    v,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    causal=False,\n",
    "    q_segment_ids=None,  # [batch_size, q_seq_len]\n",
    "    kv_segment_ids=None,  # [batch_size, kv_seq_len]\n",
    "    sm_scale=1.0,\n",
    "    *,\n",
    "    ab=None,  # [batch_size, num_heads, q_seq_len, kv_seq_len]\n",
    "    partition_spec=None,\n",
    "    mesh=None,\n",
    "):\n",
    "  # TODO: support SPMD and Dynamo with segment_ids.\n",
    "  return FlashAttention2.apply(q, k, v, causal, q_segment_ids, kv_segment_ids,\n",
    "                              sm_scale, ab, partition_spec, mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside fake fa_custom_forward\n",
      "forward done with fa_custom_forward\n",
      "Inside fake fa_custom_forward\n",
      "forward done with fa_custom_forward\n",
      "Inside backward\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch._subclasses.functional_tensor.FunctionalTensor'>(torch.Size([3, 2, 128]), dtype=torch.float32, device=xla:0)\n",
      "Inside fake fa_custom_backward\n",
      "Got graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3):\n",
      "    fa_custom_forward = torch.ops.xla.fa_custom_forward.default(primals_1, primals_2, primals_3);  primals_1 = primals_2 = primals_3 = None\n",
      "    getitem = fa_custom_forward[0]\n",
      "    getitem_1 = fa_custom_forward[1]\n",
      "    getitem_2 = fa_custom_forward[2]\n",
      "    getitem_3 = fa_custom_forward[3]\n",
      "    getitem_4 = fa_custom_forward[4]\n",
      "    getitem_5 = fa_custom_forward[5]\n",
      "    getitem_6 = fa_custom_forward[6];  fa_custom_forward = None\n",
      "    return (getitem, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5, getitem_6)\n",
      "    \n",
      "Inside fa_custom_forward\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "Outs\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128, 4]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128]), dtype=torch.float32, device=xla:0)\n",
      "<class 'torch.Tensor'>(torch.Size([3, 2, 128]), dtype=torch.float32, device=xla:0)\n",
      "Got graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5, getitem_6, tangents_1):\n",
      "    clone = torch.ops.aten.clone.default(getitem_1);  getitem_1 = None\n",
      "    clone_1 = torch.ops.aten.clone.default(getitem_2);  getitem_2 = None\n",
      "    clone_2 = torch.ops.aten.clone.default(getitem_3);  getitem_3 = None\n",
      "    clone_3 = torch.ops.aten.clone.default(getitem_4);  getitem_4 = None\n",
      "    clone_4 = torch.ops.aten.clone.default(getitem_5);  getitem_5 = None\n",
      "    clone_5 = torch.ops.aten.clone.default(getitem_6);  getitem_6 = None\n",
      "    clone_6 = torch.ops.aten.clone.default(clone);  clone = None\n",
      "    clone_7 = torch.ops.aten.clone.default(clone_1);  clone_1 = None\n",
      "    clone_8 = torch.ops.aten.clone.default(clone_2);  clone_2 = None\n",
      "    clone_9 = torch.ops.aten.clone.default(clone_3);  clone_3 = None\n",
      "    clone_10 = torch.ops.aten.clone.default(clone_4);  clone_4 = None\n",
      "    clone_11 = torch.ops.aten.clone.default(clone_5);  clone_5 = None\n",
      "    fa_custom_backward = torch.ops.xla.fa_custom_backward.default(tangents_1, clone_6, clone_7, clone_8, clone_9, clone_10, clone_11);  tangents_1 = clone_6 = clone_7 = clone_8 = clone_9 = clone_10 = clone_11 = None\n",
      "    getitem_7 = fa_custom_backward[0]\n",
      "    getitem_8 = fa_custom_backward[1]\n",
      "    getitem_9 = fa_custom_backward[2];  fa_custom_backward = None\n",
      "    return (getitem_7, getitem_8, getitem_9)\n",
      "    \n",
      "Inside fa_custom_backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_flash_attention_wrapper_with_aot_autograd(flash_attention_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
