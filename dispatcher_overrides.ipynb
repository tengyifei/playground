{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Python and Native overrides\n",
    "\n",
    "This notebook demonstrates `__torch_function__` and `__torch_dispatch__` and its\n",
    "interaction with XLA:\n",
    "\n",
    "- `__torch_function__` may be activated by a `TorchFunctionMode` context manager,\n",
    "  and intercepts `torch.` function calls. It won't see what operations are run in\n",
    "  a `loss.backward()`, for example.\n",
    "  \n",
    "- `__torch_dispatch__` may be activated by a `TorchDispatchMode` context manager,\n",
    "  and intercepts `aten` operations right before they get dispatched to a backend.\n",
    "  It will see what `aten` operations are run by `loss.backward()`.\n",
    "  \n",
    "For more details, see https://pytorch.org/docs/stable/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: TF_CPP_MIN_LOG_LEVEL=0\n",
      "env: TF_CPP_VMODULE=xla_graph_executor=8,ir=8\n"
     ]
    }
   ],
   "source": [
    "%env XLA_IR_DEBUG=1\n",
    "\n",
    "# Turn on INFO logs.\n",
    "%env TF_CPP_MIN_LOG_LEVEL=0\n",
    "\n",
    "# Turn on verbose INFO logs for these `.cc` files.\n",
    "# %env TF_CPP_MAX_VLOG_LEVEL=8\n",
    "%env TF_CPP_VMODULE=xla_graph_executor=8,ir=8\n",
    "# %env TORCH_SHOW_DISPATCH_TRACE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "import torch\n",
    "from torch.overrides import TorchFunctionMode, resolve_name\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "\n",
    "def arg_shapes(args: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Inspects each input argument and prints its shape if it's a PyTorch tensor, \n",
    "    otherwise prints the repr (unchanged).\n",
    "\n",
    "    Args:\n",
    "        *args: Variable number of arguments of any type.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated string of argument representations.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for arg in args:\n",
    "        if isinstance(arg, torch.Tensor):\n",
    "            result.append(f\"tensor(shape={tuple(list(arg.shape))})\")  # Get shape, not data\n",
    "        else:\n",
    "            result.append(repr(arg))\n",
    "    return ', '.join(result)\n",
    "\n",
    "\n",
    "class FunctionLog(TorchFunctionMode):\n",
    "    def __torch_function__(self, func, types, args, kwargs=None):\n",
    "        print(f\"Function Log: {resolve_name(func)}({arg_shapes(args)}, **{kwargs})\")\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "class DispatchLog(TorchDispatchMode):\n",
    "    def __torch_dispatch__(self, func, types, args, kwargs=None):\n",
    "        print(f\"Dispatch Log: {func}({arg_shapes(args)}, **{kwargs})\")\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "def f():\n",
    "    a = torch.rand(10, requires_grad=True)\n",
    "    b = torch.sin(a * 2)\n",
    "    loss = b.sum()\n",
    "    print(\"Backward\")\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchFunctionMode logging:\n",
      "Function Log: torch.rand(10, **{'requires_grad': True})\n",
      "Function Log: torch.Tensor.mul(tensor(shape=(10,)), 2, **None)\n",
      "Function Log: torch.sin(tensor(shape=(10,)), **None)\n",
      "Function Log: torch.Tensor.sum(tensor(shape=(10,)), **None)\n",
      "Backward\n",
      "Function Log: torch.Tensor.backward(tensor(shape=()), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TorchFunctionMode logging:\")\n",
    "with FunctionLog():\n",
    "    f()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchDispatchMode logging:\n",
      "Dispatch Log: aten.rand.default([10], **{'device': device(type='cpu'), 'pin_memory': False})\n",
      "Dispatch Log: aten.mul.Tensor(tensor(shape=(10,)), 2, **{})\n",
      "Dispatch Log: aten.sin.default(tensor(shape=(10,)), **{})\n",
      "Dispatch Log: aten.sum.default(tensor(shape=(10,)), **{})\n",
      "Backward\n",
      "Dispatch Log: aten.ones_like.default(tensor(shape=()), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n",
      "Dispatch Log: aten.expand.default(tensor(shape=()), [10], **{})\n",
      "Dispatch Log: aten.cos.default(tensor(shape=(10,)), **{})\n",
      "Dispatch Log: aten.mul.Tensor(tensor(shape=(10,)), tensor(shape=(10,)), **{})\n",
      "Dispatch Log: aten.mul.Tensor(tensor(shape=(10,)), 2, **{})\n",
      "Dispatch Log: aten.detach.default(tensor(shape=(10,)), **{})\n",
      "Dispatch Log: aten.detach.default(tensor(shape=(10,)), **{})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TorchDispatchMode logging:\")\n",
    "with DispatchLog():\n",
    "    f()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g():\n",
    "    import torch\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import time\n",
    "\n",
    "    a = torch.rand((1000, 1000), requires_grad=True, device=torch_xla.device())\n",
    "    time.sleep(1)\n",
    "    b = torch.rand((1000, 1000), requires_grad=True, device=torch_xla.device())\n",
    "    time.sleep(1)\n",
    "    c = a @ b @ a @ b\n",
    "    time.sleep(1)\n",
    "    d = c.sum()\n",
    "    time.sleep(1)\n",
    "    print(\"Mark step\")\n",
    "    time.sleep(1)\n",
    "    xm.mark_step()\n",
    "    time.sleep(1)\n",
    "    print(d)\n",
    "    time.sleep(1)\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchFunctionMode logging:\n",
      "Function Log: torch.device('xla:0', **None)\n",
      "Function Log: torch.rand((1000, 1000), **{'requires_grad': True, 'device': device(type='xla', index=0)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:22.469995: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::expand\n",
      "2024-08-05 01:58:22.471368: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mul\n",
      "2024-08-05 01:58:22.471457: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::add\n",
      "2024-08-05 01:58:22.471607: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: torch.device('xla:0', **None)\n",
      "Function Log: torch.rand((1000, 1000), **{'requires_grad': True, 'device': device(type='xla', index=0)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:23.473611: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::expand\n",
      "2024-08-05 01:58:23.474479: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mul\n",
      "2024-08-05 01:58:23.474558: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::add\n",
      "2024-08-05 01:58:23.474705: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: torch.Tensor.matmul(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **None)\n",
      "Function Log: torch.Tensor.matmul(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **None)\n",
      "Function Log: torch.Tensor.matmul(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:24.476417: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n",
      "2024-08-05 01:58:24.476969: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n",
      "2024-08-05 01:58:24.477298: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: torch.Tensor.sum(tensor(shape=(1000, 1000)), **None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:25.479016: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::sum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:27.481775: I torch_xla/csrc/xla_graph_executor.cpp:422] 6 live tensors: devices=()\n",
      "2024-08-05 01:58:27.481826: I torch_xla/csrc/xla_graph_executor.cpp:400] Trying to sync the value of 6 tensor(s)\n",
      "2024-08-05 01:58:27.481936: I torch_xla/csrc/xla_graph_executor.cpp:714] Tensors graph hash 1a89380de694b4919304f231485b7de1 on device TPU:0\n",
      "2024-08-05 01:58:27.482877: I torch_xla/csrc/xla_graph_executor.cpp:1514] Parameter sequence graph hash 90ec8fd148146797d064fba641daf419\n",
      "2024-08-05 01:58:27.484687: I torch_xla/csrc/xla_graph_executor.cpp:1206] Graph hash 90ec8fd148146797d064fba641daf419 is computation hash d664d514aa8275302560d0223f778dd6\n",
      "2024-08-05 01:58:27.484712: I torch_xla/csrc/xla_graph_executor.cpp:1232] TensorsGraphSize=19\n",
      "2024-08-05 01:58:27.484742: I torch_xla/csrc/xla_graph_executor.cpp:722] waiting barrier for device TPU:0 start\n",
      "2024-08-05 01:58:27.484783: I torch_xla/csrc/xla_graph_executor.cpp:725] waiting barrier for device TPU:0 done\n",
      "2024-08-05 01:58:27.485426: I torch_xla/csrc/xla_graph_executor.cpp:1122] Executing IR graph hash 90ec8fd148146797d064fba641daf419 on device TPU:0 ...\n",
      "2024-08-05 01:58:27.486169: I torch_xla/csrc/xla_graph_executor.cpp:1133] Executing IR graph hash 90ec8fd148146797d064fba641daf419 on device TPU:0 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: torch.device('xla:0', **None)\n",
      "Function Log: torch.Tensor.__repr__(tensor(shape=()), **{'tensor_contents': None})\n",
      "tensor(6.2506e+13, device='xla:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"TorchFunctionMode logging:\")\n",
    "with FunctionLog():\n",
    "    g()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchDispatchMode logging:\n",
      "Dispatch Log: aten.rand.default([1000, 1000], **{'device': device(type='xla', index=0), 'pin_memory': False})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:29.506924: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::expand\n",
      "2024-08-05 01:58:29.507616: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mul\n",
      "2024-08-05 01:58:29.507701: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::add\n",
      "2024-08-05 01:58:29.507869: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.rand.default([1000, 1000], **{'device': device(type='xla', index=0), 'pin_memory': False})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:30.510177: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::expand\n",
      "2024-08-05 01:58:30.510617: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mul\n",
      "2024-08-05 01:58:30.510706: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::add\n",
      "2024-08-05 01:58:30.510848: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.mm.default(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **{})\n",
      "Dispatch Log: aten.mm.default(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **{})\n",
      "Dispatch Log: aten.mm.default(tensor(shape=(1000, 1000)), tensor(shape=(1000, 1000)), **{})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:31.512893: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n",
      "2024-08-05 01:58:31.513474: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n",
      "2024-08-05 01:58:31.513821: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.sum.default(tensor(shape=(1000, 1000)), **{})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:32.516048: I torch_xla/csrc/ir.cpp:53] Create XlaNode for aten::sum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 01:58:34.518880: I torch_xla/csrc/xla_graph_executor.cpp:422] 6 live tensors: devices=()\n",
      "2024-08-05 01:58:34.518910: I torch_xla/csrc/xla_graph_executor.cpp:400] Trying to sync the value of 6 tensor(s)\n",
      "2024-08-05 01:58:34.518982: I torch_xla/csrc/xla_graph_executor.cpp:714] Tensors graph hash 1a89380de694b4919304f231485b7de1 on device TPU:0\n",
      "2024-08-05 01:58:34.519587: I torch_xla/csrc/xla_graph_executor.cpp:1514] Parameter sequence graph hash 90ec8fd148146797d064fba641daf419\n",
      "2024-08-05 01:58:34.521405: I torch_xla/csrc/xla_graph_executor.cpp:1206] Graph hash 90ec8fd148146797d064fba641daf419 is computation hash d664d514aa8275302560d0223f778dd6\n",
      "2024-08-05 01:58:34.521428: I torch_xla/csrc/xla_graph_executor.cpp:1232] TensorsGraphSize=19\n",
      "2024-08-05 01:58:34.521454: I torch_xla/csrc/xla_graph_executor.cpp:722] waiting barrier for device TPU:0 start\n",
      "2024-08-05 01:58:34.521488: I torch_xla/csrc/xla_graph_executor.cpp:725] waiting barrier for device TPU:0 done\n",
      "2024-08-05 01:58:34.521715: I torch_xla/csrc/xla_graph_executor.cpp:1122] Executing IR graph hash 90ec8fd148146797d064fba641daf419 on device TPU:0 ...\n",
      "2024-08-05 01:58:34.522619: I torch_xla/csrc/xla_graph_executor.cpp:1133] Executing IR graph hash 90ec8fd148146797d064fba641daf419 on device TPU:0 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2446e+13, device='xla:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TorchDispatchMode logging:\")\n",
    "with DispatchLog():\n",
    "    g()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
