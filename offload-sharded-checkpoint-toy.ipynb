{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "\n",
    "# MaxText flags except that we disable optimization barrier removal\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch_xla import runtime as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "      return self.m(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder_only_model\n",
    "from trainer import TrainDecoderOnlyBase\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1D FSDP sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch_xla.experimental.spmd_fully_sharded_data_parallel import SpmdFullyShardedDataParallel as FSDPv2\n",
    "from torch_xla import runtime as xr\n",
    "from torch_xla.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "# checkout our doc at https://github.com/pytorch/xla/blob/master/docs/fsdpv2.md\n",
    "class TrainDecoderOnlyFSDPv2(TrainDecoderOnlyBase):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(decoder_only_model.DecoderOnlyConfig(\n",
    "       hidden_size=4096,\n",
    "       num_hidden_layers=32,\n",
    "       num_attention_heads=16,\n",
    "       num_key_value_heads=8,\n",
    "       intermediate_size=8192,\n",
    "       vocab_size=16384,\n",
    "    ))\n",
    "    # Define the mesh following common SPMD practice\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    mesh_shape = (num_devices, 1)\n",
    "    device_ids = np.array(range(num_devices))\n",
    "    # To be noted, the mesh must have an axis named 'fsdp', which the weights and activations will be sharded on.\n",
    "    mesh = xs.Mesh(device_ids, mesh_shape, ('fsdp', 'model'))\n",
    "    xs.set_global_mesh(mesh)\n",
    "\n",
    "    # Shard the input(data parallel).\n",
    "    # Scale the batch size with num_devices since there will be only one\n",
    "    # process that handles all runtime devices.\n",
    "    self.batch_size *= num_devices\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.zeros(self.batch_size, self.seq_len, dtype=torch.int64),\n",
    "              torch.zeros(self.batch_size, self.seq_len, dtype=torch.int64)),\n",
    "        sample_count=self.train_dataset_len // self.batch_size)\n",
    "    self.train_device_loader = pl.MpDeviceLoader(\n",
    "        train_loader,\n",
    "        self.device,\n",
    "        # Shard the input's batch dimension along the `fsdp` axis, no sharding along other dimensions\n",
    "        input_sharding=xs.ShardingSpec(mesh, ('fsdp', None)))  # type:ignore\n",
    "    \n",
    "    model: decoder_only_model.DecoderOnlyModel = self.model  # type:ignore\n",
    "    self.model = model\n",
    "\n",
    "    # Apply checkpoint to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = checkpoint_module(block)\n",
    "        \n",
    "    # Apply offloading to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = OffloadingModule(block)\n",
    "\n",
    "    # Apply FSDP sharding on each DecoderLayer layer.\n",
    "    auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            OffloadingModule\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # FSDPv2 will use the global mesh set above\n",
    "    self.model: torch.nn.Module = self.model\n",
    "    self.model = FSDPv2(\n",
    "        self.model, auto_wrap_policy=auto_wrap_policy)\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train begin  6:52AM UTC on Nov 09, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:184: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: 10.497030258178711, rate: 3.885934977170958\n",
      "epoch: 1, step: 1, loss: 10.308744430541992, rate: 59.265707754513116\n",
      "Epoch 1 train end  6:54AM UTC on Nov 09, 2024\n",
      "epoch: 1, step: 2, loss: 10.120159149169922, rate: 81.529329088687\n",
      "Profiling model\n",
      "Epoch 1 train begin  6:54AM UTC on Nov 09, 2024\n",
      "Starting to trace for 10000 ms. Remaining attempt(s): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 06:54:28.902295: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 951080 nanoseconds and will start immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: 9.933348655700684, rate: 92.53957494213485\n",
      "epoch: 1, step: 1, loss: 9.745992660522461, rate: 94.92735715523048\n",
      "epoch: 1, step: 2, loss: 9.557472229003906, rate: 95.92802707093577\n",
      "epoch: 1, step: 3, loss: 9.36929988861084, rate: 96.16945501334894\n",
      "epoch: 1, step: 4, loss: 9.18035888671875, rate: 96.41094065149827\n",
      "epoch: 1, step: 5, loss: 8.997147560119629, rate: 96.5329254760474\n",
      "epoch: 1, step: 6, loss: 8.807817459106445, rate: 96.57038474682722\n",
      "epoch: 1, step: 7, loss: 8.617790222167969, rate: 96.56631265184365\n",
      "epoch: 1, step: 8, loss: 8.432229042053223, rate: 96.55531166528806\n",
      "epoch: 1, step: 9, loss: 8.246435165405273, rate: 96.54987734217524\n",
      "epoch: 1, step: 10, loss: 8.05585765838623, rate: 96.5551424700034\n",
      "epoch: 1, step: 11, loss: 7.868907928466797, rate: 96.52002129625203\n",
      "epoch: 1, step: 12, loss: 7.681149482727051, rate: 96.50362104908496\n",
      "epoch: 1, step: 13, loss: 7.494668960571289, rate: 96.45459015651032\n",
      "epoch: 1, step: 14, loss: 7.307222843170166, rate: 96.41116498773968\n",
      "epoch: 1, step: 15, loss: 7.119393348693848, rate: 96.36540928137434\n",
      "epoch: 1, step: 16, loss: 6.933435440063477, rate: 96.3466506489122\n",
      "epoch: 1, step: 17, loss: 6.748883247375488, rate: 96.33784300833551\n",
      "epoch: 1, step: 18, loss: 6.562691688537598, rate: 96.3121603550798\n",
      "Epoch 1 train end  6:56AM UTC on Nov 09, 2024\n"
     ]
    }
   ],
   "source": [
    "xr.use_spmd()\n",
    "\n",
    "print(\"Compiling model\")\n",
    "base = TrainDecoderOnlyFSDPv2()\n",
    "base.num_steps = 3\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "print(\"Profiling model\")\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)\n",
    "xp.trace_detached(\n",
    "    service_addr=\"localhost:9012\", logdir=\"profile/\", duration_ms=10000)\n",
    "base.num_steps = 5\n",
    "base.start_training()\n",
    "del server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2D (FSDP, TP) sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
