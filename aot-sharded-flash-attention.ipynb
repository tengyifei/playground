{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we test an AOTAutograd-friendly flash attention kernel with\n",
    "sharded inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
      "env: XLA_SAVE_TENSORS_FILE=ir_dumps/aot-sharded-flash-attention.txt\n",
      "env: XLA_SAVE_TENSORS_FMT=text\n",
      "env: XLA_DEBUG_HLO=1\n",
      "env: XLA_DEBUG_IR=1\n",
      "env: XLA_FLAGS=--xla_dump_to=xla_dumps/aot-sharded-flash-attention\n",
      "env: PT_XLA_DEBUG_LEVEL=2\n"
     ]
    }
   ],
   "source": [
    "%env TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
    "%env XLA_SAVE_TENSORS_FILE=ir_dumps/aot-sharded-flash-attention.txt\n",
    "%env XLA_SAVE_TENSORS_FMT=text\n",
    "%env XLA_DEBUG_HLO=1\n",
    "%env XLA_DEBUG_IR=1\n",
    "%env XLA_FLAGS=--xla_dump_to=xla_dumps/aot-sharded-flash-attention\n",
    "%env PT_XLA_DEBUG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "import torch_xla.distributed.spmd as xs\n",
    "\n",
    "torch_xla.runtime.use_spmd()\n",
    "\n",
    "num_devices = torch_xla.runtime.global_runtime_device_count()\n",
    "assert num_devices > 4\n",
    "\n",
    "mesh_shape = (num_devices // 2, 2)\n",
    "spmd_mesh = xs.Mesh(list(range(num_devices)), mesh_shape, ('fsdp', 'tensor'))\n",
    "xs.set_global_mesh(spmd_mesh)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(42)\n",
    "torch_xla._XLAC._xla_set_mat_mul_precision('highest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOTAutograd-friendly flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVOID_AS_STRIDED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from typing import List\n",
    "import functools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.debug.metrics as met\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Tuple, Dict\n",
    "from torch.library import impl, custom_op\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "from torch_xla.experimental.custom_kernel import FlashAttention\n",
    "import torch_xla.debug.profiler as xp\n",
    "\n",
    "_XLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\") == \"1\"\n",
    "\n",
    "_DEBUG = False\n",
    "\n",
    "\n",
    "def describe_value(v):\n",
    "  if v is not None and isinstance(v, torch.Tensor):\n",
    "    print(f\"{type(v)}({v.shape}, dtype={v.dtype}, device={v.device})\")\n",
    "  elif isinstance(v, list):\n",
    "    print(f\"list({len(v)})\")\n",
    "  elif v is None:\n",
    "    print(\"None\")\n",
    "  else:\n",
    "    print(type(v))\n",
    "\n",
    "\n",
    "def _extract_backend_config(\n",
    "    module: \"jaxlib.mlir._mlir_libs._mlir.ir.Module\") -> Optional[str]:\n",
    "  \"\"\"\n",
    "  This algorithm intends to extract the backend config from the compiler IR like the following,\n",
    "  and it is not designed to traverse any generic MLIR module.\n",
    "\n",
    "  module @jit_add_vectors attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
    "    func.func public @main(%arg0: tensor<8xi32> {mhlo.layout_mode = \"default\", mhlo.sharding = \"{replicated}\"}, %arg1: tensor<8xi32> {mhlo.layout_mode = \"default\", mhlo.sharding = \"{replicated}\"}) -> (tensor<8xi32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
    "      %0 = call @add_vectors(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @add_vectors(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = call @wrapped(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @wrapped(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = call @apply_kernel(%arg0, %arg1) : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "    func.func private @apply_kernel(%arg0: tensor<8xi32>, %arg1: tensor<8xi32>) -> tensor<8xi32> {\n",
    "      %0 = stablehlo.custom_call @tpu_custom_call(%arg0, %arg1) {backend_config = \"{\\22custom_call_config\\22: {\\22body\\22: \\22TUzvUgFNTElSMTkuMC4wZ2l0AAErCwEDBQcJAQMLAwUDDQcFDxEJBRMVA3lZDQFVBwsPEw8PCw8PMwsLCwtlCwsLCwsPCw8PFw8LFw8PCxcPCxcTCw8LDxcLBQNhBwNZAQ0bBxMPGw8CagMfBRcdKy0DAycpHVMREQsBBRkVMzkVTw8DCxUXGRsfCyELIyUFGwEBBR0NCWFmZmluZV9tYXA8KGQwKSAtPiAoZDApPgAFHwUhBSMFJQUnEQMBBSkVLw8dDTEXA8IfAR01NwUrFwPWHwEVO0EdPT8FLRcD9h8BHUNFBS8XA3InAQMDSVcFMR1NEQUzHQ1RFwPGHwEFNSN0cHUubWVtb3J5X3NwYWNlPHZtZW0+ACNhcml0aC5vdmVyZmxvdzxub25lPgAXVQMhBx0DJwMhBwECAgUHAQEBAQECBASpBQEQAQcDAQUDEQETBwMVJwcBAQEBAQEHAwUHAwMLBgUDBQUBBwcDBQcDAwsGBQMFBQMLCQdLRwMFBQkNBwMJBwMDCwYJAwUFBRENBAkHDwURBQABBgMBBQEAxgg32wsdE2EZ2Q0LEyMhHSknaw0LCxMPDw8NCQsRYnVpbHRpbgBmdW5jAHRwdQBhcml0aAB2ZWN0b3IAbW9kdWxlAHJldHVybgBjb25zdGFudABhZGRpAGxvYWQAc3RvcmUAL3dvcmtzcGFjZXMvd29yay9weXRvcmNoL3hsYS90ZXN0L3Rlc3Rfb3BlcmF0aW9ucy5weQBhZGRfdmVjdG9yc19rZXJuZWwAZGltZW5zaW9uX3NlbWFudGljcwBmdW5jdGlvbl90eXBlAHNjYWxhcl9wcmVmZXRjaABzY3JhdGNoX29wZXJhbmRzAHN5bV9uYW1lAG1haW4AdmFsdWUAL2dldFt0cmVlPVB5VHJlZURlZigoQ3VzdG9tTm9kZShOREluZGV4ZXJbKFB5VHJlZURlZigoQ3VzdG9tTm9kZShTbGljZVsoMCwgOCldLCBbXSksKSksICg4LCksICgpKV0sIFtdKSwpKV0AYWRkX3ZlY3RvcnMAdGVzdF90cHVfY3VzdG9tX2NhbGxfcGFsbGFzX2V4dHJhY3RfYWRkX3BheWxvYWQAPG1vZHVsZT4Ab3ZlcmZsb3dGbGFncwAvYWRkAC9zd2FwW3RyZWU9UHlUcmVlRGVmKChDdXN0b21Ob2RlKE5ESW5kZXhlclsoUHlUcmVlRGVmKChDdXN0b21Ob2RlKFNsaWNlWygwLCA4KV0sIFtdKSwpKSwgKDgsKSwgKCkpXSwgW10pLCkpXQA=\\22, \\22needs_layout_passes\\22: true}}\", kernel_name = \"add_vectors_kernel\", operand_layouts = [dense<0> : tensor<1xindex>, dense<0> : tensor<1xindex>], result_layouts = [dense<0> : tensor<1xindex>]} : (tensor<8xi32>, tensor<8xi32>) -> tensor<8xi32>\n",
    "      return %0 : tensor<8xi32>\n",
    "    }\n",
    "  }\n",
    "\n",
    "  Basically, what we are looking for is a two level of operations, and the tpu_custom_call operation in the inner level. It will return None if the payload is not found.\n",
    "  \"\"\"\n",
    "  for operation in module.body.operations:\n",
    "    assert len(\n",
    "        operation.body.blocks) == 1, \"The passing module is not compatible.\"\n",
    "    for op in operation.body.blocks[0].operations:\n",
    "      if op.name == \"stablehlo.custom_call\":\n",
    "        return op.backend_config.value\n",
    "  return None\n",
    "\n",
    "\n",
    "def jax_import_guard():\n",
    "  # Somehow, we need to grab the TPU before JAX locks it. Otherwise, any pt-xla TPU operations will hang.\n",
    "  torch_xla._XLAC._init_computation_client()\n",
    "\n",
    "\n",
    "def convert_torch_dtype_to_jax(dtype: torch.dtype) -> \"jnp.dtype\":\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax.numpy as jnp\n",
    "  if _XLA_USE_BF16:\n",
    "    raise RuntimeError(\n",
    "        \"Pallas kernel does not support XLA_USE_BF16, please unset the env var\")\n",
    "  if dtype == torch.float32:\n",
    "    return jnp.float32\n",
    "  elif dtype == torch.float64:\n",
    "    return jnp.float64\n",
    "  elif dtype == torch.float16:\n",
    "    return jnp.float16\n",
    "  elif dtype == torch.bfloat16:\n",
    "    return jnp.bfloat16\n",
    "  elif dtype == torch.int32:\n",
    "    return jnp.int32\n",
    "  elif dtype == torch.int64:\n",
    "    return jnp.int64\n",
    "  elif dtype == torch.int16:\n",
    "    return jnp.int16\n",
    "  elif dtype == torch.int8:\n",
    "    return jnp.int8\n",
    "  elif dtype == torch.uint8:\n",
    "    return jnp.uint8\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def to_jax_shape_dtype_struct(tensor: torch.Tensor) -> \"jax.ShapeDtypeStruct\":\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "\n",
    "  return jax.ShapeDtypeStruct(tensor.shape,\n",
    "                              convert_torch_dtype_to_jax(tensor.dtype))\n",
    "\n",
    "\n",
    "trace_pallas_arg_to_payload: Dict[Tuple[Any], str] = {}\n",
    "\n",
    "\n",
    "def trace_pallas(kernel: Callable,\n",
    "                 *args,\n",
    "                 static_argnums=None,\n",
    "                 static_argnames=None,\n",
    "                 use_cache=False,\n",
    "                 **kwargs):\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "  import jax._src.pallas.mosaic.pallas_call_registration\n",
    "\n",
    "  jax_args = []  # for tracing\n",
    "  tensor_args = []  # for execution\n",
    "  for i, arg in enumerate(args):\n",
    "    # TODO: Could the args be a tuple of tensors or a list of tensors? Flattern them?\n",
    "    if torch.is_tensor(arg):\n",
    "      # ShapeDtypeStruct doesn't have any storage and thus is very suitable for generating the payload.\n",
    "      jax_meta_tensor = to_jax_shape_dtype_struct(arg)\n",
    "      jax_args.append(jax_meta_tensor)\n",
    "      tensor_args.append(arg)\n",
    "    else:\n",
    "      jax_args.append(arg)\n",
    "\n",
    "  hash_key = ()\n",
    "  if use_cache:\n",
    "    global trace_pallas_arg_to_payload\n",
    "    # implcit assumption here that everything in kwargs is hashable and not a tensor,\n",
    "    # which is true for the gmm and tgmm.\n",
    "    hash_key = (jax.config.jax_default_matmul_precision, kernel, static_argnums,\n",
    "                tuple(static_argnames)\n",
    "                if static_argnames is not None else static_argnames,\n",
    "                tuple(jax_args), repr(sorted(kwargs.items())).encode())\n",
    "    if hash_key in trace_pallas_arg_to_payload:\n",
    "      torch_xla._XLAC._xla_increment_counter('trace_pallas_cache_hit', 1)\n",
    "      return trace_pallas_arg_to_payload[hash_key], tensor_args\n",
    "\n",
    "  # Here we ignore the kwargs for execution as most of the time, the kwargs is only used in traced code.\n",
    "  ir = jax.jit(\n",
    "      kernel, static_argnums=static_argnums,\n",
    "      static_argnames=static_argnames).lower(*jax_args, **kwargs).compiler_ir()\n",
    "  payload = _extract_backend_config(ir)\n",
    "\n",
    "  if use_cache:\n",
    "    # if we reach here it means we have a cache miss.\n",
    "    trace_pallas_arg_to_payload[hash_key] = payload\n",
    "\n",
    "  return payload, tensor_args\n",
    "\n",
    "\n",
    "def make_kernel_from_pallas(kernel: Callable, output_shape_dtype_fn: Callable):\n",
    "  # TODO: Maybe we can cache the payload for the same input.\n",
    "  def wrapped_kernel(kernel: Callable,\n",
    "                     output_shape_dtype_fn: Callable,\n",
    "                     *args,\n",
    "                     static_argnums=None,\n",
    "                     static_argnames=None,\n",
    "                     **kwargs) -> Callable:\n",
    "    payload, tensor_args = trace_pallas(\n",
    "        kernel,\n",
    "        *args,\n",
    "        static_argnums=static_argnums,\n",
    "        static_argnames=static_argnames,\n",
    "        **kwargs)\n",
    "    output_shape_dtype = output_shape_dtype_fn(*args)\n",
    "    assert isinstance(output_shape_dtype,\n",
    "                      list), \"The output_shape_dtype_fn should return a list.\"\n",
    "    output_shapes = [shape for shape, _ in output_shape_dtype]\n",
    "    output_dtypes = [dtype for _, dtype in output_shape_dtype]\n",
    "    outputs = torch_xla._XLAC._xla_tpu_custom_call(tensor_args, payload,\n",
    "                                                   output_shapes, output_dtypes)\n",
    "\n",
    "    # Make the output easier to use.\n",
    "    if len(outputs) == 1:\n",
    "      return outputs[0]\n",
    "    return tuple(outputs)\n",
    "\n",
    "  return functools.partial(wrapped_kernel, kernel, output_shape_dtype_fn)\n",
    "\n",
    "\n",
    "def defeat_alias(v):\n",
    "  return v * 1\n",
    "\n",
    "\n",
    "# Note: the alias inference and mutation removal in PyTorch doesn't work. So we\n",
    "#\n",
    "# - Explicitly clone all inputs.\n",
    "# - Clone outputs if the output aliases an input.\n",
    "#\n",
    "@custom_op(\"xla::fa_custom_forward\", mutates_args=())\n",
    "def fa_custom_forward(\n",
    "    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n",
    "           torch.Tensor]:\n",
    "  partition_spec = ('fsdp', 'tensor', None, None)\n",
    "  mesh = xs.get_global_mesh()\n",
    "  assert mesh is not None\n",
    "\n",
    "  if _DEBUG:\n",
    "    print(\"Inside fa_custom_forward\")\n",
    "    for t in [q, k, v]:\n",
    "      describe_value(t)\n",
    "\n",
    "  q_segment_ids = kv_segment_ids = ab = None\n",
    "  sm_scale = 1.0\n",
    "  causal = False \n",
    "\n",
    "  # Import JAX within the function such that we don't need to call the jax_import_guard()\n",
    "  # in the global scope which could cause problems for xmp.spawn.\n",
    "  jax_import_guard()\n",
    "  import jax\n",
    "  from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_impl\n",
    "\n",
    "  q_full_shape = None\n",
    "  kv_full_shape = None\n",
    "  save_residuals = True\n",
    "  \n",
    "  q = defeat_alias(q)\n",
    "  k = defeat_alias(k)\n",
    "  v = defeat_alias(v)\n",
    "\n",
    "  # SPMD integration.\n",
    "  # mark_sharding is in-placed, and therefore save the full q, k, v for the backward.\n",
    "  # PyTorch tell us clone is necessary:\n",
    "  #\n",
    "  # RuntimeError: Found a custom (non-ATen) operator whose output has alias\n",
    "  # annotations: xla::fa_custom_forward(Tensor(a0!) q, Tensor(a1!) k,\n",
    "  # Tensor(a2!) v) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor). We only\n",
    "  # support functionalizing operators whose outputs do not have alias\n",
    "  # annotations (e.g. 'Tensor(a)' is a Tensor with an alias annotation whereas\n",
    "  # 'Tensor' is a Tensor without. The '(a)' is the alias annotation). The alias\n",
    "  # annotation specifies that the output Tensor shares storage with an input\n",
    "  # that has the same annotation. Please check if (1) the output needs to be an\n",
    "  # output (if not, don't return it), (2) if the output doesn't share storage\n",
    "  # with any inputs, then delete the alias annotation. (3) if the output indeed\n",
    "  # shares storage with an input, then add a .clone() before returning it to\n",
    "  # prevent storage sharing and then delete the alias annotation. Otherwise,\n",
    "  # please file an issue on GitHub.\n",
    "  #\n",
    "  with xp.Trace('shard'):\n",
    "    full_q = q.clone()\n",
    "    full_k = k.clone()\n",
    "    full_v = v.clone()\n",
    "    full_ab = ab\n",
    "    if partition_spec is not None:\n",
    "      q_full_shape = q.shape\n",
    "      kv_full_shape = k.shape\n",
    "      q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "      k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "      v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "      if ab:\n",
    "        ab = xs.enable_manual_sharding(\n",
    "            ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "  # It computes the shape and type of o, l, m.\n",
    "  shapes = [q.shape]\n",
    "  dtypes = [q.dtype]\n",
    "  if save_residuals:\n",
    "    res_shape = list(q.shape)\n",
    "    res_shape[-1] = FlashAttention.MIN_BLOCK_SIZE\n",
    "    for _ in range(2):\n",
    "      shapes.append(res_shape)\n",
    "      dtypes.append(torch.float32)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    if partition_spec is not None and q_segment_ids is not None and kv_segment_ids is not None:\n",
    "      # partition_spec is for q,k,v with shape [batch, num_head, seq_len, head_dim], segment id\n",
    "      # is of shape [batch, seq_len], hence we need to tweak it a bit\n",
    "      segment_id_partition_spec = (partition_spec[0], partition_spec[2])\n",
    "      q_segment_ids = xs.enable_manual_sharding(\n",
    "          q_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "      kv_segment_ids = xs.enable_manual_sharding(\n",
    "          kv_segment_ids, segment_id_partition_spec, mesh=mesh).global_tensor\n",
    "    segment_ids, q_segment_ids_fa, kv_segment_ids_fa = FlashAttention.prepare_segment_ids(\n",
    "        q_segment_ids, kv_segment_ids)\n",
    "\n",
    "    with xp.Trace('pallas'):\n",
    "      # We can't directly use flash_attention as we need to override the save_residuals flag which returns\n",
    "      # l and m that is needed for the backward. Then we lose all the shape checks.\n",
    "      # TODO: replicate the shape checks on flash_attention.\n",
    "      # Here we seperate the tracing and execution part just to support SegmentIds.\n",
    "      payload, _ = trace_pallas(\n",
    "          _flash_attention_impl,\n",
    "          q,\n",
    "          k,\n",
    "          v,\n",
    "          ab,\n",
    "          segment_ids,\n",
    "          save_residuals,\n",
    "          causal,\n",
    "          sm_scale,\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_b\"], q.shape[0]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q\"], q.shape[2]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major\"], k.shape[2]),\n",
    "          min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k\"], k.shape[2]),\n",
    "          False,\n",
    "          static_argnums=range(5, 13),\n",
    "          use_cache=True,\n",
    "      )\n",
    "\n",
    "    with xp.Trace('custom_call'):\n",
    "      args = [q, k, v]\n",
    "      if ab is not None:\n",
    "        args += [ab]\n",
    "      if segment_ids is not None:\n",
    "        args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "      o = torch_xla._XLAC._xla_tpu_custom_call(args, payload, shapes, dtypes)\n",
    "\n",
    "    if not save_residuals:\n",
    "      o = o[0]\n",
    "      # SPMD integration\n",
    "      if partition_spec is not None:\n",
    "        o = xs.disable_manual_sharding(\n",
    "            o, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "      return o\n",
    "\n",
    "    assert isinstance(o, list)\n",
    "    o, *aux = o\n",
    "    \n",
    "    print(\"About to index into aux to get l, m\")\n",
    "    import torch_xla.debug.metrics as met\n",
    "    met.clear_all()\n",
    "\n",
    "    if AVOID_AS_STRIDED:\n",
    "      l = aux[-2]\n",
    "      m = aux[-1]\n",
    "\n",
    "      print(\"Done indexing into aux. Metrics:\")\n",
    "      print(met.metrics_report())\n",
    "      \n",
    "      # SPMD integration\n",
    "      with xp.Trace('index_lm'):\n",
    "        if partition_spec is not None:\n",
    "          o = xs.disable_manual_sharding(\n",
    "              o, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "          l = xs.disable_manual_sharding(\n",
    "              l, partition_spec, q_full_shape[0:3] + (l.shape[-1], ),\n",
    "              mesh=mesh).global_tensor\n",
    "          m = xs.disable_manual_sharding(\n",
    "              m, partition_spec, q_full_shape[0:3] + (m.shape[-1], ),\n",
    "              mesh=mesh).global_tensor\n",
    "          \n",
    "          l = l.permute(3, 0, 1, 2)[0]\n",
    "          m = m.permute(3, 0, 1, 2)[0]\n",
    "    else:\n",
    "      l, m = (v[..., 0] for v in aux[-2:])\n",
    "\n",
    "      print(\"Done indexing into aux. Metrics:\")\n",
    "      print(met.metrics_report())\n",
    "\n",
    "      # SPMD integration\n",
    "      with xp.Trace('index_lm'):\n",
    "        if partition_spec is not None:\n",
    "          o = xs.disable_manual_sharding(\n",
    "              o, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "          l = xs.disable_manual_sharding(\n",
    "              l, partition_spec[0:3], q_full_shape[0:3],\n",
    "              mesh=mesh).global_tensor\n",
    "          m = xs.disable_manual_sharding(\n",
    "              m, partition_spec[0:3], q_full_shape[0:3],\n",
    "              mesh=mesh).global_tensor\n",
    "\n",
    "  assert partition_spec is not None\n",
    "\n",
    "  # q_segment_ids and kv_segment_ids are sharded here if partition_spec is provided\n",
    "  # but it should be OK as the backward will use the same partition_spec\n",
    "  outs = [o] + [full_q, full_k, full_v, l, m]\n",
    "  if _DEBUG:\n",
    "    print(\"Outs\")\n",
    "    for t in outs:\n",
    "      describe_value(t)\n",
    "  return tuple(outs)\n",
    "\n",
    "\n",
    "@fa_custom_forward.register_fake\n",
    "def fa_custom_forward_fake(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "  if _DEBUG:\n",
    "    print(\"Inside fake fa_custom_forward\")\n",
    "\n",
    "  assert q.shape == k.shape\n",
    "  assert k.shape == v.shape\n",
    "\n",
    "  # full_q, full_k, full_v, o, l, m\n",
    "  full_q = torch.empty_like(q)\n",
    "  full_k = torch.empty_like(k)\n",
    "  full_v = torch.empty_like(v)\n",
    "  o = torch.empty_like(v)\n",
    "  l = torch.empty_like(v, dtype=torch.float32)[..., 0]\n",
    "  m = torch.empty_like(v, dtype=torch.float32)[..., 0]\n",
    "\n",
    "  return tuple([torch.empty_like(o)] +\n",
    "               [torch.empty_like(t) for t in (\n",
    "                   full_q,\n",
    "                   full_k,\n",
    "                   full_v,\n",
    "                   l,\n",
    "                   m,\n",
    "               )])\n",
    "\n",
    "\n",
    "@custom_op(\"xla::fa_custom_backward\", mutates_args=())\n",
    "def fa_custom_backward(\n",
    "    grad_output: torch.Tensor, q: torch.Tensor, k: torch.Tensor,\n",
    "    v: torch.Tensor, o: torch.Tensor, l: torch.Tensor, m: torch.Tensor,\n",
    "    q_shape: List[int],\n",
    "    k_shape: List[int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "  q_segment_ids_fa = kv_segment_ids_fa = ab = None\n",
    "\n",
    "  partition_spec = ('fsdp', 'tensor', None, None)\n",
    "  mesh = xs.get_global_mesh()\n",
    "  assert mesh is not None\n",
    "\n",
    "  if _DEBUG:\n",
    "    print(\"Inside fa_custom_backward\")\n",
    "\n",
    "  from jax.experimental.pallas.ops.tpu.flash_attention import _flash_attention_bwd_dq, _flash_attention_bwd_dkv\n",
    "\n",
    "  grad_output = defeat_alias(grad_output)\n",
    "  saved_tensors = (q, k, v, o, l, m)\n",
    "  q, k, v, o, l, m = (defeat_alias(t) for t in saved_tensors)\n",
    "\n",
    "  causal = False\n",
    "  sm_scale = 1.0\n",
    "  q_full_shape = torch.Size(q_shape)\n",
    "  kv_full_shape = torch.Size(k_shape)\n",
    "  # this segment_ids only reflects the local shape of segment_ids\n",
    "  segment_ids = None\n",
    "  grad_q = grad_k = grad_v = grad_ab = None\n",
    "  needs_input_grad = [True, True, True]\n",
    "  grad_i = torch.sum(\n",
    "      o.to(torch.float32) * grad_output.to(torch.float32),\n",
    "      axis=-1)  # [batch_size, num_heads, q_seq_len]\n",
    "\n",
    "  expanded_l = l.unsqueeze(-1).expand([-1 for _ in l.shape] +\n",
    "                                      [FlashAttention.MIN_BLOCK_SIZE])\n",
    "  expanded_m = m.unsqueeze(-1).expand([-1 for _ in m.shape] +\n",
    "                                      [FlashAttention.MIN_BLOCK_SIZE])\n",
    "  expanded_grad_i = grad_i.unsqueeze(-1).expand([-1 for _ in grad_i.shape] +\n",
    "                                                [FlashAttention.MIN_BLOCK_SIZE])\n",
    "\n",
    "  # SPMD integration\n",
    "  if partition_spec is not None:\n",
    "    q = xs.enable_manual_sharding(q, partition_spec, mesh=mesh).global_tensor\n",
    "    k = xs.enable_manual_sharding(k, partition_spec, mesh=mesh).global_tensor\n",
    "    v = xs.enable_manual_sharding(v, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_l = xs.enable_manual_sharding(\n",
    "        expanded_l, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_m = xs.enable_manual_sharding(\n",
    "        expanded_m, partition_spec, mesh=mesh).global_tensor\n",
    "    grad_output = xs.enable_manual_sharding(\n",
    "        grad_output, partition_spec, mesh=mesh).global_tensor\n",
    "    expanded_grad_i = xs.enable_manual_sharding(\n",
    "        expanded_grad_i, partition_spec, mesh=mesh).global_tensor\n",
    "    if ab:\n",
    "      ab = xs.enable_manual_sharding(\n",
    "          ab, partition_spec, mesh=mesh).global_tensor\n",
    "\n",
    "  if needs_input_grad[0]:\n",
    "    payload, _ = trace_pallas(\n",
    "        _flash_attention_bwd_dq,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        ab,\n",
    "        segment_ids,\n",
    "        l,\n",
    "        m,\n",
    "        grad_output,\n",
    "        grad_i,\n",
    "        block_q_major=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dq\"],\n",
    "                          q.shape[2]),\n",
    "        block_k_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dq\"], k.shape[2]),\n",
    "        block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dq\"],\n",
    "                    k.shape[2]),\n",
    "        sm_scale=sm_scale,\n",
    "        causal=causal,\n",
    "        mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "        debug=False,\n",
    "        static_argnames=[\n",
    "            \"block_q_major\", \"block_k_major\", \"block_k\", \"sm_scale\", \"causal\",\n",
    "            \"mask_value\", \"debug\"\n",
    "        ],\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    args = [q, k, v]\n",
    "    if ab is not None:\n",
    "      args += [ab]\n",
    "    if segment_ids is not None:\n",
    "      args += [q_segment_ids_fa, kv_segment_ids_fa]\n",
    "    args += [expanded_l, expanded_m, grad_output, expanded_grad_i]\n",
    "\n",
    "    outputs = [q]\n",
    "    if ab is not None:\n",
    "      outputs += [ab]\n",
    "    grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                 [i.shape for i in outputs],\n",
    "                                                 [i.dtype for i in outputs])\n",
    "    if needs_input_grad[0]:\n",
    "      grad_q = grads[0]\n",
    "\n",
    "  if needs_input_grad[1] or needs_input_grad[2]:\n",
    "    payload, _ = trace_pallas(\n",
    "        _flash_attention_bwd_dkv,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        ab,\n",
    "        segment_ids,\n",
    "        l,\n",
    "        m,\n",
    "        grad_output,\n",
    "        grad_i,\n",
    "        block_q_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_major_dkv\"],\n",
    "            q.shape[2]),\n",
    "        block_k_major=min(\n",
    "            FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_major_dkv\"],\n",
    "            k.shape[2]),\n",
    "        block_k=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_k_dkv\"],\n",
    "                    k.shape[2]),\n",
    "        block_q=min(FlashAttention.DEFAULT_BLOCK_SIZES[\"block_q_dkv\"],\n",
    "                    q.shape[2]),\n",
    "        sm_scale=sm_scale,\n",
    "        causal=causal,\n",
    "        mask_value=FlashAttention.DEFAULT_MASK_VALUE,\n",
    "        debug=False,\n",
    "        static_argnames=[\n",
    "            \"block_q_major\", \"block_k_major\", \"block_k\", \"block_q\", \"sm_scale\",\n",
    "            \"causal\", \"mask_value\", \"debug\"\n",
    "        ],\n",
    "        use_cache=True)\n",
    "\n",
    "    grads = torch_xla._XLAC._xla_tpu_custom_call(args, payload,\n",
    "                                                 [k.shape, v.shape],\n",
    "                                                 [k.dtype, v.dtype])\n",
    "\n",
    "  if needs_input_grad[1]:\n",
    "    grad_k = grads[0]\n",
    "  if needs_input_grad[2]:\n",
    "    grad_v = grads[1]\n",
    "\n",
    "  # SPMD integration\n",
    "  if partition_spec is not None:\n",
    "    grad_q = xs.disable_manual_sharding(\n",
    "        grad_q, partition_spec, q_full_shape, mesh=mesh).global_tensor\n",
    "    grad_k = xs.disable_manual_sharding(\n",
    "        grad_k, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "    grad_v = xs.disable_manual_sharding(\n",
    "        grad_v, partition_spec, kv_full_shape, mesh=mesh).global_tensor\n",
    "\n",
    "  assert partition_spec is not None\n",
    "\n",
    "  return grad_q, grad_k, grad_v\n",
    "\n",
    "\n",
    "@fa_custom_backward.register_fake\n",
    "def fa_custom_backward_fake(grad_output, q, k, v, o, l, m, q_shape, k_shape):\n",
    "  if _DEBUG:\n",
    "    print(\"Inside fake fa_custom_backward\")\n",
    "  return torch.empty_like(grad_output), torch.empty_like(\n",
    "      grad_output), torch.empty_like(grad_output)\n",
    "\n",
    "\n",
    "class FlashAttention2(torch.autograd.Function):\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, q, k, v):\n",
    "    with torch.no_grad():\n",
    "      ctx.q_shape = q.shape\n",
    "      ctx.k_shape = k.shape\n",
    "\n",
    "      outs = torch.ops.xla.fa_custom_forward(q, k, v)\n",
    "      if _DEBUG:\n",
    "        print(\"forward done with fa_custom_forward\")\n",
    "\n",
    "      o = outs[0]\n",
    "      full_q, full_k, full_v, l, m = [x for x in outs[1:]]\n",
    "\n",
    "      # q_segment_ids and kv_segment_ids are sharded here if partition_spec is provided\n",
    "      # but it should be OK as the backward will use the same partition_spec\n",
    "      ctx.save_for_backward(full_q, full_k, full_v, o, l, m)\n",
    "      return o\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    with torch.no_grad():\n",
    "      grad_ab = None\n",
    "      if _DEBUG:\n",
    "        print(\"Inside backward\")\n",
    "\n",
    "      saved = [v for v in ctx.saved_tensors]\n",
    "      if _DEBUG:\n",
    "        for t in [grad_output] + saved:\n",
    "          describe_value(t)\n",
    "\n",
    "      return torch.ops.xla.fa_custom_backward(grad_output, *saved,\n",
    "                                              list(ctx.q_shape),\n",
    "                                              list(ctx.k_shape))\n",
    "\n",
    "\n",
    "def flash_attention_2(\n",
    "    q,  # [batch_size, num_heads, q_seq_len, d_model]\n",
    "    k,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    v,  # [batch_size, num_heads, kv_seq_len, d_model]\n",
    "    causal=False,\n",
    "    q_segment_ids=None,  # [batch_size, q_seq_len]\n",
    "    kv_segment_ids=None,\n",
    "    sm_scale=1.0,\n",
    "    *,\n",
    "    ab=None,  # [batch_size, num_heads, q_seq_len, kv_seq_len]\n",
    "    partition_spec=None,\n",
    "    mesh=None,\n",
    "):\n",
    "  assert not causal, \"causal must be False\"\n",
    "  assert partition_spec == ('fsdp', 'tensor', None, None)\n",
    "  return FlashAttention2.apply(q, k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a test with the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_import_guard()\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _attention(q, k, v, *, attn_mask=None, ab=None):\n",
    "  attn_weight = q @ k.transpose(-2, -1)\n",
    "  if attn_mask is not None:\n",
    "    # Masked out the unrelevant parts.\n",
    "    attn_weight = attn_weight.masked_fill(attn_mask,\n",
    "                                          torch.finfo(attn_weight.dtype).min)\n",
    "  if ab is not None:\n",
    "    attn_weight = attn_weight + ab\n",
    "  attn_weight = nn.functional.softmax(attn_weight, dim=-1)\n",
    "  attn_output = attn_weight @ v\n",
    "  return attn_output\n",
    "\n",
    "\n",
    "def do_test(attn_fn, aot_autograd: bool):\n",
    "  from functorch.compile import aot_function, make_boxed_func  # type: ignore\n",
    "\n",
    "  def flash_attention_wrapper(q, k, v):\n",
    "    return attn_fn(q, k, v, partition_spec=('fsdp', 'tensor', None, None))\n",
    "\n",
    "  q = torch.randn(16, 2, 128, 8).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "  k = torch.randn(16, 2, 128, 8).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "  v = torch.randn(16, 2, 128, 8).to(\"xla\").clone().detach().requires_grad_(True)\n",
    "  \n",
    "  xs.mark_sharding(q, xs.get_global_mesh(), ('fsdp', 'tensor', None, None))\n",
    "  xs.mark_sharding(k, xs.get_global_mesh(), ('fsdp', 'tensor', None, None))\n",
    "  xs.mark_sharding(v, xs.get_global_mesh(), ('fsdp', 'tensor', None, None))\n",
    "\n",
    "  q_clone = q.clone().detach().requires_grad_(True)\n",
    "  k_clone = k.clone().detach().requires_grad_(True)\n",
    "  v_clone = v.clone().detach().requires_grad_(True)\n",
    "  \n",
    "  def compiler(gm, _):\n",
    "    print(\"Got graph:\")\n",
    "    print(gm.code)\n",
    "    return make_boxed_func(gm)\n",
    "\n",
    "  if aot_autograd:\n",
    "    compiled_flash_attention = aot_function(\n",
    "        flash_attention_wrapper, fw_compiler=compiler)\n",
    "    o_actual = compiled_flash_attention(q, k, v)\n",
    "  else:\n",
    "    o_actual = flash_attention_wrapper(q, k, v)\n",
    "  o_actual.sum().backward()\n",
    "\n",
    "  expected_o = _attention(q_clone, k_clone, v_clone)\n",
    "  expected_o.sum().backward()\n",
    "  \n",
    "  print(\"Executing graph\", flush=True)\n",
    "  import time\n",
    "  time.sleep(0.1)\n",
    "  torch_xla.sync(wait=True)\n",
    "\n",
    "  torch.testing.assert_close(o_actual.cpu(), expected_o.cpu())\n",
    "  assert q.grad is not None and q_clone.grad is not None, f\"{q.grad}, {q_clone.grad}\"\n",
    "  torch.testing.assert_close(q.grad.cpu(), q_clone.grad.cpu())\n",
    "  assert k.grad is not None and k_clone.grad is not None, f\"{k.grad}, {k_clone.grad}\"\n",
    "  torch.testing.assert_close(k.grad.cpu(), k_clone.grad.cpu())\n",
    "  assert v.grad is not None and v_clone.grad is not None, f\"{v.grad}, {v_clone.grad}\"\n",
    "  torch.testing.assert_close(v.grad.cpu(), v_clone.grad.cpu())\n",
    "\n",
    "\n",
    "def test_flash_attention_wrapper(attn_fn):\n",
    "  jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "  try:\n",
    "    do_test(attn_fn, aot_autograd=False)\n",
    "  finally:\n",
    "    jax.config.update(\"jax_default_matmul_precision\", \"default\")\n",
    "\n",
    "\n",
    "def test_flash_attention_wrapper_with_aot_autograd(attn_fn):\n",
    "  jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "  try:\n",
    "    do_test(attn_fn, aot_autograd=True)\n",
    "  finally:\n",
    "    jax.config.update(\"jax_default_matmul_precision\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3):\n",
      "    fa_custom_forward = torch.ops.xla.fa_custom_forward.default(primals_1, primals_2, primals_3);  primals_1 = primals_2 = primals_3 = None\n",
      "    getitem = fa_custom_forward[0]\n",
      "    getitem_1 = fa_custom_forward[1]\n",
      "    getitem_2 = fa_custom_forward[2]\n",
      "    getitem_3 = fa_custom_forward[3]\n",
      "    getitem_4 = fa_custom_forward[4]\n",
      "    getitem_5 = fa_custom_forward[5];  fa_custom_forward = None\n",
      "    return (getitem, getitem, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5)\n",
      "    \n",
      "About to index into aux to get l, m\n",
      "Done indexing into aux. Metrics:\n",
      "Metric: IrValueTensorToXlaData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 279.130us\n",
      "  ValueRate: 01s194ms036.874us / second\n",
      "  Rate: 8555.42 / second\n",
      "  Percentiles: 1%=113.920us; 5%=113.920us; 10%=113.920us; 20%=113.920us; 50%=165.210us; 80%=165.210us; 90%=165.210us; 95%=165.210us; 99%=165.210us\n",
      "Metric: LazyTracing\n",
      "  TotalSamples: 10\n",
      "  Accumulator: 001ms310.940us\n",
      "  ValueRate: 03s756ms160.121us / second\n",
      "  Rate: 21024.3 / second\n",
      "  Percentiles: 1%=006.910us; 5%=006.910us; 10%=014.730us; 20%=017.630us; 50%=104.840us; 80%=207.330us; 90%=500.820us; 95%=500.820us; 99%=500.820us\n",
      "Metric: TensorToData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 276.210us\n",
      "  ValueRate: 01s181ms444.886us / second\n",
      "  Rate: 8554.69 / second\n",
      "  Percentiles: 1%=112.870us; 5%=112.870us; 10%=112.870us; 20%=112.870us; 50%=163.340us; 80%=163.340us; 90%=163.340us; 95%=163.340us; 99%=163.340us\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 4\n",
      "Counter: DestroyLtcTensor\n",
      "  Value: 2\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 2\n",
      "Counter: xla::_copy_from\n",
      "  Value: 2\n",
      "Counter: xla::_to_copy\n",
      "  Value: 2\n",
      "Counter: xla::as_strided_copy\n",
      "  Value: 2\n",
      "Counter: xla::empty_symint\n",
      "  Value: 2\n",
      "Counter: xla::take\n",
      "  Value: 2\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 64.00KB\n",
      "  ValueRate: 265.54MB / second\n",
      "  Rate: 8497.26 / second\n",
      "  Percentiles: 1%=32.00KB; 5%=32.00KB; 10%=32.00KB; 20%=32.00KB; 50%=32.00KB; 80%=32.00KB; 90%=32.00KB; 95%=32.00KB; 99%=32.00KB\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 161.710us\n",
      "  ValueRate: 689ms596.491us / second\n",
      "  Rate: 8516.44 / second\n",
      "  Percentiles: 1%=054.450us; 5%=054.450us; 10%=054.450us; 20%=054.450us; 50%=107.260us; 80%=107.260us; 90%=107.260us; 95%=107.260us; 99%=107.260us\n",
      "Counter: CreateDataHandles\n",
      "  Value: 16\n",
      "\n",
      "Got graph:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, getitem, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5, tangents_1):\n",
      "    detach = torch.ops.aten.detach.default(getitem);  getitem = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    fa_custom_backward = torch.ops.xla.fa_custom_backward.default(tangents_1, getitem_1, getitem_2, getitem_3, detach_2, getitem_4, getitem_5, [16, 2, 128, 8], [16, 2, 128, 8]);  tangents_1 = getitem_1 = getitem_2 = getitem_3 = detach_2 = getitem_4 = getitem_5 = None\n",
      "    getitem_6 = fa_custom_backward[0]\n",
      "    getitem_7 = fa_custom_backward[1]\n",
      "    getitem_8 = fa_custom_backward[2];  fa_custom_backward = None\n",
      "    return (getitem_6, getitem_7, getitem_8)\n",
      "    \n",
      "Executing graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compilation Analysis: ================================================================================\n",
      "Compilation Analysis: Compilation Cause\n",
      "Compilation Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Compilation Analysis: Graph Info: \n",
      "Compilation Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Compilation Analysis:   Number of Graph Inputs: 1\n",
      "Compilation Analysis:   Number of Graph Outputs: 1\n",
      "Compilation Analysis: Python Frame Triggered Execution: \n",
      "Compilation Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Compilation Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:31)\n",
      "Compilation Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Compilation Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Compilation Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Compilation Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Compilation Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Compilation Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Compilation Analysis:   ..........\n",
      "Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Compilation Analysis: ================================================================================\n",
      "\n",
      "Post Compilation Analysis: ================================================================================\n",
      "Post Compilation Analysis: Graph input size: 0.000122 GB\n",
      "Post Compilation Analysis: Graph output size: 0.000122 GB\n",
      "Post Compilation Analysis: Aliased Input size: 0.000000 GB\n",
      "Post Compilation Analysis: Intermediate tensor size: 0.000000 GB\n",
      "Post Compilation Analysis: Compiled program size: 0.000031 GB\n",
      "Post Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Post Compilation Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:31)\n",
      "Execution Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:32)\n",
      "Execution Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:33)\n",
      "Execution Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Compilation Analysis: ================================================================================\n",
      "Compilation Analysis: Compilation Cause\n",
      "Compilation Analysis:   user mark_step\n",
      "Compilation Analysis: Graph Info: \n",
      "Compilation Analysis:   Graph Hash: c8bd24662afbf36b758da103566f8803\n",
      "Compilation Analysis:   Number of Graph Inputs: 5\n",
      "Compilation Analysis:   Number of Graph Outputs: 15\n",
      "Compilation Analysis: Python Frame Triggered Execution: \n",
      "Compilation Analysis:   sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "Compilation Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:58)\n",
      "Compilation Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Compilation Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Compilation Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Compilation Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Compilation Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Compilation Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Compilation Analysis:   ..........\n",
      "Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Compilation Analysis: ================================================================================\n",
      "\n",
      "Post Compilation Analysis: ================================================================================\n",
      "Post Compilation Analysis: Graph input size: 0.000053 GB\n",
      "Post Compilation Analysis: Graph output size: 0.000351 GB\n",
      "Post Compilation Analysis: Aliased Input size: 0.000000 GB\n",
      "Post Compilation Analysis: Intermediate tensor size: 0.000000 GB\n",
      "Post Compilation Analysis: Compiled program size: 0.001012 GB\n",
      "Post Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Post Compilation Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   user mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: c8bd24662afbf36b758da103566f8803\n",
      "Execution Analysis:   Number of Graph Inputs: 5\n",
      "Execution Analysis:   Number of Graph Outputs: 15\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:58)\n",
      "Execution Analysis:   test_flash_attention_wrapper_with_aot_autograd (/tmp/ipykernel_2190827/803061339.py:80)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/1076877315.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_flash_attention_wrapper_with_aot_autograd(flash_attention_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that works great, although it transfers two 32 KiB indexing tensors.\n",
    "What if the flash attention is wrapped in a scan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the dumps before running the offending graph\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_dumps():\n",
    "  for f in os.listdir('xla_dumps/aot-sharded-flash-attention'):\n",
    "    p = Path(f\"xla_dumps/aot-sharded-flash-attention/{f}\")\n",
    "    if p.is_file():\n",
    "      os.remove(p)\n",
    "\n",
    "clear_dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This combine_fn indexes into q, k, v, and outputs o.\n",
    "def combine_fn(carry, x):\n",
    "  q, k, v = [defeat_alias(v) for v in x]\n",
    "  y = flash_attention_2(q, k, v, causal=False, partition_spec=('fsdp', 'tensor', None, None))\n",
    "  assert isinstance(y, torch.Tensor)\n",
    "  return defeat_alias(carry), defeat_alias(y)\n",
    "\n",
    "def flash_attention_in_scan(q, k, v, partition_spec=None):\n",
    "  # Create a leading dim of length 2.\n",
    "  q = q.reshape(2, q.shape[0] // 2, q.shape[1], q.shape[2], q.shape[3])\n",
    "  k = k.reshape(2, k.shape[0] // 2, k.shape[1], k.shape[2], k.shape[3])\n",
    "  v = v.reshape(2, v.shape[0] // 2, v.shape[1], v.shape[2], v.shape[3])\n",
    "  from torch_xla.experimental.scan import scan\n",
    "  init = torch.zeros_like(q, requires_grad=True)\n",
    "  carry, ys = scan(combine_fn, init, (q, k, v))\n",
    "  assert isinstance(ys, torch.Tensor)\n",
    "  return ys.reshape(ys.shape[0] * ys.shape[1], ys.shape[2], ys.shape[3], ys.shape[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to index into aux to get l, m\n",
      "Done indexing into aux. Metrics:\n",
      "Metric: IrValueTensorToXlaData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 348.930us\n",
      "  ValueRate: 01s250ms704.523us / second\n",
      "  Rate: 7163.07 / second\n",
      "  Percentiles: 1%=170.150us; 5%=170.150us; 10%=170.150us; 20%=170.150us; 50%=178.780us; 80%=178.780us; 90%=178.780us; 95%=178.780us; 99%=178.780us\n",
      "Metric: LazyTracing\n",
      "  TotalSamples: 10\n",
      "  Accumulator: 001ms179.870us\n",
      "  ValueRate: 02s459ms882.127us / second\n",
      "  Rate: 20840.3 / second\n",
      "  Percentiles: 1%=006.410us; 5%=006.410us; 10%=014.950us; 20%=019.400us; 50%=079.030us; 80%=244.810us; 90%=351.350us; 95%=351.350us; 99%=351.350us\n",
      "Metric: TensorToData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 346.200us\n",
      "  ValueRate: 01s239ms216.809us / second\n",
      "  Rate: 7158.96 / second\n",
      "  Percentiles: 1%=168.460us; 5%=168.460us; 10%=168.460us; 20%=168.460us; 50%=177.740us; 80%=177.740us; 90%=177.740us; 95%=177.740us; 99%=177.740us\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 4\n",
      "Counter: DestroyLtcTensor\n",
      "  Value: 2\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 2\n",
      "Counter: xla::_copy_from\n",
      "  Value: 2\n",
      "Counter: xla::_to_copy\n",
      "  Value: 2\n",
      "Counter: xla::as_strided_copy\n",
      "  Value: 2\n",
      "Counter: xla::empty_symint\n",
      "  Value: 2\n",
      "Counter: xla::take\n",
      "  Value: 2\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 32.00KB\n",
      "  ValueRate: 130.33MB / second\n",
      "  Rate: 8340.98 / second\n",
      "  Percentiles: 1%=16.00KB; 5%=16.00KB; 10%=16.00KB; 20%=16.00KB; 50%=16.00KB; 80%=16.00KB; 90%=16.00KB; 95%=16.00KB; 99%=16.00KB\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 173.940us\n",
      "  ValueRate: 727ms113.118us / second\n",
      "  Rate: 8360.5 / second\n",
      "  Percentiles: 1%=058.630us; 5%=058.630us; 10%=058.630us; 20%=058.630us; 50%=115.310us; 80%=115.310us; 90%=115.310us; 95%=115.310us; 99%=115.310us\n",
      "Counter: CreateDataHandles\n",
      "  Value: 16\n",
      "\n",
      "Backward graph: \n",
      "\n",
      "\n",
      "def forward(self, getitem, getitem_1, getitem_2, getitem_3, getitem_4, getitem_5, tangents_1, tangents_2):\n",
      "    detach = torch.ops.aten.detach.default(getitem);  getitem = None\n",
      "    mul_5 = torch.ops.aten.mul.Tensor(tangents_2, 1);  tangents_2 = None\n",
      "    mul_6 = torch.ops.aten.mul.Tensor(tangents_1, 1);  tangents_1 = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    fa_custom_backward = torch.ops.xla.fa_custom_backward.default(mul_5, getitem_1, getitem_2, getitem_3, detach_2, getitem_4, getitem_5, [8, 2, 128, 8], [8, 2, 128, 8]);  mul_5 = getitem_1 = getitem_2 = getitem_3 = detach_2 = getitem_4 = getitem_5 = None\n",
      "    getitem_6 = fa_custom_backward[0]\n",
      "    getitem_7 = fa_custom_backward[1]\n",
      "    getitem_8 = fa_custom_backward[2];  fa_custom_backward = None\n",
      "    mul_7 = torch.ops.aten.mul.Tensor(getitem_8, 1);  getitem_8 = None\n",
      "    mul_8 = torch.ops.aten.mul.Tensor(getitem_7, 1);  getitem_7 = None\n",
      "    mul_9 = torch.ops.aten.mul.Tensor(getitem_6, 1);  getitem_6 = None\n",
      "    return (mul_6, mul_9, mul_8, mul_7)\n",
      "    \n",
      "About to index into aux to get l, m\n",
      "Done indexing into aux. Metrics:\n",
      "Metric: IrValueTensorToXlaData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 293.410us\n",
      "  ValueRate: 01s266ms625.674us / second\n",
      "  Rate: 8627.01 / second\n",
      "  Percentiles: 1%=145.930us; 5%=145.930us; 10%=145.930us; 20%=145.930us; 50%=147.480us; 80%=147.480us; 90%=147.480us; 95%=147.480us; 99%=147.480us\n",
      "Metric: LazyTracing\n",
      "  TotalSamples: 10\n",
      "  Accumulator: 821.020us\n",
      "  ValueRate: 02s047ms512.787us / second\n",
      "  Rate: 24926.5 / second\n",
      "  Percentiles: 1%=005.180us; 5%=005.180us; 10%=006.880us; 20%=007.330us; 50%=018.070us; 80%=202.890us; 90%=239.700us; 95%=239.700us; 99%=239.700us\n",
      "Metric: TensorToData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 291.150us\n",
      "  ValueRate: 01s257ms636.022us / second\n",
      "  Rate: 8632.22 / second\n",
      "  Percentiles: 1%=144.810us; 5%=144.810us; 10%=144.810us; 20%=144.810us; 50%=146.340us; 80%=146.340us; 90%=146.340us; 95%=146.340us; 99%=146.340us\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 4\n",
      "Counter: DestroyLtcTensor\n",
      "  Value: 2\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 2\n",
      "Counter: xla::_copy_from\n",
      "  Value: 2\n",
      "Counter: xla::_to_copy\n",
      "  Value: 2\n",
      "Counter: xla::as_strided_copy\n",
      "  Value: 2\n",
      "Counter: xla::empty_symint\n",
      "  Value: 2\n",
      "Counter: xla::take\n",
      "  Value: 2\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 32.00KB\n",
      "  ValueRate: 128.79MB / second\n",
      "  Rate: 8242.32 / second\n",
      "  Percentiles: 1%=16.00KB; 5%=16.00KB; 10%=16.00KB; 20%=16.00KB; 50%=16.00KB; 80%=16.00KB; 90%=16.00KB; 95%=16.00KB; 99%=16.00KB\n",
      "Metric: TransferToDeviceTime\n",
      "  TotalSamples: 2\n",
      "  Accumulator: 170.520us\n",
      "  ValueRate: 703ms885.408us / second\n",
      "  Rate: 8244.02 / second\n",
      "  Percentiles: 1%=079.150us; 5%=079.150us; 10%=079.150us; 20%=079.150us; 50%=091.370us; 80%=091.370us; 90%=091.370us; 95%=091.370us; 99%=091.370us\n",
      "Counter: CreateDataHandles\n",
      "  Value: 16\n",
      "\n",
      "Executing graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2140329/803061339.py:31)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2140329/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2140329/1813840263.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2140329/803061339.py:32)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2140329/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2140329/1813840263.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2140329/803061339.py:33)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2140329/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2140329/1813840263.py:1)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Compilation Analysis: ================================================================================\n",
      "Compilation Analysis: Compilation Cause\n",
      "Compilation Analysis:   user mark_step\n",
      "Compilation Analysis: Graph Info: \n",
      "Compilation Analysis:   Graph Hash: f5eebbf5977d37625a040b12d3222819\n",
      "Compilation Analysis:   Number of Graph Inputs: 5\n",
      "Compilation Analysis:   Number of Graph Outputs: 21\n",
      "Compilation Analysis: Python Frame Triggered Execution: \n",
      "Compilation Analysis:   sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "Compilation Analysis:   do_test (/tmp/ipykernel_2140329/803061339.py:58)\n",
      "Compilation Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2140329/803061339.py:72)\n",
      "Compilation Analysis:   <module> (/tmp/ipykernel_2140329/1813840263.py:1)\n",
      "Compilation Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Compilation Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Compilation Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Compilation Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Compilation Analysis:   ..........\n",
      "Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Compilation Analysis: ================================================================================\n",
      "F0103 04:55:21.295447 2141042 hlo_sharding.cc:1024] Check failed: !IsManual() \n",
      "*** Check failure stack trace: ***\n",
      "    @     0x76b43edb5844  absl::log_internal::LogMessage::SendToLog()\n",
      "    @     0x76b43edb5378  absl::log_internal::LogMessage::Flush()\n",
      "    @     0x76b43f00cde9  absl::log_internal::LogMessageFatal::~LogMessageFatal()\n",
      "    @     0x76b43de34792  xla::HloSharding::NumTiles()\n",
      "    @     0x76b43be80ae1  xla::spmd::PartitionedHlo::Reshard()\n",
      "    @     0x76b43beb2d7a  xla::spmd::SpmdPartitioningVisitor::HandleTuple()\n",
      "    @     0x76b43dc4b85f  xla::PostOrderDFS<>()\n",
      "    @     0x76b43dc495d6  xla::HloInstruction::Accept<>()\n",
      "    @     0x76b43333d1aa  xla::HloComputation::Accept<>()\n",
      "    @     0x76b437aa0a70  xla::jellyfish::TpuSpmdPartitioningVisitor::DoPartition()\n",
      "    @     0x76b43beb54e1  xla::spmd::SpmdPartitioner::Run()\n",
      "    @     0x76b437aa26ed  xla::jellyfish::TpuSpmdPartitioner::Run()\n",
      "    @     0x76b43bee7b53  xla::HloPassPipeline::RunPassesInternal<>()\n",
      "    @     0x76b43bee74f1  xla::HloPassPipeline::Run()\n",
      "    @     0x76b43bee7b53  xla::HloPassPipeline::RunPassesInternal<>()\n",
      "    @     0x76b43bee74f1  xla::HloPassPipeline::Run()\n",
      "    @     0x76b436d62569  xla::jellyfish::(anonymous namespace)::Phase2PreLayoutAssignment()\n",
      "    @     0x76b436d57d1b  xla::jellyfish::(anonymous namespace)::HloOptimize()::$_0::operator()()\n",
      "    @     0x76b439517713  absl::internal_any_invocable::LocalInvoker<>()\n",
      "    @     0x76b43e972eb3  thread::Fiber::Body()\n",
      "    @     0x76b43e979256  thread::(anonymous namespace)::FutexDomainThread::WorkLoop()\n",
      "    @     0x76b43e981d25  thread::CommonFiberDomainThread::Run()\n",
      "    @     0x76b43ec30403  Thread::ThreadBody()\n",
      "    @     0x76bb99477ea7  start_thread\n",
      "https://symbolize.corp.google.com/r/?trace=76b43edb5844,76b43edb5377,76b43f00cde8,76b43de34791,76b43be80ae0,76b43beb2d79,76b43dc4b85e,76b43dc495d5,76b43333d1a9,76b437aa0a6f,76b43beb54e0,76b437aa26ec,76b43bee7b52,76b43bee74f0,76b43bee7b52,76b43bee74f0,76b436d62568,76b436d57d1a,76b439517712,76b43e972eb2,76b43e979255,76b43e981d24,76b43ec30402,76bb99477ea6&map= \n",
      "https://symbolize.corp.google.com/r/?trace=76bb994cad51,76bb994cadcf,76b43edb58a8,76b43edb5377,76b43f00cde8,76b43de34791,76b43be80ae0,76b43beb2d79,76b43dc4b85e,76b43dc495d5,76b43333d1a9,76b437aa0a6f,76b43beb54e0,76b437aa26ec,76b43bee7b52,76b43bee74f0,76b43bee7b52,76b43bee74f0,76b436d62568,76b436d57d1a,76b439517712,76b43e972eb2,76b43e979255,76b43e981d24,76b43ec30402,76bb99477ea6&map= \n",
      "*** SIGABRT received by PID 2140329 (TID 2141042) on cpu 136 from PID 2140329; ***\n",
      "E0103 04:55:21.391258 2141042 coredump_hook.cc:301] RAW: Remote crash data gathering hook invoked.\n",
      "E0103 04:55:21.391270 2141042 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\n",
      "E0103 04:55:21.391273 2141042 coredump_hook.cc:396] RAW: Sending fingerprint to remote end.\n",
      "E0103 04:55:21.391289 2141042 coredump_hook.cc:405] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\n",
      "E0103 04:55:21.391292 2141042 coredump_hook.cc:457] RAW: Dumping core locally.\n"
     ]
    }
   ],
   "source": [
    "test_flash_attention_wrapper(flash_attention_in_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we encounter a problem: for some reason we get an `!IsManual()` assertion\n",
    "in the GSPMD partitioner.\n",
    "\n",
    "The relevant IR and HLO are rendered below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorsGraphInfo:\n",
      "  sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "  do_test (/tmp/ipykernel_2140329/803061339.py:58)\n",
      "  test_flash_attention_wrapper (/tmp/ipykernel_2140329/803061339.py:72)\n",
      "  <module> (/tmp/ipykernel_2140329/1813840263.py:1)\n",
      "  run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "  run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "  run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "  _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "  _run_cell (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3130)\n",
      "  run_cell (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3075)\n",
      "  run_cell (/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py:549)\n",
      "  do_execute (/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py:449)\n",
      "  execute_request (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:778)\n",
      "  execute_request (/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py:362)\n",
      "  dispatch_shell (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:437)\n",
      "  process_one (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:534)\n",
      "  dispatch_queue (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:545)\n",
      "  _run (/usr/local/lib/python3.10/asyncio/events.py:80)\n",
      "  _run_once (/usr/local/lib/python3.10/asyncio/base_events.py:1909)\n",
      "  run_forever (/usr/local/lib/python3.10/asyncio/base_events.py:603)\n",
      "  start (/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py:205)\n",
      "  start (/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py:739)\n",
      "  launch_instance (/root/.local/lib/python3.10/site-packages/traitlets/config/application.py:1075)\n",
      "  <module> (/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py:18)\n",
      "  _run_code (/usr/local/lib/python3.10/runpy.py:86)\n",
      "  _run_module_as_main (/usr/local/lib/python3.10/runpy.py:196)\n",
      "\n",
      "Root Hashes: (f68b5996bbe97b2548777ac59d3b1368, f68b5996bbe97b2548777ac59d3b1368, f68b5996bbe97b2548777ac59d3b1368, 846a0a54e0cc41c26c3b0e26aeac0a97, 7372cee9c3d57cc50d74313fb3a0335, 2f0d994e0b330f621651849bf9291d5e, 96b87ab907f9646fd4374d3087dbae2e, b1735590e90e0806118bcf419c25d1bb, 2ecff0c89e31676649864e3b9e7ee017, 5484006c7f5b05fa73d6c5cb52cdfba1, bfc808757aad0a8f428586090b1fd633, ebf05cc4e0de1bc1caaadb83506d818e, 8cb65e226881b043a3004522602388b1, 9e3554dc27cfa16b07ef972a5c6c935b, a90a18141447873948f464a3887261c9, e672e2cdc5682601f520676bdd689028, 30b6745aebcd7d78eed3d69dcb80949f, 171b9cf5c78bc540dfeaa4ad2a3ff1d, 57e5aa7e6bc2239032d566d1c3d3f0fe, 46a7f24013ab7a2af4a8ca23fe0d8fb4, 907d70972f2e22429232f930bad276e0)\n",
      "\n",
      "## BEGIN_GRAPH\n",
      "IR {\n",
      "  %0 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=0\n",
      "  %1 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=1\n",
      "  %2 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=2\n",
      "  %3 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %4 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%3), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}, ROOT=3\n",
      "  %5 = s64[2,1,128]{2,0,1} xla::device_data(), xla_shape=s64[2,1,128]{2,0,1}\n",
      "  %6 = s64[2,1,128]{2,0,1} xla::device_data(), xla_shape=s64[2,1,128]{2,0,1}\n",
      "  %7 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %8 = f32[2,8,2,128]{3,2,1,0} aten::expand(%7), xla_shape=f32[2,8,2,128]{3,2,1,0}\n",
      "  %9 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %10 = f32[2,8,2,128]{3,2,1,0} aten::expand(%9), xla_shape=f32[2,8,2,128]{3,2,1,0}\n",
      "  %11 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %12 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%11), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %13 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %14 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%13), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %15 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %16 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%15), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %17 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %18 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%17), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %19 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %20 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%19), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %21 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%2), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %22 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%1), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %23 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%0), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %24 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %25 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) xla::scan(%24, %4, %23, %22, %21, %20, %18, %16, %14, %12, %10, %8, %6, %5), num_outputs=14, xla_shape=(s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}), ROOT=4\n",
      "  %26 = f32[16,2,128,8]{3,2,1,0} aten::view(%25.5), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=5\n",
      "  %27 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %28 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%27), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %29 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %30 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%29), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %31 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %32 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%31), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %33 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %34 = f32[] aten::expand(%33), xla_shape=f32[]\n",
      "  %35 = f32[16,2,128,8]{3,2,1,0} aten::expand(%34), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %36 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%35), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %37 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %38 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%37), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) xla::scan(%39, %38, %36, %25.6, %25.7, %25.8, %25.9, %25.10, %25.11, %32, %30, %28), num_outputs=12, xla_shape=(s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}), ROOT=9\n",
      "  %41 = f32[16,2,128,8]{3,2,1,0} aten::view(%40.11), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=10\n",
      "  %42 = f32[16,2,128,8]{3,2,1,0} aten::view(%40.10), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=11\n",
      "  %43 = f32[16,2,128,8]{3,2,1,0} aten::view(%40.9), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=12\n",
      "  %44 = f32[16,2,128,8]{3,2,1,0} aten::expand(%2), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %45 = f32[32,128,8]{2,1,0} aten::view(%44), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %46 = f32[16,2,8,128]{3,0,2,1} aten::permute(%1), xla_shape=f32[16,2,8,128]{3,0,2,1}\n",
      "  %47 = f32[16,2,8,128]{3,2,1,0} aten::expand(%46), xla_shape=f32[16,2,8,128]{3,2,1,0}\n",
      "  %48 = f32[32,8,128]{2,1,0} aten::view(%47), xla_shape=f32[32,8,128]{2,1,0}\n",
      "  %49 = f32[16,2,128,8]{3,2,1,0} aten::expand(%0), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %50 = f32[32,128,8]{2,1,0} aten::view(%49), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %51 = f32[32,128,128]{2,1,0} aten::matmul(%50, %48), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %52 = f32[16,2,128,128]{3,2,1,0} aten::view(%51), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %53 = f32[16,2,128,128]{3,2,1,0} aten::softmax(%52), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %54 = f32[16,2,128,128]{3,2,1,0} aten::expand(%53), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %55 = f32[32,128,128]{2,1,0} aten::view(%54), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %56 = f32[32,128,8]{2,1,0} aten::matmul(%55, %45), xla_shape=f32[32,128,8]{2,1,0}, ROOT=13\n",
      "  %57 = f32[16,2,128,8]{3,2,1,0} aten::view(%56), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=14\n",
      "  %58 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %59 = f32[] aten::expand(%58), xla_shape=f32[]\n",
      "  %60 = f32[16,2,128,8]{3,2,1,0} aten::expand(%59), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %61 = f32[32,128,8]{2,1,0} aten::view(%60), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %62 = f32[32,128,128]{1,2,0} aten::permute(%55), xla_shape=f32[32,128,128]{1,2,0}\n",
      "  %63 = f32[32,128,8]{2,1,0} aten::matmul(%62, %61), xla_shape=f32[32,128,8]{2,1,0}, ROOT=15\n",
      "  %64 = f32[16,2,128,8]{3,2,1,0} aten::view(%63), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=16\n",
      "  %65 = f32[32,8,128]{1,2,0} aten::permute(%45), xla_shape=f32[32,8,128]{1,2,0}\n",
      "  %66 = f32[32,128,128]{2,1,0} aten::matmul(%61, %65), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %67 = f32[16,2,128,128]{3,2,1,0} aten::view(%66), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %68 = f32[16,2,128,128]{3,2,1,0} aten::_softmax_backward_data(%67, %53), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %69 = f32[32,128,128]{2,1,0} aten::view(%68), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %70 = f32[32,8,128]{1,2,0} aten::permute(%50), xla_shape=f32[32,8,128]{1,2,0}\n",
      "  %71 = f32[32,8,128]{2,1,0} aten::matmul(%70, %69), xla_shape=f32[32,8,128]{2,1,0}, ROOT=17\n",
      "  %72 = f32[32,128,8]{1,2,0} aten::permute(%48), xla_shape=f32[32,128,8]{1,2,0}\n",
      "  %73 = f32[32,128,8]{2,1,0} aten::matmul(%69, %72), xla_shape=f32[32,128,8]{2,1,0}, ROOT=18\n",
      "  %74 = f32[16,2,128,8]{3,2,1,0} aten::view(%73), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=19\n",
      "  %75 = f32[16,2,8,128]{3,2,1,0} aten::view(%71), xla_shape=f32[16,2,8,128]{3,2,1,0}\n",
      "  %76 = f32[16,2,128,8]{2,3,1,0} aten::permute(%75), xla_shape=f32[16,2,128,8]{2,3,1,0}, ROOT=20\n",
      "}\n",
      "\n",
      "Graph Hash: f5eebbf5977d37625a040b12d3222819\n",
      "\n",
      "## END_GRAPH\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path(\"ir_dumps/aot-sharded-flash-attention.txt.0\").read_text()\n",
    "print(text.split(\"[ScheduleSyncTensorsGraph]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule SyncTensorsGraph.644, entry_computation_layout={(f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, s64[2,1,128]{2,1,0:T(1,128)}, s64[2,1,128]{2,1,0:T(1,128)})->(f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, /*index=5*/f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, /*index=10*/f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, /*index=15*/f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[32,8,128]{2,1,0:T(8,128)}, f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, /*index=20*/f32[16,2,128,8]{2,3,1,0:T(8,128)})}, allow_spmd_sharding_propagation_to_output={true}, num_partitions=8\n",
      "\n",
      "FnComputation.50 {\n",
      "  p0.52 = f32[2,8,2,128,8]{3,4,1,2,0} parameter(0), sharding={replicated}\n",
      "  constant.51 = f32[] constant(1)\n",
      "  broadcast.53 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(constant.51), dimensions={}\n",
      "  multiply.54 = f32[2,8,2,128,8]{4,3,2,1,0} multiply(p0.52, broadcast.53)\n",
      "  p3.76 = f32[8,2,128,8]{2,3,0,1} parameter(3), sharding={replicated}\n",
      "  constant.75 = f32[] constant(1)\n",
      "  broadcast.77 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.75), dimensions={}\n",
      "  multiply.78 = f32[8,2,128,8]{3,2,1,0} multiply(p3.76, broadcast.77)\n",
      "  constant.74 = f32[] constant(1)\n",
      "  broadcast.79 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.74), dimensions={}\n",
      "  multiply.80 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.78, broadcast.79)\n",
      "  custom-call.81 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.80), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.82 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.81), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p2.67 = f32[8,2,128,8]{2,3,0,1} parameter(2), sharding={replicated}\n",
      "  constant.66 = f32[] constant(1)\n",
      "  broadcast.68 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.66), dimensions={}\n",
      "  multiply.69 = f32[8,2,128,8]{3,2,1,0} multiply(p2.67, broadcast.68)\n",
      "  constant.65 = f32[] constant(1)\n",
      "  broadcast.70 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.65), dimensions={}\n",
      "  multiply.71 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.69, broadcast.70)\n",
      "  custom-call.72 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.71), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.73 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.72), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p1.58 = f32[8,2,128,8]{2,3,0,1} parameter(1), sharding={replicated}\n",
      "  constant.57 = f32[] constant(1)\n",
      "  broadcast.59 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.57), dimensions={}\n",
      "  multiply.60 = f32[8,2,128,8]{3,2,1,0} multiply(p1.58, broadcast.59)\n",
      "  constant.56 = f32[] constant(1)\n",
      "  broadcast.61 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.56), dimensions={}\n",
      "  multiply.62 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.60, broadcast.61)\n",
      "  custom-call.63 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.62), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.64 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.63), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  custom-call.83 = (f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}) custom-call(custom-call.82, custom-call.73, custom-call.64), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"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\", \"cost_estimate\": {\"flops\": 1114112, \"transcendentals\": 32768, \"bytes_accessed\": 294912}, \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  get-tuple-element.84 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.83), index=0\n",
      "  custom-call.87 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.84), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.88 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.87), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.55 = f32[] constant(1)\n",
      "  broadcast.89 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.55), dimensions={}\n",
      "  multiply.90 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.88, broadcast.89)\n",
      "  get-tuple-element.85 = f32[2,1,128,128]{3,2,1,0} get-tuple-element(custom-call.83), index=1\n",
      "  reshape.92 = f32[32768]{0} reshape(get-tuple-element.85)\n",
      "  p4.91 = s64[2,1,128]{2,0,1} parameter(4), sharding={replicated}\n",
      "  reshape.93 = s64[256]{0} reshape(p4.91)\n",
      "  constant.97 = s64[] constant(0)\n",
      "  broadcast.98 = s64[256]{0} broadcast(constant.97), dimensions={}\n",
      "  compare.99 = pred[256]{0} compare(reshape.93, broadcast.98), direction=GE\n",
      "  constant.94 = s64[] constant(32768)\n",
      "  broadcast.95 = s64[256]{0} broadcast(constant.94), dimensions={}\n",
      "  add.96 = s64[256]{0} add(reshape.93, broadcast.95)\n",
      "  select.100 = s64[256]{0} select(compare.99, reshape.93, add.96)\n",
      "  convert.101 = u32[256]{0} convert(select.100)\n",
      "  gather.102 = f32[256]{0} gather(reshape.92, convert.101), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}\n",
      "  reshape.103 = f32[2,1,128]{2,1,0} reshape(gather.102)\n",
      "  custom-call.104 = f32[2,1,128]{2,0,1} custom-call(reshape.103), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.105 = f32[8,2,128]{2,1,0} custom-call(custom-call.104), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n",
      "  get-tuple-element.86 = f32[2,1,128,128]{3,2,1,0} get-tuple-element(custom-call.83), index=2\n",
      "  reshape.107 = f32[32768]{0} reshape(get-tuple-element.86)\n",
      "  p5.106 = s64[2,1,128]{2,0,1} parameter(5), sharding={replicated}\n",
      "  reshape.108 = s64[256]{0} reshape(p5.106)\n",
      "  constant.112 = s64[] constant(0)\n",
      "  broadcast.113 = s64[256]{0} broadcast(constant.112), dimensions={}\n",
      "  compare.114 = pred[256]{0} compare(reshape.108, broadcast.113), direction=GE\n",
      "  constant.109 = s64[] constant(32768)\n",
      "  broadcast.110 = s64[256]{0} broadcast(constant.109), dimensions={}\n",
      "  add.111 = s64[256]{0} add(reshape.108, broadcast.110)\n",
      "  select.115 = s64[256]{0} select(compare.114, reshape.108, add.111)\n",
      "  convert.116 = u32[256]{0} convert(select.115)\n",
      "  gather.117 = f32[256]{0} gather(reshape.107, convert.116), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}\n",
      "  reshape.118 = f32[2,1,128]{2,1,0} reshape(gather.117)\n",
      "  custom-call.119 = f32[2,1,128]{2,0,1} custom-call(reshape.118), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.120 = f32[8,2,128]{2,1,0} custom-call(custom-call.119), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1]0,1,2,3,4,5,6,7}\n",
      "  ROOT tuple.121 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, /*index=5*/f32[8,2,128,8]{3,2,1,0}, f32[8,2,128]{2,1,0}, f32[8,2,128]{2,1,0}) tuple(multiply.54, multiply.90, custom-call.88, multiply.80, multiply.71, /*index=5*/multiply.62, custom-call.105, custom-call.120)\n",
      "} // FnComputation.50\n",
      "\n",
      "Body.122 {\n",
      "  p0.123 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) parameter(0)\n",
      "  get-tuple-element.124 = s64[] get-tuple-element(p0.123), index=0\n",
      "  constant.243 = s64[] constant(1)\n",
      "  add.244 = s64[] add(get-tuple-element.124, constant.243)\n",
      "  get-tuple-element.125 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=1\n",
      "  get-tuple-element.128 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=4\n",
      "  constant.158 = s64[] constant(0)\n",
      "  broadcast.159 = s64[] broadcast(constant.158), dimensions={}\n",
      "  constant.160 = s64[] constant(0)\n",
      "  broadcast.161 = s64[] broadcast(constant.160), dimensions={}\n",
      "  constant.162 = s64[] constant(0)\n",
      "  broadcast.163 = s64[] broadcast(constant.162), dimensions={}\n",
      "  constant.164 = s64[] constant(0)\n",
      "  broadcast.165 = s64[] broadcast(constant.164), dimensions={}\n",
      "  dynamic-slice.166 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.128, get-tuple-element.124, broadcast.159, broadcast.161, broadcast.163, /*index=5*/broadcast.165), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.167 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.166)\n",
      "  get-tuple-element.127 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=3\n",
      "  constant.148 = s64[] constant(0)\n",
      "  broadcast.149 = s64[] broadcast(constant.148), dimensions={}\n",
      "  constant.150 = s64[] constant(0)\n",
      "  broadcast.151 = s64[] broadcast(constant.150), dimensions={}\n",
      "  constant.152 = s64[] constant(0)\n",
      "  broadcast.153 = s64[] broadcast(constant.152), dimensions={}\n",
      "  constant.154 = s64[] constant(0)\n",
      "  broadcast.155 = s64[] broadcast(constant.154), dimensions={}\n",
      "  dynamic-slice.156 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.127, get-tuple-element.124, broadcast.149, broadcast.151, broadcast.153, /*index=5*/broadcast.155), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.157 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.156)\n",
      "  get-tuple-element.126 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=2\n",
      "  constant.138 = s64[] constant(0)\n",
      "  broadcast.139 = s64[] broadcast(constant.138), dimensions={}\n",
      "  constant.140 = s64[] constant(0)\n",
      "  broadcast.141 = s64[] broadcast(constant.140), dimensions={}\n",
      "  constant.142 = s64[] constant(0)\n",
      "  broadcast.143 = s64[] broadcast(constant.142), dimensions={}\n",
      "  constant.144 = s64[] constant(0)\n",
      "  broadcast.145 = s64[] broadcast(constant.144), dimensions={}\n",
      "  dynamic-slice.146 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.126, get-tuple-element.124, broadcast.139, broadcast.141, broadcast.143, /*index=5*/broadcast.145), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.147 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.146)\n",
      "  get-tuple-element.137 = s64[2,1,128]{2,1,0} get-tuple-element(p0.123), index=13\n",
      "  get-tuple-element.136 = s64[2,1,128]{2,1,0} get-tuple-element(p0.123), index=12\n",
      "  call.168 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, /*index=5*/f32[8,2,128,8]{3,2,1,0}, f32[8,2,128]{2,1,0}, f32[8,2,128]{2,1,0}) call(get-tuple-element.125, reshape.167, reshape.157, reshape.147, get-tuple-element.137, /*index=5*/get-tuple-element.136), to_apply=FnComputation.50\n",
      "  get-tuple-element.169 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.168), index=0\n",
      "  get-tuple-element.129 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=5\n",
      "  get-tuple-element.170 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.168), index=1\n",
      "  broadcast.171 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.170), dimensions={1,2,3,4}\n",
      "  constant.172 = s64[] constant(0)\n",
      "  broadcast.173 = s64[] broadcast(constant.172), dimensions={}\n",
      "  constant.174 = s64[] constant(0)\n",
      "  broadcast.175 = s64[] broadcast(constant.174), dimensions={}\n",
      "  constant.176 = s64[] constant(0)\n",
      "  broadcast.177 = s64[] broadcast(constant.176), dimensions={}\n",
      "  constant.178 = s64[] constant(0)\n",
      "  broadcast.179 = s64[] broadcast(constant.178), dimensions={}\n",
      "  dynamic-update-slice.180 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.129, broadcast.171, get-tuple-element.124, broadcast.173, broadcast.175, /*index=5*/broadcast.177, broadcast.179)\n",
      "  get-tuple-element.130 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=6\n",
      "  get-tuple-element.181 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.168), index=2\n",
      "  broadcast.182 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.181), dimensions={1,2,3,4}\n",
      "  constant.183 = s64[] constant(0)\n",
      "  broadcast.184 = s64[] broadcast(constant.183), dimensions={}\n",
      "  constant.185 = s64[] constant(0)\n",
      "  broadcast.186 = s64[] broadcast(constant.185), dimensions={}\n",
      "  constant.187 = s64[] constant(0)\n",
      "  broadcast.188 = s64[] broadcast(constant.187), dimensions={}\n",
      "  constant.189 = s64[] constant(0)\n",
      "  broadcast.190 = s64[] broadcast(constant.189), dimensions={}\n",
      "  dynamic-update-slice.191 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.130, broadcast.182, get-tuple-element.124, broadcast.184, broadcast.186, /*index=5*/broadcast.188, broadcast.190)\n",
      "  get-tuple-element.131 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=7\n",
      "  get-tuple-element.192 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.168), index=3\n",
      "  broadcast.193 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.192), dimensions={1,2,3,4}\n",
      "  constant.194 = s64[] constant(0)\n",
      "  broadcast.195 = s64[] broadcast(constant.194), dimensions={}\n",
      "  constant.196 = s64[] constant(0)\n",
      "  broadcast.197 = s64[] broadcast(constant.196), dimensions={}\n",
      "  constant.198 = s64[] constant(0)\n",
      "  broadcast.199 = s64[] broadcast(constant.198), dimensions={}\n",
      "  constant.200 = s64[] constant(0)\n",
      "  broadcast.201 = s64[] broadcast(constant.200), dimensions={}\n",
      "  dynamic-update-slice.202 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.131, broadcast.193, get-tuple-element.124, broadcast.195, broadcast.197, /*index=5*/broadcast.199, broadcast.201)\n",
      "  get-tuple-element.132 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=8\n",
      "  get-tuple-element.203 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.168), index=4\n",
      "  broadcast.204 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.203), dimensions={1,2,3,4}\n",
      "  constant.205 = s64[] constant(0)\n",
      "  broadcast.206 = s64[] broadcast(constant.205), dimensions={}\n",
      "  constant.207 = s64[] constant(0)\n",
      "  broadcast.208 = s64[] broadcast(constant.207), dimensions={}\n",
      "  constant.209 = s64[] constant(0)\n",
      "  broadcast.210 = s64[] broadcast(constant.209), dimensions={}\n",
      "  constant.211 = s64[] constant(0)\n",
      "  broadcast.212 = s64[] broadcast(constant.211), dimensions={}\n",
      "  dynamic-update-slice.213 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.132, broadcast.204, get-tuple-element.124, broadcast.206, broadcast.208, /*index=5*/broadcast.210, broadcast.212)\n",
      "  get-tuple-element.133 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.123), index=9\n",
      "  get-tuple-element.214 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.168), index=5\n",
      "  broadcast.215 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.214), dimensions={1,2,3,4}\n",
      "  constant.216 = s64[] constant(0)\n",
      "  broadcast.217 = s64[] broadcast(constant.216), dimensions={}\n",
      "  constant.218 = s64[] constant(0)\n",
      "  broadcast.219 = s64[] broadcast(constant.218), dimensions={}\n",
      "  constant.220 = s64[] constant(0)\n",
      "  broadcast.221 = s64[] broadcast(constant.220), dimensions={}\n",
      "  constant.222 = s64[] constant(0)\n",
      "  broadcast.223 = s64[] broadcast(constant.222), dimensions={}\n",
      "  dynamic-update-slice.224 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.133, broadcast.215, get-tuple-element.124, broadcast.217, broadcast.219, /*index=5*/broadcast.221, broadcast.223)\n",
      "  get-tuple-element.134 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.123), index=10\n",
      "  get-tuple-element.225 = f32[8,2,128]{2,1,0} get-tuple-element(call.168), index=6\n",
      "  broadcast.226 = f32[1,8,2,128]{3,2,1,0} broadcast(get-tuple-element.225), dimensions={1,2,3}\n",
      "  constant.227 = s64[] constant(0)\n",
      "  broadcast.228 = s64[] broadcast(constant.227), dimensions={}\n",
      "  constant.229 = s64[] constant(0)\n",
      "  broadcast.230 = s64[] broadcast(constant.229), dimensions={}\n",
      "  constant.231 = s64[] constant(0)\n",
      "  broadcast.232 = s64[] broadcast(constant.231), dimensions={}\n",
      "  dynamic-update-slice.233 = f32[2,8,2,128]{3,2,1,0} dynamic-update-slice(get-tuple-element.134, broadcast.226, get-tuple-element.124, broadcast.228, broadcast.230, /*index=5*/broadcast.232)\n",
      "  get-tuple-element.135 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.123), index=11\n",
      "  get-tuple-element.234 = f32[8,2,128]{2,1,0} get-tuple-element(call.168), index=7\n",
      "  broadcast.235 = f32[1,8,2,128]{3,2,1,0} broadcast(get-tuple-element.234), dimensions={1,2,3}\n",
      "  constant.236 = s64[] constant(0)\n",
      "  broadcast.237 = s64[] broadcast(constant.236), dimensions={}\n",
      "  constant.238 = s64[] constant(0)\n",
      "  broadcast.239 = s64[] broadcast(constant.238), dimensions={}\n",
      "  constant.240 = s64[] constant(0)\n",
      "  broadcast.241 = s64[] broadcast(constant.240), dimensions={}\n",
      "  dynamic-update-slice.242 = f32[2,8,2,128]{3,2,1,0} dynamic-update-slice(get-tuple-element.135, broadcast.235, get-tuple-element.124, broadcast.237, broadcast.239, /*index=5*/broadcast.241)\n",
      "  ROOT tuple.245 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) tuple(add.244, get-tuple-element.169, get-tuple-element.126, get-tuple-element.127, get-tuple-element.128, /*index=5*/dynamic-update-slice.180, dynamic-update-slice.191, dynamic-update-slice.202, dynamic-update-slice.213, dynamic-update-slice.224, /*index=10*/dynamic-update-slice.233, dynamic-update-slice.242, get-tuple-element.136, get-tuple-element.137)\n",
      "} // Body.122\n",
      "\n",
      "Condition.246 {\n",
      "  p0.247 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) parameter(0)\n",
      "  get-tuple-element.248 = s64[] get-tuple-element(p0.247), index=0\n",
      "  constant.262 = s64[] constant(2)\n",
      "  ROOT compare.263 = pred[] compare(get-tuple-element.248, constant.262), direction=LT\n",
      "}\n",
      "\n",
      "scan.264 {\n",
      "  p0.265 = s64[] parameter(0)\n",
      "  p1.266 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(1)\n",
      "  p2.267 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(2)\n",
      "  p3.268 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(3)\n",
      "  p4.269 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(4)\n",
      "  p5.270 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(5)\n",
      "  p6.271 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(6)\n",
      "  p7.272 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(7)\n",
      "  p8.273 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(8)\n",
      "  p9.274 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(9)\n",
      "  p10.275 = f32[2,8,2,128]{3,2,1,0} parameter(10)\n",
      "  p11.276 = f32[2,8,2,128]{3,2,1,0} parameter(11)\n",
      "  p12.277 = s64[2,1,128]{2,1,0} parameter(12)\n",
      "  p13.278 = s64[2,1,128]{2,1,0} parameter(13)\n",
      "  tuple.279 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) tuple(p0.265, p1.266, p2.267, p3.268, p4.269, /*index=5*/p5.270, p6.271, p7.272, p8.273, p9.274, /*index=10*/p10.275, p11.276, p12.277, p13.278)\n",
      "  ROOT while.280 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) while(tuple.279), condition=Condition.246, body=Body.122\n",
      "} // scan.264\n",
      "\n",
      "AddComputation.325 {\n",
      "  x.326 = f32[] parameter(0)\n",
      "  y.327 = f32[] parameter(1)\n",
      "  ROOT add.328 = f32[] add(x.326, y.327)\n",
      "}\n",
      "\n",
      "FnComputation.329 {\n",
      "  p0.331 = f32[2,8,2,128,8]{3,4,1,2,0} parameter(0), sharding={replicated}\n",
      "  constant.330 = f32[] constant(1)\n",
      "  broadcast.332 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(constant.330), dimensions={}\n",
      "  multiply.333 = f32[2,8,2,128,8]{4,3,2,1,0} multiply(p0.331, broadcast.332)\n",
      "  p7.391 = f32[8,2,128,8]{2,3,0,1} parameter(7), sharding={replicated}\n",
      "  constant.390 = f32[] constant(1)\n",
      "  broadcast.392 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.390), dimensions={}\n",
      "  multiply.393 = f32[8,2,128,8]{3,2,1,0} multiply(p7.391, broadcast.392)\n",
      "  custom-call.394 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.393), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.395 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.394), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p6.385 = f32[8,2,128,8]{2,3,0,1} parameter(6), sharding={replicated}\n",
      "  constant.384 = f32[] constant(1)\n",
      "  broadcast.386 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.384), dimensions={}\n",
      "  multiply.387 = f32[8,2,128,8]{3,2,1,0} multiply(p6.385, broadcast.386)\n",
      "  custom-call.388 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.387), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.389 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.388), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p5.379 = f32[8,2,128,8]{2,3,0,1} parameter(5), sharding={replicated}\n",
      "  constant.378 = f32[] constant(1)\n",
      "  broadcast.380 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.378), dimensions={}\n",
      "  multiply.381 = f32[8,2,128,8]{3,2,1,0} multiply(p5.379, broadcast.380)\n",
      "  custom-call.382 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.381), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.383 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.382), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p4.369 = f32[8,2,128]{2,0,1} parameter(4), sharding={replicated}\n",
      "  constant.368 = f32[] constant(1)\n",
      "  broadcast.370 = f32[8,2,128]{2,1,0} broadcast(constant.368), dimensions={}\n",
      "  multiply.371 = f32[8,2,128]{2,1,0} multiply(p4.369, broadcast.370)\n",
      "  reshape.372 = f32[8,2,128,1]{3,2,1,0} reshape(multiply.371)\n",
      "  broadcast.373 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.372), dimensions={0,1,2,3}\n",
      "  reshape.374 = f32[8,2,128]{2,1,0} reshape(broadcast.373)\n",
      "  broadcast.375 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.374), dimensions={0,1,2}\n",
      "  custom-call.376 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.375), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.377 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.376), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p3.359 = f32[8,2,128]{2,0,1} parameter(3), sharding={replicated}\n",
      "  constant.358 = f32[] constant(1)\n",
      "  broadcast.360 = f32[8,2,128]{2,1,0} broadcast(constant.358), dimensions={}\n",
      "  multiply.361 = f32[8,2,128]{2,1,0} multiply(p3.359, broadcast.360)\n",
      "  reshape.362 = f32[8,2,128,1]{3,2,1,0} reshape(multiply.361)\n",
      "  broadcast.363 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.362), dimensions={0,1,2,3}\n",
      "  reshape.364 = f32[8,2,128]{2,1,0} reshape(broadcast.363)\n",
      "  broadcast.365 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.364), dimensions={0,1,2}\n",
      "  custom-call.366 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.365), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.367 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.366), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p1.337 = f32[8,2,128,8]{2,3,0,1} parameter(1), sharding={replicated}\n",
      "  constant.336 = f32[] constant(1)\n",
      "  broadcast.338 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.336), dimensions={}\n",
      "  multiply.339 = f32[8,2,128,8]{3,2,1,0} multiply(p1.337, broadcast.338)\n",
      "  constant.335 = f32[] constant(1)\n",
      "  broadcast.340 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.335), dimensions={}\n",
      "  multiply.341 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.339, broadcast.340)\n",
      "  custom-call.356 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.341), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.357 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.356), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p2.343 = f32[8,2,128,8]{2,3,0,1} parameter(2), sharding={replicated}\n",
      "  constant.342 = f32[] constant(1)\n",
      "  broadcast.344 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.342), dimensions={}\n",
      "  multiply.345 = f32[8,2,128,8]{3,2,1,0} multiply(p2.343, broadcast.344)\n",
      "  multiply.346 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.345, multiply.341)\n",
      "  constant.347 = f32[] constant(0)\n",
      "  reduce.349 = f32[8,2,128]{2,1,0} reduce(multiply.346, constant.347), dimensions={3}, to_apply=AddComputation.325\n",
      "  reshape.350 = f32[8,2,128,1]{3,2,1,0} reshape(reduce.349)\n",
      "  broadcast.351 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.350), dimensions={0,1,2,3}\n",
      "  reshape.352 = f32[8,2,128]{2,1,0} reshape(broadcast.351)\n",
      "  broadcast.353 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.352), dimensions={0,1,2}\n",
      "  custom-call.354 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.353), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.355 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.354), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  custom-call.396 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.395, custom-call.389, custom-call.383, custom-call.377, custom-call.367, /*index=5*/custom-call.357, custom-call.355), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"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\", \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  custom-call.397 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.396), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.398 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.397), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.334 = f32[] constant(1)\n",
      "  broadcast.399 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.334), dimensions={}\n",
      "  multiply.400 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.398, broadcast.399)\n",
      "  custom-call.402 = (f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}) custom-call(custom-call.395, custom-call.389, custom-call.383, custom-call.377, custom-call.367, /*index=5*/custom-call.357, custom-call.355), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"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\", \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  get-tuple-element.403 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.402), index=0\n",
      "  custom-call.405 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.403), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.406 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.405), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.401 = f32[] constant(1)\n",
      "  broadcast.407 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.401), dimensions={}\n",
      "  multiply.408 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.406, broadcast.407)\n",
      "  get-tuple-element.404 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.402), index=1\n",
      "  custom-call.410 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.404), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.411 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.410), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.409 = f32[] constant(1)\n",
      "  broadcast.412 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.409), dimensions={}\n",
      "  multiply.413 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.411, broadcast.412)\n",
      "  ROOT tuple.414 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}) tuple(multiply.333, multiply.400, multiply.408, multiply.413)\n",
      "} // FnComputation.329\n",
      "\n",
      "Body.415 {\n",
      "  p0.416 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.417 = s64[] get-tuple-element(p0.416), index=0\n",
      "  constant.532 = s64[] constant(1)\n",
      "  add.533 = s64[] add(get-tuple-element.417, constant.532)\n",
      "  get-tuple-element.418 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=1\n",
      "  get-tuple-element.419 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=2\n",
      "  constant.429 = s64[] constant(1)\n",
      "  subtract.430 = s64[] subtract(constant.429, get-tuple-element.417)\n",
      "  constant.431 = s64[] constant(0)\n",
      "  broadcast.432 = s64[] broadcast(constant.431), dimensions={}\n",
      "  constant.433 = s64[] constant(0)\n",
      "  broadcast.434 = s64[] broadcast(constant.433), dimensions={}\n",
      "  constant.435 = s64[] constant(0)\n",
      "  broadcast.436 = s64[] broadcast(constant.435), dimensions={}\n",
      "  constant.437 = s64[] constant(0)\n",
      "  broadcast.438 = s64[] broadcast(constant.437), dimensions={}\n",
      "  dynamic-slice.439 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.419, subtract.430, broadcast.432, broadcast.434, broadcast.436, /*index=5*/broadcast.438), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.440 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.439)\n",
      "  get-tuple-element.420 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=3\n",
      "  constant.441 = s64[] constant(0)\n",
      "  broadcast.442 = s64[] broadcast(constant.441), dimensions={}\n",
      "  constant.443 = s64[] constant(0)\n",
      "  broadcast.444 = s64[] broadcast(constant.443), dimensions={}\n",
      "  constant.445 = s64[] constant(0)\n",
      "  broadcast.446 = s64[] broadcast(constant.445), dimensions={}\n",
      "  constant.447 = s64[] constant(0)\n",
      "  broadcast.448 = s64[] broadcast(constant.447), dimensions={}\n",
      "  dynamic-slice.449 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.420, subtract.430, broadcast.442, broadcast.444, broadcast.446, /*index=5*/broadcast.448), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.450 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.449)\n",
      "  get-tuple-element.425 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.416), index=8\n",
      "  constant.489 = s64[] constant(0)\n",
      "  broadcast.490 = s64[] broadcast(constant.489), dimensions={}\n",
      "  constant.491 = s64[] constant(0)\n",
      "  broadcast.492 = s64[] broadcast(constant.491), dimensions={}\n",
      "  constant.493 = s64[] constant(0)\n",
      "  broadcast.494 = s64[] broadcast(constant.493), dimensions={}\n",
      "  dynamic-slice.495 = f32[1,8,2,128]{3,2,1,0} dynamic-slice(get-tuple-element.425, subtract.430, broadcast.490, broadcast.492, broadcast.494), dynamic_slice_sizes={1,8,2,128}\n",
      "  reshape.496 = f32[8,2,128]{2,1,0} reshape(dynamic-slice.495)\n",
      "  get-tuple-element.424 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.416), index=7\n",
      "  constant.481 = s64[] constant(0)\n",
      "  broadcast.482 = s64[] broadcast(constant.481), dimensions={}\n",
      "  constant.483 = s64[] constant(0)\n",
      "  broadcast.484 = s64[] broadcast(constant.483), dimensions={}\n",
      "  constant.485 = s64[] constant(0)\n",
      "  broadcast.486 = s64[] broadcast(constant.485), dimensions={}\n",
      "  dynamic-slice.487 = f32[1,8,2,128]{3,2,1,0} dynamic-slice(get-tuple-element.424, subtract.430, broadcast.482, broadcast.484, broadcast.486), dynamic_slice_sizes={1,8,2,128}\n",
      "  reshape.488 = f32[8,2,128]{2,1,0} reshape(dynamic-slice.487)\n",
      "  get-tuple-element.423 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=6\n",
      "  constant.471 = s64[] constant(0)\n",
      "  broadcast.472 = s64[] broadcast(constant.471), dimensions={}\n",
      "  constant.473 = s64[] constant(0)\n",
      "  broadcast.474 = s64[] broadcast(constant.473), dimensions={}\n",
      "  constant.475 = s64[] constant(0)\n",
      "  broadcast.476 = s64[] broadcast(constant.475), dimensions={}\n",
      "  constant.477 = s64[] constant(0)\n",
      "  broadcast.478 = s64[] broadcast(constant.477), dimensions={}\n",
      "  dynamic-slice.479 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.423, subtract.430, broadcast.472, broadcast.474, broadcast.476, /*index=5*/broadcast.478), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.480 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.479)\n",
      "  get-tuple-element.422 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=5\n",
      "  constant.461 = s64[] constant(0)\n",
      "  broadcast.462 = s64[] broadcast(constant.461), dimensions={}\n",
      "  constant.463 = s64[] constant(0)\n",
      "  broadcast.464 = s64[] broadcast(constant.463), dimensions={}\n",
      "  constant.465 = s64[] constant(0)\n",
      "  broadcast.466 = s64[] broadcast(constant.465), dimensions={}\n",
      "  constant.467 = s64[] constant(0)\n",
      "  broadcast.468 = s64[] broadcast(constant.467), dimensions={}\n",
      "  dynamic-slice.469 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.422, subtract.430, broadcast.462, broadcast.464, broadcast.466, /*index=5*/broadcast.468), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.470 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.469)\n",
      "  get-tuple-element.421 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=4\n",
      "  constant.451 = s64[] constant(0)\n",
      "  broadcast.452 = s64[] broadcast(constant.451), dimensions={}\n",
      "  constant.453 = s64[] constant(0)\n",
      "  broadcast.454 = s64[] broadcast(constant.453), dimensions={}\n",
      "  constant.455 = s64[] constant(0)\n",
      "  broadcast.456 = s64[] broadcast(constant.455), dimensions={}\n",
      "  constant.457 = s64[] constant(0)\n",
      "  broadcast.458 = s64[] broadcast(constant.457), dimensions={}\n",
      "  dynamic-slice.459 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.421, subtract.430, broadcast.452, broadcast.454, broadcast.456, /*index=5*/broadcast.458), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.460 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.459)\n",
      "  call.497 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}) call(get-tuple-element.418, reshape.440, reshape.450, reshape.496, reshape.488, /*index=5*/reshape.480, reshape.470, reshape.460), to_apply=FnComputation.329\n",
      "  get-tuple-element.498 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.497), index=0\n",
      "  get-tuple-element.426 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=9\n",
      "  get-tuple-element.499 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.497), index=1\n",
      "  broadcast.500 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.499), dimensions={1,2,3,4}\n",
      "  constant.501 = s64[] constant(0)\n",
      "  broadcast.502 = s64[] broadcast(constant.501), dimensions={}\n",
      "  constant.503 = s64[] constant(0)\n",
      "  broadcast.504 = s64[] broadcast(constant.503), dimensions={}\n",
      "  constant.505 = s64[] constant(0)\n",
      "  broadcast.506 = s64[] broadcast(constant.505), dimensions={}\n",
      "  constant.507 = s64[] constant(0)\n",
      "  broadcast.508 = s64[] broadcast(constant.507), dimensions={}\n",
      "  dynamic-update-slice.509 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.426, broadcast.500, subtract.430, broadcast.502, broadcast.504, /*index=5*/broadcast.506, broadcast.508)\n",
      "  get-tuple-element.427 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=10\n",
      "  get-tuple-element.510 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.497), index=2\n",
      "  broadcast.511 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.510), dimensions={1,2,3,4}\n",
      "  constant.512 = s64[] constant(0)\n",
      "  broadcast.513 = s64[] broadcast(constant.512), dimensions={}\n",
      "  constant.514 = s64[] constant(0)\n",
      "  broadcast.515 = s64[] broadcast(constant.514), dimensions={}\n",
      "  constant.516 = s64[] constant(0)\n",
      "  broadcast.517 = s64[] broadcast(constant.516), dimensions={}\n",
      "  constant.518 = s64[] constant(0)\n",
      "  broadcast.519 = s64[] broadcast(constant.518), dimensions={}\n",
      "  dynamic-update-slice.520 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.427, broadcast.511, subtract.430, broadcast.513, broadcast.515, /*index=5*/broadcast.517, broadcast.519)\n",
      "  get-tuple-element.428 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.416), index=11\n",
      "  get-tuple-element.521 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.497), index=3\n",
      "  broadcast.522 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.521), dimensions={1,2,3,4}\n",
      "  constant.523 = s64[] constant(0)\n",
      "  broadcast.524 = s64[] broadcast(constant.523), dimensions={}\n",
      "  constant.525 = s64[] constant(0)\n",
      "  broadcast.526 = s64[] broadcast(constant.525), dimensions={}\n",
      "  constant.527 = s64[] constant(0)\n",
      "  broadcast.528 = s64[] broadcast(constant.527), dimensions={}\n",
      "  constant.529 = s64[] constant(0)\n",
      "  broadcast.530 = s64[] broadcast(constant.529), dimensions={}\n",
      "  dynamic-update-slice.531 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.428, broadcast.522, subtract.430, broadcast.524, broadcast.526, /*index=5*/broadcast.528, broadcast.530)\n",
      "  ROOT tuple.534 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) tuple(add.533, get-tuple-element.498, get-tuple-element.419, get-tuple-element.420, get-tuple-element.421, /*index=5*/get-tuple-element.422, get-tuple-element.423, get-tuple-element.424, get-tuple-element.425, dynamic-update-slice.509, /*index=10*/dynamic-update-slice.520, dynamic-update-slice.531)\n",
      "} // Body.415\n",
      "\n",
      "Condition.535 {\n",
      "  p0.536 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.537 = s64[] get-tuple-element(p0.536), index=0\n",
      "  constant.549 = s64[] constant(2)\n",
      "  ROOT compare.550 = pred[] compare(get-tuple-element.537, constant.549), direction=LT\n",
      "}\n",
      "\n",
      "scan.551 {\n",
      "  p0.552 = s64[] parameter(0)\n",
      "  p1.553 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(1)\n",
      "  p2.554 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(2)\n",
      "  p3.555 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(3)\n",
      "  p4.556 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(4)\n",
      "  p5.557 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(5)\n",
      "  p6.558 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(6)\n",
      "  p7.559 = f32[2,8,2,128]{3,2,1,0} parameter(7)\n",
      "  p8.560 = f32[2,8,2,128]{3,2,1,0} parameter(8)\n",
      "  p9.561 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(9)\n",
      "  p10.562 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(10)\n",
      "  p11.563 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(11)\n",
      "  tuple.564 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) tuple(p0.552, p1.553, p2.554, p3.555, p4.556, /*index=5*/p5.557, p6.558, p7.559, p8.560, p9.561, /*index=10*/p10.562, p11.563)\n",
      "  ROOT while.565 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) while(tuple.564), condition=Condition.535, body=Body.415\n",
      "} // scan.551\n",
      "\n",
      "MaxComputation.592 {\n",
      "  x.593 = f32[] parameter(0)\n",
      "  y.594 = f32[] parameter(1)\n",
      "  ROOT maximum.595 = f32[] maximum(x.593, y.594)\n",
      "}\n",
      "\n",
      "AddComputation.601 {\n",
      "  x.602 = f32[] parameter(0)\n",
      "  y.603 = f32[] parameter(1)\n",
      "  ROOT add.604 = f32[] add(x.602, y.603)\n",
      "}\n",
      "\n",
      "AddComputation.627 {\n",
      "  x.628 = f32[] parameter(0)\n",
      "  y.629 = f32[] parameter(1)\n",
      "  ROOT add.630 = f32[] add(x.628, y.629)\n",
      "}\n",
      "\n",
      "ENTRY SyncTensorsGraph.644 {\n",
      "  p0.1 = f32[16,2,128,8]{2,0,3,1} parameter(0), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  p1.2 = f32[16,2,128,8]{2,0,3,1} parameter(1), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  p2.3 = f32[16,2,128,8]{2,0,3,1} parameter(2), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.4 = f32[] constant(0)\n",
      "  reshape.5 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.4)\n",
      "  broadcast.6 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.5), dimensions={0,1,2,3,4}\n",
      "  reshape.7 = f32[] reshape(broadcast.6)\n",
      "  broadcast.8 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.7), dimensions={}\n",
      "  constant.49 = s64[] constant(0)\n",
      "  reshape.48 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p0.1)\n",
      "  reshape.47 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p1.2)\n",
      "  reshape.46 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p2.3)\n",
      "  constant.41 = f32[] constant(0)\n",
      "  reshape.42 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.41)\n",
      "  broadcast.43 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.42), dimensions={0,1,2,3,4}\n",
      "  reshape.44 = f32[] reshape(broadcast.43)\n",
      "  broadcast.45 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.44), dimensions={}\n",
      "  constant.36 = f32[] constant(0)\n",
      "  reshape.37 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.36)\n",
      "  broadcast.38 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.37), dimensions={0,1,2,3,4}\n",
      "  reshape.39 = f32[] reshape(broadcast.38)\n",
      "  broadcast.40 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.39), dimensions={}\n",
      "  constant.31 = f32[] constant(0)\n",
      "  reshape.32 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.31)\n",
      "  broadcast.33 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.32), dimensions={0,1,2,3,4}\n",
      "  reshape.34 = f32[] reshape(broadcast.33)\n",
      "  broadcast.35 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.34), dimensions={}\n",
      "  constant.26 = f32[] constant(0)\n",
      "  reshape.27 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.26)\n",
      "  broadcast.28 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.27), dimensions={0,1,2,3,4}\n",
      "  reshape.29 = f32[] reshape(broadcast.28)\n",
      "  broadcast.30 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.29), dimensions={}\n",
      "  constant.21 = f32[] constant(0)\n",
      "  reshape.22 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.21)\n",
      "  broadcast.23 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.22), dimensions={0,1,2,3,4}\n",
      "  reshape.24 = f32[] reshape(broadcast.23)\n",
      "  broadcast.25 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.24), dimensions={}\n",
      "  constant.16 = f32[] constant(0)\n",
      "  reshape.17 = f32[1,1,1,1]{3,2,1,0} reshape(constant.16)\n",
      "  broadcast.18 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.17), dimensions={0,1,2,3}\n",
      "  reshape.19 = f32[] reshape(broadcast.18)\n",
      "  broadcast.20 = f32[2,8,2,128]{3,2,1,0} broadcast(reshape.19), dimensions={}\n",
      "  constant.11 = f32[] constant(0)\n",
      "  reshape.12 = f32[1,1,1,1]{3,2,1,0} reshape(constant.11)\n",
      "  broadcast.13 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.12), dimensions={0,1,2,3}\n",
      "  reshape.14 = f32[] reshape(broadcast.13)\n",
      "  broadcast.15 = f32[2,8,2,128]{3,2,1,0} broadcast(reshape.14), dimensions={}\n",
      "  p4.10 = s64[2,1,128]{2,0,1} parameter(4), sharding={replicated}\n",
      "  p3.9 = s64[2,1,128]{2,0,1} parameter(3), sharding={replicated}\n",
      "  call.281 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, s64[2,1,128]{2,1,0}, s64[2,1,128]{2,1,0}) call(constant.49, broadcast.8, reshape.48, reshape.47, reshape.46, /*index=5*/broadcast.45, broadcast.40, broadcast.35, broadcast.30, broadcast.25, /*index=10*/broadcast.20, broadcast.15, p4.10, p3.9), to_apply=scan.264\n",
      "  get-tuple-element.287 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.281), index=5\n",
      "  reshape.296 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.287)\n",
      "  constant.324 = s64[] constant(0)\n",
      "  constant.319 = f32[] constant(0)\n",
      "  reshape.320 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.319)\n",
      "  broadcast.321 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.320), dimensions={0,1,2,3,4}\n",
      "  reshape.322 = f32[] reshape(broadcast.321)\n",
      "  broadcast.323 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.322), dimensions={}\n",
      "  constant.312 = f32[] constant(1)\n",
      "  broadcast.313 = f32[] broadcast(constant.312), dimensions={}\n",
      "  reshape.314 = f32[1,1,1,1]{3,2,1,0} reshape(broadcast.313)\n",
      "  broadcast.315 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.314), dimensions={0,1,2,3}\n",
      "  reshape.316 = f32[] reshape(broadcast.315)\n",
      "  broadcast.317 = f32[16,2,128,8]{3,2,1,0} broadcast(reshape.316), dimensions={}\n",
      "  reshape.318 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(broadcast.317)\n",
      "  get-tuple-element.288 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.281), index=6\n",
      "  get-tuple-element.289 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.281), index=7\n",
      "  get-tuple-element.290 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.281), index=8\n",
      "  get-tuple-element.291 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.281), index=9\n",
      "  get-tuple-element.292 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(call.281), index=10\n",
      "  get-tuple-element.293 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(call.281), index=11\n",
      "  constant.307 = f32[] constant(0)\n",
      "  reshape.308 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.307)\n",
      "  broadcast.309 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.308), dimensions={0,1,2,3,4}\n",
      "  reshape.310 = f32[] reshape(broadcast.309)\n",
      "  broadcast.311 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.310), dimensions={}\n",
      "  constant.302 = f32[] constant(0)\n",
      "  reshape.303 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.302)\n",
      "  broadcast.304 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.303), dimensions={0,1,2,3,4}\n",
      "  reshape.305 = f32[] reshape(broadcast.304)\n",
      "  broadcast.306 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.305), dimensions={}\n",
      "  constant.297 = f32[] constant(0)\n",
      "  reshape.298 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.297)\n",
      "  broadcast.299 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.298), dimensions={0,1,2,3,4}\n",
      "  reshape.300 = f32[] reshape(broadcast.299)\n",
      "  broadcast.301 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.300), dimensions={}\n",
      "  call.566 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) call(constant.324, broadcast.323, reshape.318, get-tuple-element.288, get-tuple-element.289, /*index=5*/get-tuple-element.290, get-tuple-element.291, get-tuple-element.292, get-tuple-element.293, broadcast.311, /*index=10*/broadcast.306, broadcast.301), to_apply=scan.551\n",
      "  get-tuple-element.568 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.566), index=1\n",
      "  get-tuple-element.576 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.566), index=9\n",
      "  get-tuple-element.577 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.566), index=10\n",
      "  get-tuple-element.578 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.566), index=11\n",
      "  reshape.579 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.578)\n",
      "  reshape.580 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.577)\n",
      "  reshape.581 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.576)\n",
      "  broadcast.587 = f32[16,2,128,8]{3,2,1,0} broadcast(p0.1), dimensions={0,1,2,3}\n",
      "  reshape.588 = f32[32,128,8]{2,1,0} reshape(broadcast.587)\n",
      "  transpose.584 = f32[16,2,8,128]{3,0,2,1} transpose(p1.2), dimensions={0,1,3,2}\n",
      "  broadcast.585 = f32[16,2,8,128]{3,2,1,0} broadcast(transpose.584), dimensions={0,1,2,3}\n",
      "  reshape.586 = f32[32,8,128]{2,1,0} reshape(broadcast.585)\n",
      "  dot.589 = f32[32,128,128]{2,1,0} dot(reshape.588, reshape.586), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.590 = f32[16,2,128,128]{3,2,1,0} reshape(dot.589)\n",
      "  constant.591 = f32[] constant(-inf)\n",
      "  reduce.596 = f32[16,2,128]{2,1,0} reduce(reshape.590, constant.591), dimensions={3}, to_apply=MaxComputation.592\n",
      "  broadcast.597 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.596), dimensions={0,1,2}\n",
      "  subtract.598 = f32[16,2,128,128]{3,2,1,0} subtract(reshape.590, broadcast.597)\n",
      "  exponential.599 = f32[16,2,128,128]{3,2,1,0} exponential(subtract.598)\n",
      "  constant.600 = f32[] constant(0)\n",
      "  reduce.605 = f32[16,2,128]{2,1,0} reduce(exponential.599, constant.600), dimensions={3}, to_apply=AddComputation.601\n",
      "  broadcast.606 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.605), dimensions={0,1,2}\n",
      "  divide.607 = f32[16,2,128,128]{3,2,1,0} divide(exponential.599, broadcast.606)\n",
      "  broadcast.608 = f32[16,2,128,128]{3,2,1,0} broadcast(divide.607), dimensions={0,1,2,3}\n",
      "  reshape.609 = f32[32,128,128]{2,1,0} reshape(broadcast.608)\n",
      "  broadcast.582 = f32[16,2,128,8]{3,2,1,0} broadcast(p2.3), dimensions={0,1,2,3}\n",
      "  reshape.583 = f32[32,128,8]{2,1,0} reshape(broadcast.582)\n",
      "  dot.610 = f32[32,128,8]{2,1,0} dot(reshape.609, reshape.583), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.611 = f32[16,2,128,8]{3,2,1,0} reshape(dot.610)\n",
      "  transpose.619 = f32[32,128,128]{1,2,0} transpose(reshape.609), dimensions={0,2,1}\n",
      "  constant.612 = f32[] constant(1)\n",
      "  broadcast.613 = f32[] broadcast(constant.612), dimensions={}\n",
      "  reshape.614 = f32[1,1,1,1]{3,2,1,0} reshape(broadcast.613)\n",
      "  broadcast.615 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.614), dimensions={0,1,2,3}\n",
      "  reshape.616 = f32[] reshape(broadcast.615)\n",
      "  broadcast.617 = f32[16,2,128,8]{3,2,1,0} broadcast(reshape.616), dimensions={}\n",
      "  reshape.618 = f32[32,128,8]{2,1,0} reshape(broadcast.617)\n",
      "  dot.620 = f32[32,128,8]{2,1,0} dot(transpose.619, reshape.618), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.621 = f32[16,2,128,8]{3,2,1,0} reshape(dot.620)\n",
      "  transpose.636 = f32[32,8,128]{1,2,0} transpose(reshape.588), dimensions={0,2,1}\n",
      "  transpose.622 = f32[32,8,128]{1,2,0} transpose(reshape.583), dimensions={0,2,1}\n",
      "  dot.623 = f32[32,128,128]{2,1,0} dot(reshape.618, transpose.622), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.624 = f32[16,2,128,128]{3,2,1,0} reshape(dot.623)\n",
      "  multiply.625 = f32[16,2,128,128]{3,2,1,0} multiply(reshape.624, divide.607)\n",
      "  constant.626 = f32[] constant(0)\n",
      "  reduce.631 = f32[16,2,128]{2,1,0} reduce(multiply.625, constant.626), dimensions={3}, to_apply=AddComputation.627\n",
      "  broadcast.632 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.631), dimensions={0,1,2}\n",
      "  subtract.633 = f32[16,2,128,128]{3,2,1,0} subtract(reshape.624, broadcast.632)\n",
      "  multiply.634 = f32[16,2,128,128]{3,2,1,0} multiply(divide.607, subtract.633)\n",
      "  reshape.635 = f32[32,128,128]{2,1,0} reshape(multiply.634)\n",
      "  dot.637 = f32[32,8,128]{2,1,0} dot(transpose.636, reshape.635), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  transpose.638 = f32[32,128,8]{1,2,0} transpose(reshape.586), dimensions={0,2,1}\n",
      "  dot.639 = f32[32,128,8]{2,1,0} dot(reshape.635, transpose.638), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.640 = f32[16,2,128,8]{3,2,1,0} reshape(dot.639)\n",
      "  reshape.641 = f32[16,2,8,128]{3,2,1,0} reshape(dot.637)\n",
      "  transpose.642 = f32[16,2,128,8]{2,3,1,0} transpose(reshape.641), dimensions={0,1,3,2}\n",
      "  ROOT tuple.643 = (f32[16,2,128,8]{2,0,3,1}, f32[16,2,128,8]{2,0,3,1}, f32[16,2,128,8]{2,0,3,1}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[16,2,128,8]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[16,2,128,8]{3,2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, /*index=15*/f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[32,8,128]{2,1,0}, f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, /*index=20*/f32[16,2,128,8]{2,3,1,0}) tuple(p0.1, p1.2, p2.3, broadcast.8, get-tuple-element.287, /*index=5*/reshape.296, get-tuple-element.568, get-tuple-element.576, get-tuple-element.577, get-tuple-element.578, /*index=10*/reshape.579, reshape.580, reshape.581, dot.610, reshape.611, /*index=15*/dot.620, reshape.621, dot.637, dot.639, reshape.640, /*index=20*/transpose.642)\n",
      "} // SyncTensorsGraph.644\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "for f in os.listdir('xla_dumps/aot-sharded-flash-attention'):\n",
    "  p = Path(f\"xla_dumps/aot-sharded-flash-attention/{f}\")\n",
    "  if p.is_file():\n",
    "    if \"before_optimizations\" in str(p.stem):\n",
    "      print(p.read_text())\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewriting `as_strided` into some other operation that doesn't eagerly send\n",
    "tensors (`permute` in this notebook) avoids this crash.\n",
    "\n",
    "The numbers are still wrong, possibly due to another bug, but at least GSPMD\n",
    "sharding annotations are propagated successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to index into aux to get l, m\n",
      "Done indexing into aux. Metrics:\n",
      "\n",
      "About to index into aux to get l, m\n",
      "Done indexing into aux. Metrics:\n",
      "\n",
      "Executing graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:31)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:32)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   most likely user code trying to access tensor value before mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 3d9ebaf4585054aaca5ace517e577f8a\n",
      "Execution Analysis:   Number of Graph Inputs: 1\n",
      "Execution Analysis:   Number of Graph Outputs: 1\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   mark_sharding (/workspaces/torch/pytorch/xla/torch_xla/distributed/spmd/xla_sharding.py:627)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:33)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compilation Analysis: ================================================================================\n",
      "Compilation Analysis: Compilation Cause\n",
      "Compilation Analysis:   user mark_step\n",
      "Compilation Analysis: Graph Info: \n",
      "Compilation Analysis:   Graph Hash: 4b3ed403f54b79f0c84b4ac4c47bbdad\n",
      "Compilation Analysis:   Number of Graph Inputs: 3\n",
      "Compilation Analysis:   Number of Graph Outputs: 21\n",
      "Compilation Analysis: Python Frame Triggered Execution: \n",
      "Compilation Analysis:   sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "Compilation Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:58)\n",
      "Compilation Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "Compilation Analysis:   <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "Compilation Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Compilation Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Compilation Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Compilation Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Compilation Analysis:   ..........\n",
      "Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Compilation Analysis: ================================================================================\n",
      "\n",
      "Post Compilation Analysis: ================================================================================\n",
      "Post Compilation Analysis: Graph input size: 0.000046 GB\n",
      "Post Compilation Analysis: Graph output size: 0.000839 GB\n",
      "Post Compilation Analysis: Aliased Input size: 0.000000 GB\n",
      "Post Compilation Analysis: Intermediate tensor size: 0.000000 GB\n",
      "Post Compilation Analysis: Compiled program size: 0.001832 GB\n",
      "Post Compilation Analysis: --------------------------------------------------------------------------------\n",
      "Post Compilation Analysis: ================================================================================\n",
      "\n",
      "Execution Analysis: ================================================================================\n",
      "Execution Analysis: Execution Cause\n",
      "Execution Analysis:   user mark_step\n",
      "Execution Analysis: Graph Info: \n",
      "Execution Analysis:   Graph Hash: 4b3ed403f54b79f0c84b4ac4c47bbdad\n",
      "Execution Analysis:   Number of Graph Inputs: 3\n",
      "Execution Analysis:   Number of Graph Outputs: 21\n",
      "Execution Analysis: Python Frame Triggered Execution: \n",
      "Execution Analysis:   sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "Execution Analysis:   do_test (/tmp/ipykernel_2190827/803061339.py:58)\n",
      "Execution Analysis:   test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "Execution Analysis:   <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "Execution Analysis:   run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "Execution Analysis:   run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "Execution Analysis:   run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "Execution Analysis:   _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "Execution Analysis:   ..........\n",
      "Execution Analysis: --------------------------------------------------------------------------------\n",
      "Execution Analysis: ================================================================================\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 32752 / 32768 (100.0%)\nGreatest absolute difference: 12691.0400390625 at index (5, 0, 58, 4) (up to 1e-05 allowed)\nGreatest relative difference: 80902.328125 at index (15, 0, 6, 5) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m AVOID_AS_STRIDED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m clear_dumps()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_flash_attention_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflash_attention_in_scan\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m, in \u001b[0;36mtest_flash_attention_wrapper\u001b[0;34m(attn_fn)\u001b[0m\n\u001b[1;32m     70\u001b[0m jax\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax_default_matmul_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m   \u001b[43mdo_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_autograd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m   jax\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax_default_matmul_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36mdo_test\u001b[0;34m(attn_fn, aot_autograd)\u001b[0m\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_close(o_actual\u001b[38;5;241m.\u001b[39mcpu(), expected_o\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m q\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_clone\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_clone\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_clone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m k_clone\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_clone\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_close(k\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcpu(), k_clone\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 32752 / 32768 (100.0%)\nGreatest absolute difference: 12691.0400390625 at index (5, 0, 58, 4) (up to 1e-05 allowed)\nGreatest relative difference: 80902.328125 at index (15, 0, 6, 5) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "AVOID_AS_STRIDED = True\n",
    "\n",
    "clear_dumps()\n",
    "\n",
    "test_flash_attention_wrapper(flash_attention_in_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorsGraphInfo:\n",
      "  sync (/workspaces/torch/pytorch/xla/torch_xla/torch_xla.py:69)\n",
      "  do_test (/tmp/ipykernel_2190827/803061339.py:58)\n",
      "  test_flash_attention_wrapper (/tmp/ipykernel_2190827/803061339.py:72)\n",
      "  <module> (/tmp/ipykernel_2190827/3180461044.py:5)\n",
      "  run_code (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577)\n",
      "  run_ast_nodes (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3517)\n",
      "  run_cell_async (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3334)\n",
      "  _pseudo_sync_runner (/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py:128)\n",
      "  _run_cell (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3130)\n",
      "  run_cell (/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3075)\n",
      "  run_cell (/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py:549)\n",
      "  do_execute (/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py:449)\n",
      "  execute_request (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:778)\n",
      "  execute_request (/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py:362)\n",
      "  dispatch_shell (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:437)\n",
      "  process_one (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:534)\n",
      "  dispatch_queue (/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:545)\n",
      "  _run (/usr/local/lib/python3.10/asyncio/events.py:80)\n",
      "  _run_once (/usr/local/lib/python3.10/asyncio/base_events.py:1909)\n",
      "  run_forever (/usr/local/lib/python3.10/asyncio/base_events.py:603)\n",
      "  start (/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py:205)\n",
      "  start (/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py:739)\n",
      "  launch_instance (/root/.local/lib/python3.10/site-packages/traitlets/config/application.py:1075)\n",
      "  <module> (/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py:18)\n",
      "  _run_code (/usr/local/lib/python3.10/runpy.py:86)\n",
      "  _run_module_as_main (/usr/local/lib/python3.10/runpy.py:196)\n",
      "\n",
      "Root Hashes: (f68b5996bbe97b2548777ac59d3b1368, f68b5996bbe97b2548777ac59d3b1368, f68b5996bbe97b2548777ac59d3b1368, 846a0a54e0cc41c26c3b0e26aeac0a97, 715cbdf8fc3d3bc25f6d1103c2ab6a9, e3eb108659d78261ac3619794b1a3801, 9d8b5079c672a7f33cfc3777623d32ce, a3dc8b62e5a9d31b7791b5644a74fe31, 223f26297cbaa4f89f8d34424c5becd5, dc47754e9f1cc147505abffd390cd847, f9f728c0a8861bd41ef4f8f8af6f4042, c9f2f8fec5a0289c5e8d8cf66b690776, f6a27f1b998cefdc5b7155231cc23847, 9e3554dc27cfa16b07ef972a5c6c935b, a90a18141447873948f464a3887261c9, e672e2cdc5682601f520676bdd689028, 30b6745aebcd7d78eed3d69dcb80949f, 171b9cf5c78bc540dfeaa4ad2a3ff1d, 57e5aa7e6bc2239032d566d1c3d3f0fe, 46a7f24013ab7a2af4a8ca23fe0d8fb4, 907d70972f2e22429232f930bad276e0)\n",
      "\n",
      "## BEGIN_GRAPH\n",
      "IR {\n",
      "  %0 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=0\n",
      "  %1 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=1\n",
      "  %2 = f32[16,2,128,8]{2,0,3,1} xla::device_data(), xla_shape=f32[16,2,128,8]{2,0,3,1}, ROOT=2\n",
      "  %3 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %4 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%3), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}, ROOT=3\n",
      "  %5 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %6 = f32[2,8,2,128]{3,2,1,0} aten::expand(%5), xla_shape=f32[2,8,2,128]{3,2,1,0}\n",
      "  %7 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %8 = f32[2,8,2,128]{3,2,1,0} aten::expand(%7), xla_shape=f32[2,8,2,128]{3,2,1,0}\n",
      "  %9 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %10 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%9), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %11 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %12 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%11), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %13 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %14 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%13), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %15 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %16 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%15), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %17 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %18 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%17), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %19 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%2), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %20 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%1), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %21 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%0), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %22 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %23 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) xla::scan(%22, %4, %21, %20, %19, %18, %16, %14, %12, %10, %8, %6), num_outputs=12, xla_shape=(s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}), ROOT=4\n",
      "  %24 = f32[16,2,128,8]{3,2,1,0} aten::view(%23.5), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=5\n",
      "  %25 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %26 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%25), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %27 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %28 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%27), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %29 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %30 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%29), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %31 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %32 = f32[] aten::expand(%31), xla_shape=f32[]\n",
      "  %33 = f32[16,2,128,8]{3,2,1,0} aten::expand(%32), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %34 = f32[2,8,2,128,8]{4,3,2,1,0} aten::view(%33), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %35 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %36 = f32[2,8,2,128,8]{4,3,2,1,0} aten::expand(%35), xla_shape=f32[2,8,2,128,8]{4,3,2,1,0}\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) xla::scan(%37, %36, %34, %23.6, %23.7, %23.8, %23.9, %23.10, %23.11, %30, %28, %26), num_outputs=12, xla_shape=(s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}), ROOT=9\n",
      "  %39 = f32[16,2,128,8]{3,2,1,0} aten::view(%38.11), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=10\n",
      "  %40 = f32[16,2,128,8]{3,2,1,0} aten::view(%38.10), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=11\n",
      "  %41 = f32[16,2,128,8]{3,2,1,0} aten::view(%38.9), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=12\n",
      "  %42 = f32[16,2,128,8]{3,2,1,0} aten::expand(%2), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %43 = f32[32,128,8]{2,1,0} aten::view(%42), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %44 = f32[16,2,8,128]{3,0,2,1} aten::permute(%1), xla_shape=f32[16,2,8,128]{3,0,2,1}\n",
      "  %45 = f32[16,2,8,128]{3,2,1,0} aten::expand(%44), xla_shape=f32[16,2,8,128]{3,2,1,0}\n",
      "  %46 = f32[32,8,128]{2,1,0} aten::view(%45), xla_shape=f32[32,8,128]{2,1,0}\n",
      "  %47 = f32[16,2,128,8]{3,2,1,0} aten::expand(%0), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %48 = f32[32,128,8]{2,1,0} aten::view(%47), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %49 = f32[32,128,128]{2,1,0} aten::matmul(%48, %46), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %50 = f32[16,2,128,128]{3,2,1,0} aten::view(%49), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %51 = f32[16,2,128,128]{3,2,1,0} aten::softmax(%50), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %52 = f32[16,2,128,128]{3,2,1,0} aten::expand(%51), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %53 = f32[32,128,128]{2,1,0} aten::view(%52), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %54 = f32[32,128,8]{2,1,0} aten::matmul(%53, %43), xla_shape=f32[32,128,8]{2,1,0}, ROOT=13\n",
      "  %55 = f32[16,2,128,8]{3,2,1,0} aten::view(%54), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=14\n",
      "  %56 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %57 = f32[] aten::expand(%56), xla_shape=f32[]\n",
      "  %58 = f32[16,2,128,8]{3,2,1,0} aten::expand(%57), xla_shape=f32[16,2,128,8]{3,2,1,0}\n",
      "  %59 = f32[32,128,8]{2,1,0} aten::view(%58), xla_shape=f32[32,128,8]{2,1,0}\n",
      "  %60 = f32[32,128,128]{1,2,0} aten::permute(%53), xla_shape=f32[32,128,128]{1,2,0}\n",
      "  %61 = f32[32,128,8]{2,1,0} aten::matmul(%60, %59), xla_shape=f32[32,128,8]{2,1,0}, ROOT=15\n",
      "  %62 = f32[16,2,128,8]{3,2,1,0} aten::view(%61), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=16\n",
      "  %63 = f32[32,8,128]{1,2,0} aten::permute(%43), xla_shape=f32[32,8,128]{1,2,0}\n",
      "  %64 = f32[32,128,128]{2,1,0} aten::matmul(%59, %63), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %65 = f32[16,2,128,128]{3,2,1,0} aten::view(%64), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %66 = f32[16,2,128,128]{3,2,1,0} aten::_softmax_backward_data(%65, %51), xla_shape=f32[16,2,128,128]{3,2,1,0}\n",
      "  %67 = f32[32,128,128]{2,1,0} aten::view(%66), xla_shape=f32[32,128,128]{2,1,0}\n",
      "  %68 = f32[32,8,128]{1,2,0} aten::permute(%48), xla_shape=f32[32,8,128]{1,2,0}\n",
      "  %69 = f32[32,8,128]{2,1,0} aten::matmul(%68, %67), xla_shape=f32[32,8,128]{2,1,0}, ROOT=17\n",
      "  %70 = f32[32,128,8]{1,2,0} aten::permute(%46), xla_shape=f32[32,128,8]{1,2,0}\n",
      "  %71 = f32[32,128,8]{2,1,0} aten::matmul(%67, %70), xla_shape=f32[32,128,8]{2,1,0}, ROOT=18\n",
      "  %72 = f32[16,2,128,8]{3,2,1,0} aten::view(%71), xla_shape=f32[16,2,128,8]{3,2,1,0}, ROOT=19\n",
      "  %73 = f32[16,2,8,128]{3,2,1,0} aten::view(%69), xla_shape=f32[16,2,8,128]{3,2,1,0}\n",
      "  %74 = f32[16,2,128,8]{2,3,1,0} aten::permute(%73), xla_shape=f32[16,2,128,8]{2,3,1,0}, ROOT=20\n",
      "}\n",
      "\n",
      "Graph Hash: 4b3ed403f54b79f0c84b4ac4c47bbdad\n",
      "\n",
      "## END_GRAPH\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path(\"ir_dumps/aot-sharded-flash-attention.txt.0\").read_text()\n",
    "print(text.split(\"[ScheduleSyncTensorsGraph]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule SyncTensorsGraph.614, entry_computation_layout={(f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)})->(f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, /*index=5*/f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, f32[2,8,2,128,8]{3,4,2,1,0:T(8,128)}, /*index=10*/f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, /*index=15*/f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, f32[32,8,128]{2,1,0:T(8,128)}, f32[32,128,8]{1,2,0:T(8,128)}, f32[16,2,128,8]{2,3,1,0:T(8,128)}, /*index=20*/f32[16,2,128,8]{2,3,1,0:T(8,128)})}, allow_spmd_sharding_propagation_to_output={true}, num_partitions=8\n",
      "\n",
      "FnComputation.48 {\n",
      "  p0.50 = f32[2,8,2,128,8]{3,4,1,2,0} parameter(0), sharding={replicated}\n",
      "  constant.49 = f32[] constant(1)\n",
      "  broadcast.51 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(constant.49), dimensions={}\n",
      "  multiply.52 = f32[2,8,2,128,8]{4,3,2,1,0} multiply(p0.50, broadcast.51)\n",
      "  p3.74 = f32[8,2,128,8]{2,3,0,1} parameter(3), sharding={replicated}\n",
      "  constant.73 = f32[] constant(1)\n",
      "  broadcast.75 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.73), dimensions={}\n",
      "  multiply.76 = f32[8,2,128,8]{3,2,1,0} multiply(p3.74, broadcast.75)\n",
      "  constant.72 = f32[] constant(1)\n",
      "  broadcast.77 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.72), dimensions={}\n",
      "  multiply.78 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.76, broadcast.77)\n",
      "  custom-call.79 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.78), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.80 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.79), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p2.65 = f32[8,2,128,8]{2,3,0,1} parameter(2), sharding={replicated}\n",
      "  constant.64 = f32[] constant(1)\n",
      "  broadcast.66 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.64), dimensions={}\n",
      "  multiply.67 = f32[8,2,128,8]{3,2,1,0} multiply(p2.65, broadcast.66)\n",
      "  constant.63 = f32[] constant(1)\n",
      "  broadcast.68 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.63), dimensions={}\n",
      "  multiply.69 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.67, broadcast.68)\n",
      "  custom-call.70 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.69), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.71 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.70), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p1.56 = f32[8,2,128,8]{2,3,0,1} parameter(1), sharding={replicated}\n",
      "  constant.55 = f32[] constant(1)\n",
      "  broadcast.57 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.55), dimensions={}\n",
      "  multiply.58 = f32[8,2,128,8]{3,2,1,0} multiply(p1.56, broadcast.57)\n",
      "  constant.54 = f32[] constant(1)\n",
      "  broadcast.59 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.54), dimensions={}\n",
      "  multiply.60 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.58, broadcast.59)\n",
      "  custom-call.61 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.60), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.62 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.61), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  custom-call.81 = (f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}) custom-call(custom-call.80, custom-call.71, custom-call.62), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"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\", \"cost_estimate\": {\"flops\": 1114112, \"transcendentals\": 32768, \"bytes_accessed\": 294912}, \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  get-tuple-element.82 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.81), index=0\n",
      "  custom-call.85 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.82), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.86 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.85), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.53 = f32[] constant(1)\n",
      "  broadcast.87 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.53), dimensions={}\n",
      "  multiply.88 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.86, broadcast.87)\n",
      "  get-tuple-element.83 = f32[2,1,128,128]{3,2,1,0} get-tuple-element(custom-call.81), index=1\n",
      "  custom-call.89 = f32[2,1,128,128]{3,2,1,0} custom-call(get-tuple-element.83), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.90 = f32[8,2,128,128]{3,2,1,0} custom-call(custom-call.89), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  reshape.91 = f32[262144]{0} reshape(custom-call.90)\n",
      "  slice.92 = f32[2048]{0} slice(reshape.91), slice={[0:2048]}\n",
      "  reshape.93 = f32[8,2,128]{2,1,0} reshape(slice.92)\n",
      "  get-tuple-element.84 = f32[2,1,128,128]{3,2,1,0} get-tuple-element(custom-call.81), index=2\n",
      "  custom-call.94 = f32[2,1,128,128]{3,2,1,0} custom-call(get-tuple-element.84), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.95 = f32[8,2,128,128]{3,2,1,0} custom-call(custom-call.94), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  reshape.96 = f32[262144]{0} reshape(custom-call.95)\n",
      "  slice.97 = f32[2048]{0} slice(reshape.96), slice={[0:2048]}\n",
      "  reshape.98 = f32[8,2,128]{2,1,0} reshape(slice.97)\n",
      "  ROOT tuple.99 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, /*index=5*/f32[8,2,128,8]{3,2,1,0}, f32[8,2,128]{2,1,0}, f32[8,2,128]{2,1,0}) tuple(multiply.52, multiply.88, custom-call.86, multiply.78, multiply.69, /*index=5*/multiply.60, reshape.93, reshape.98)\n",
      "} // FnComputation.48\n",
      "\n",
      "Body.100 {\n",
      "  p0.101 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.102 = s64[] get-tuple-element(p0.101), index=0\n",
      "  constant.219 = s64[] constant(1)\n",
      "  add.220 = s64[] add(get-tuple-element.102, constant.219)\n",
      "  get-tuple-element.103 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=1\n",
      "  get-tuple-element.106 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=4\n",
      "  constant.134 = s64[] constant(0)\n",
      "  broadcast.135 = s64[] broadcast(constant.134), dimensions={}\n",
      "  constant.136 = s64[] constant(0)\n",
      "  broadcast.137 = s64[] broadcast(constant.136), dimensions={}\n",
      "  constant.138 = s64[] constant(0)\n",
      "  broadcast.139 = s64[] broadcast(constant.138), dimensions={}\n",
      "  constant.140 = s64[] constant(0)\n",
      "  broadcast.141 = s64[] broadcast(constant.140), dimensions={}\n",
      "  dynamic-slice.142 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.106, get-tuple-element.102, broadcast.135, broadcast.137, broadcast.139, /*index=5*/broadcast.141), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.143 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.142)\n",
      "  get-tuple-element.105 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=3\n",
      "  constant.124 = s64[] constant(0)\n",
      "  broadcast.125 = s64[] broadcast(constant.124), dimensions={}\n",
      "  constant.126 = s64[] constant(0)\n",
      "  broadcast.127 = s64[] broadcast(constant.126), dimensions={}\n",
      "  constant.128 = s64[] constant(0)\n",
      "  broadcast.129 = s64[] broadcast(constant.128), dimensions={}\n",
      "  constant.130 = s64[] constant(0)\n",
      "  broadcast.131 = s64[] broadcast(constant.130), dimensions={}\n",
      "  dynamic-slice.132 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.105, get-tuple-element.102, broadcast.125, broadcast.127, broadcast.129, /*index=5*/broadcast.131), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.133 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.132)\n",
      "  get-tuple-element.104 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=2\n",
      "  constant.114 = s64[] constant(0)\n",
      "  broadcast.115 = s64[] broadcast(constant.114), dimensions={}\n",
      "  constant.116 = s64[] constant(0)\n",
      "  broadcast.117 = s64[] broadcast(constant.116), dimensions={}\n",
      "  constant.118 = s64[] constant(0)\n",
      "  broadcast.119 = s64[] broadcast(constant.118), dimensions={}\n",
      "  constant.120 = s64[] constant(0)\n",
      "  broadcast.121 = s64[] broadcast(constant.120), dimensions={}\n",
      "  dynamic-slice.122 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.104, get-tuple-element.102, broadcast.115, broadcast.117, broadcast.119, /*index=5*/broadcast.121), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.123 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.122)\n",
      "  call.144 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, /*index=5*/f32[8,2,128,8]{3,2,1,0}, f32[8,2,128]{2,1,0}, f32[8,2,128]{2,1,0}) call(get-tuple-element.103, reshape.143, reshape.133, reshape.123), to_apply=FnComputation.48\n",
      "  get-tuple-element.145 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.144), index=0\n",
      "  get-tuple-element.107 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=5\n",
      "  get-tuple-element.146 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.144), index=1\n",
      "  broadcast.147 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.146), dimensions={1,2,3,4}\n",
      "  constant.148 = s64[] constant(0)\n",
      "  broadcast.149 = s64[] broadcast(constant.148), dimensions={}\n",
      "  constant.150 = s64[] constant(0)\n",
      "  broadcast.151 = s64[] broadcast(constant.150), dimensions={}\n",
      "  constant.152 = s64[] constant(0)\n",
      "  broadcast.153 = s64[] broadcast(constant.152), dimensions={}\n",
      "  constant.154 = s64[] constant(0)\n",
      "  broadcast.155 = s64[] broadcast(constant.154), dimensions={}\n",
      "  dynamic-update-slice.156 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.107, broadcast.147, get-tuple-element.102, broadcast.149, broadcast.151, /*index=5*/broadcast.153, broadcast.155)\n",
      "  get-tuple-element.108 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=6\n",
      "  get-tuple-element.157 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.144), index=2\n",
      "  broadcast.158 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.157), dimensions={1,2,3,4}\n",
      "  constant.159 = s64[] constant(0)\n",
      "  broadcast.160 = s64[] broadcast(constant.159), dimensions={}\n",
      "  constant.161 = s64[] constant(0)\n",
      "  broadcast.162 = s64[] broadcast(constant.161), dimensions={}\n",
      "  constant.163 = s64[] constant(0)\n",
      "  broadcast.164 = s64[] broadcast(constant.163), dimensions={}\n",
      "  constant.165 = s64[] constant(0)\n",
      "  broadcast.166 = s64[] broadcast(constant.165), dimensions={}\n",
      "  dynamic-update-slice.167 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.108, broadcast.158, get-tuple-element.102, broadcast.160, broadcast.162, /*index=5*/broadcast.164, broadcast.166)\n",
      "  get-tuple-element.109 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=7\n",
      "  get-tuple-element.168 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.144), index=3\n",
      "  broadcast.169 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.168), dimensions={1,2,3,4}\n",
      "  constant.170 = s64[] constant(0)\n",
      "  broadcast.171 = s64[] broadcast(constant.170), dimensions={}\n",
      "  constant.172 = s64[] constant(0)\n",
      "  broadcast.173 = s64[] broadcast(constant.172), dimensions={}\n",
      "  constant.174 = s64[] constant(0)\n",
      "  broadcast.175 = s64[] broadcast(constant.174), dimensions={}\n",
      "  constant.176 = s64[] constant(0)\n",
      "  broadcast.177 = s64[] broadcast(constant.176), dimensions={}\n",
      "  dynamic-update-slice.178 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.109, broadcast.169, get-tuple-element.102, broadcast.171, broadcast.173, /*index=5*/broadcast.175, broadcast.177)\n",
      "  get-tuple-element.110 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=8\n",
      "  get-tuple-element.179 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.144), index=4\n",
      "  broadcast.180 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.179), dimensions={1,2,3,4}\n",
      "  constant.181 = s64[] constant(0)\n",
      "  broadcast.182 = s64[] broadcast(constant.181), dimensions={}\n",
      "  constant.183 = s64[] constant(0)\n",
      "  broadcast.184 = s64[] broadcast(constant.183), dimensions={}\n",
      "  constant.185 = s64[] constant(0)\n",
      "  broadcast.186 = s64[] broadcast(constant.185), dimensions={}\n",
      "  constant.187 = s64[] constant(0)\n",
      "  broadcast.188 = s64[] broadcast(constant.187), dimensions={}\n",
      "  dynamic-update-slice.189 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.110, broadcast.180, get-tuple-element.102, broadcast.182, broadcast.184, /*index=5*/broadcast.186, broadcast.188)\n",
      "  get-tuple-element.111 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.101), index=9\n",
      "  get-tuple-element.190 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.144), index=5\n",
      "  broadcast.191 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.190), dimensions={1,2,3,4}\n",
      "  constant.192 = s64[] constant(0)\n",
      "  broadcast.193 = s64[] broadcast(constant.192), dimensions={}\n",
      "  constant.194 = s64[] constant(0)\n",
      "  broadcast.195 = s64[] broadcast(constant.194), dimensions={}\n",
      "  constant.196 = s64[] constant(0)\n",
      "  broadcast.197 = s64[] broadcast(constant.196), dimensions={}\n",
      "  constant.198 = s64[] constant(0)\n",
      "  broadcast.199 = s64[] broadcast(constant.198), dimensions={}\n",
      "  dynamic-update-slice.200 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.111, broadcast.191, get-tuple-element.102, broadcast.193, broadcast.195, /*index=5*/broadcast.197, broadcast.199)\n",
      "  get-tuple-element.112 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.101), index=10\n",
      "  get-tuple-element.201 = f32[8,2,128]{2,1,0} get-tuple-element(call.144), index=6\n",
      "  broadcast.202 = f32[1,8,2,128]{3,2,1,0} broadcast(get-tuple-element.201), dimensions={1,2,3}\n",
      "  constant.203 = s64[] constant(0)\n",
      "  broadcast.204 = s64[] broadcast(constant.203), dimensions={}\n",
      "  constant.205 = s64[] constant(0)\n",
      "  broadcast.206 = s64[] broadcast(constant.205), dimensions={}\n",
      "  constant.207 = s64[] constant(0)\n",
      "  broadcast.208 = s64[] broadcast(constant.207), dimensions={}\n",
      "  dynamic-update-slice.209 = f32[2,8,2,128]{3,2,1,0} dynamic-update-slice(get-tuple-element.112, broadcast.202, get-tuple-element.102, broadcast.204, broadcast.206, /*index=5*/broadcast.208)\n",
      "  get-tuple-element.113 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.101), index=11\n",
      "  get-tuple-element.210 = f32[8,2,128]{2,1,0} get-tuple-element(call.144), index=7\n",
      "  broadcast.211 = f32[1,8,2,128]{3,2,1,0} broadcast(get-tuple-element.210), dimensions={1,2,3}\n",
      "  constant.212 = s64[] constant(0)\n",
      "  broadcast.213 = s64[] broadcast(constant.212), dimensions={}\n",
      "  constant.214 = s64[] constant(0)\n",
      "  broadcast.215 = s64[] broadcast(constant.214), dimensions={}\n",
      "  constant.216 = s64[] constant(0)\n",
      "  broadcast.217 = s64[] broadcast(constant.216), dimensions={}\n",
      "  dynamic-update-slice.218 = f32[2,8,2,128]{3,2,1,0} dynamic-update-slice(get-tuple-element.113, broadcast.211, get-tuple-element.102, broadcast.213, broadcast.215, /*index=5*/broadcast.217)\n",
      "  ROOT tuple.221 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) tuple(add.220, get-tuple-element.145, get-tuple-element.104, get-tuple-element.105, get-tuple-element.106, /*index=5*/dynamic-update-slice.156, dynamic-update-slice.167, dynamic-update-slice.178, dynamic-update-slice.189, dynamic-update-slice.200, /*index=10*/dynamic-update-slice.209, dynamic-update-slice.218)\n",
      "} // Body.100\n",
      "\n",
      "Condition.222 {\n",
      "  p0.223 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.224 = s64[] get-tuple-element(p0.223), index=0\n",
      "  constant.236 = s64[] constant(2)\n",
      "  ROOT compare.237 = pred[] compare(get-tuple-element.224, constant.236), direction=LT\n",
      "}\n",
      "\n",
      "scan.238 {\n",
      "  p0.239 = s64[] parameter(0)\n",
      "  p1.240 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(1)\n",
      "  p2.241 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(2)\n",
      "  p3.242 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(3)\n",
      "  p4.243 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(4)\n",
      "  p5.244 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(5)\n",
      "  p6.245 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(6)\n",
      "  p7.246 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(7)\n",
      "  p8.247 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(8)\n",
      "  p9.248 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(9)\n",
      "  p10.249 = f32[2,8,2,128]{3,2,1,0} parameter(10)\n",
      "  p11.250 = f32[2,8,2,128]{3,2,1,0} parameter(11)\n",
      "  tuple.251 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) tuple(p0.239, p1.240, p2.241, p3.242, p4.243, /*index=5*/p5.244, p6.245, p7.246, p8.247, p9.248, /*index=10*/p10.249, p11.250)\n",
      "  ROOT while.252 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) while(tuple.251), condition=Condition.222, body=Body.100\n",
      "} // scan.238\n",
      "\n",
      "AddComputation.295 {\n",
      "  x.296 = f32[] parameter(0)\n",
      "  y.297 = f32[] parameter(1)\n",
      "  ROOT add.298 = f32[] add(x.296, y.297)\n",
      "}\n",
      "\n",
      "FnComputation.299 {\n",
      "  p0.301 = f32[2,8,2,128,8]{3,4,1,2,0} parameter(0), sharding={replicated}\n",
      "  constant.300 = f32[] constant(1)\n",
      "  broadcast.302 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(constant.300), dimensions={}\n",
      "  multiply.303 = f32[2,8,2,128,8]{4,3,2,1,0} multiply(p0.301, broadcast.302)\n",
      "  p7.361 = f32[8,2,128,8]{2,3,0,1} parameter(7), sharding={replicated}\n",
      "  constant.360 = f32[] constant(1)\n",
      "  broadcast.362 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.360), dimensions={}\n",
      "  multiply.363 = f32[8,2,128,8]{3,2,1,0} multiply(p7.361, broadcast.362)\n",
      "  custom-call.364 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.363), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.365 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.364), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p6.355 = f32[8,2,128,8]{2,3,0,1} parameter(6), sharding={replicated}\n",
      "  constant.354 = f32[] constant(1)\n",
      "  broadcast.356 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.354), dimensions={}\n",
      "  multiply.357 = f32[8,2,128,8]{3,2,1,0} multiply(p6.355, broadcast.356)\n",
      "  custom-call.358 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.357), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.359 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.358), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p5.349 = f32[8,2,128,8]{2,3,0,1} parameter(5), sharding={replicated}\n",
      "  constant.348 = f32[] constant(1)\n",
      "  broadcast.350 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.348), dimensions={}\n",
      "  multiply.351 = f32[8,2,128,8]{3,2,1,0} multiply(p5.349, broadcast.350)\n",
      "  custom-call.352 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.351), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.353 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.352), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p4.339 = f32[8,2,128]{2,0,1} parameter(4), sharding={replicated}\n",
      "  constant.338 = f32[] constant(1)\n",
      "  broadcast.340 = f32[8,2,128]{2,1,0} broadcast(constant.338), dimensions={}\n",
      "  multiply.341 = f32[8,2,128]{2,1,0} multiply(p4.339, broadcast.340)\n",
      "  reshape.342 = f32[8,2,128,1]{3,2,1,0} reshape(multiply.341)\n",
      "  broadcast.343 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.342), dimensions={0,1,2,3}\n",
      "  reshape.344 = f32[8,2,128]{2,1,0} reshape(broadcast.343)\n",
      "  broadcast.345 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.344), dimensions={0,1,2}\n",
      "  custom-call.346 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.345), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.347 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.346), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p3.329 = f32[8,2,128]{2,0,1} parameter(3), sharding={replicated}\n",
      "  constant.328 = f32[] constant(1)\n",
      "  broadcast.330 = f32[8,2,128]{2,1,0} broadcast(constant.328), dimensions={}\n",
      "  multiply.331 = f32[8,2,128]{2,1,0} multiply(p3.329, broadcast.330)\n",
      "  reshape.332 = f32[8,2,128,1]{3,2,1,0} reshape(multiply.331)\n",
      "  broadcast.333 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.332), dimensions={0,1,2,3}\n",
      "  reshape.334 = f32[8,2,128]{2,1,0} reshape(broadcast.333)\n",
      "  broadcast.335 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.334), dimensions={0,1,2}\n",
      "  custom-call.336 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.335), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.337 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.336), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p1.307 = f32[8,2,128,8]{2,3,0,1} parameter(1), sharding={replicated}\n",
      "  constant.306 = f32[] constant(1)\n",
      "  broadcast.308 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.306), dimensions={}\n",
      "  multiply.309 = f32[8,2,128,8]{3,2,1,0} multiply(p1.307, broadcast.308)\n",
      "  constant.305 = f32[] constant(1)\n",
      "  broadcast.310 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.305), dimensions={}\n",
      "  multiply.311 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.309, broadcast.310)\n",
      "  custom-call.326 = f32[8,2,128,8]{3,2,1,0} custom-call(multiply.311), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.327 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.326), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  p2.313 = f32[8,2,128,8]{2,3,0,1} parameter(2), sharding={replicated}\n",
      "  constant.312 = f32[] constant(1)\n",
      "  broadcast.314 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.312), dimensions={}\n",
      "  multiply.315 = f32[8,2,128,8]{3,2,1,0} multiply(p2.313, broadcast.314)\n",
      "  multiply.316 = f32[8,2,128,8]{3,2,1,0} multiply(multiply.315, multiply.311)\n",
      "  constant.317 = f32[] constant(0)\n",
      "  reduce.319 = f32[8,2,128]{2,1,0} reduce(multiply.316, constant.317), dimensions={3}, to_apply=AddComputation.295\n",
      "  reshape.320 = f32[8,2,128,1]{3,2,1,0} reshape(reduce.319)\n",
      "  broadcast.321 = f32[8,2,128,1]{3,2,1,0} broadcast(reshape.320), dimensions={0,1,2,3}\n",
      "  reshape.322 = f32[8,2,128]{2,1,0} reshape(broadcast.321)\n",
      "  broadcast.323 = f32[8,2,128,128]{3,2,1,0} broadcast(reshape.322), dimensions={0,1,2}\n",
      "  custom-call.324 = f32[8,2,128,128]{3,2,1,0} custom-call(broadcast.323), custom_call_target=\"Sharding\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  custom-call.325 = f32[2,1,128,128]{3,2,1,0} custom-call(custom-call.324), custom_call_target=\"SPMDFullToShardShape\", sharding={manual}\n",
      "  custom-call.366 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.365, custom-call.359, custom-call.353, custom-call.347, custom-call.337, /*index=5*/custom-call.327, custom-call.325), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"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\", \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  custom-call.367 = f32[2,1,128,8]{3,2,1,0} custom-call(custom-call.366), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.368 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.367), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.304 = f32[] constant(1)\n",
      "  broadcast.369 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.304), dimensions={}\n",
      "  multiply.370 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.368, broadcast.369)\n",
      "  custom-call.372 = (f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}) custom-call(custom-call.365, custom-call.359, custom-call.353, custom-call.347, custom-call.337, /*index=5*/custom-call.327, custom-call.325), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}, f32[2,1,128,8]{3,2,1,0}, f32[2,1,128,128]{3,2,1,0}}, backend_config={\"custom_call_config\": {\"body\": \"TUzvUgFNTElSMjAuMC4wZ2l0AAE/CQEDBQcBAwkDKwsNDxETFRcZGx0fISMlJykrLS8xMwNCBcIEHQH5BxcLFxMLCwsLCwsLCw8PIwuTExMTExMTExcPDxMLCw8THxMTCw8PkxcjDwsXDxMPDw/FDwsLCwsLCwsLCxMLCxMLCwsXDwsXEwsbDxMXEy8LCwsLEwsLExsLExcPLxMLCwsLExMTDxMTEwsTCx8TFw8LEwsPExsLD2cLHwUHjWF5BwdZWVkBrgILhQuTCwsPCwtTHwsfCx8LHwsfCx8LHwsfCx8LGxsbGxsbGxsbDxMTFxcLFxcXCxcXFwsXFwsXCw8TFw8XCxcTExcTExMXCxMXExMXDxMTFxMTFxMTFxMTFxMTFxMTFxMTFx8TExcLCw8TFwsPExcLExMXFx8LExMXDxMXDwtTDxMXHxMXDxMTF1MTExcPExcPDw8TFxMXDxMTFxMTFw8TFxMXUxMXDxMXExcFB5EqAioCAR0PBxsfKwcPIy8LJydLLwJuHB8DAxk6AwU1AwMZngIVJgNRBTcFOQU7BT0FPw0bBUEFQx1J1x1J3QMFtgMGAqv/BUUjDQlBAQAAAAAAAAABAAAAAAAAAIAAAAAAAAAACAAAAAAAAAAdFz4DHRdKAx0XVgMdF2IDHRduAx0XegMdF4YDFa4CugIdhzkdh0cV7gIzBUcFSRETAB3BIgQDBcMuBMXHHcFqBBWGBDMFSx1J5R1J6yMNCUEBAAAAAAAAAAEAAAAAAAAAgAAAAAAAAACAAAAAAAAAABX6AgYDAwWiA1WmA1URAQUFTR3eA+IDHRe9FUIECR0Xzx3V1x3V3WFmZmluZV9tYXA8KGQwLCBkMSwgZDIsIGQzKSAtPiAoZDAsIGQxLCBkMiwgZDMpPgARDQEFTwVRBVMFVQVXBVkFWwVdBV8dfaICBWEFYwMDg2cFZQVnBWkDA4P2Ah2NUQVrAwMZEgMdOxYDBW0DAyIDAgIdjQkdOy4DAwMZkgMdPZYDAwmhugSj/aU/pz8FbwVxBXMFdRWuAwkFdwV5FfIDCQMD/gMCBAV7FQoECQMDGRIEHT21AwmhvgSj/aU/pz8VFgQJBX0FfwWBBYMjAQEBHT0yBBVWBAkdPcsVXgQJAwMZVR19dgQFhRWOBEcFhwMFw5YExccVmgRHAwMZogQd4+UFiRWmBDkFix3j6xWuBDkDBe/xDSEFjRENDQMP9fcPCgISAhYCGgJnHgIiAg0mAioCLgIFjwEJ+fn5tgQjdHB1LmRpbWVuc2lvbl9zZW1hbnRpY3M8cGFyYWxsZWw+ACN0cHUubWVtb3J5X3NwYWNlPHZtZW0+ACN0cHUuY29udHJhY3RfcHJlY2lzaW9uPGZwMzI+ACNhcml0aC5mYXN0bWF0aDxub25lPgAjYXJpdGgub3ZlcmZsb3c8bm9uZT4AI2FyaXRoLmRlbm9ybWFsPGllZWU+AA0ZYWZmaW5lX21hcDwoZDAsIGQxKSAtPiAoZDAsIGQxKT4ABZEjDQlBAgAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAAFkwWVEQ0JBZcFmQETMgI6AkICSgJSAloCYgJqAnICAwURNgITIwlpAwURPgITIwlrAwURRgITIwltAwURTgITTwlvAwURVgITTwlxAwURXgITIwlzAwURZgITTwl1AwURbgITIwl3AwURdgITIwl5AwUPFQ1pAwUPFQ1rAwUPFQ1tAwUPFQ1vAwUPFQ1xAwUPFQ1zAwUPFQ11AwUPFQ13AwUPFQ15EQEBFaYCMx0hqgIXBQoNAR2yArYCBZsXBcIRARW+AsoCHcICxgIFnRd/fgIBFc4C2gId0gLWAgWfF38yCAEd3gLiAgWhF+YCFQEFox2FOR0h8gIXBQ4NARENBR3+AgIDBaUXBdIOARUKAzMdIQ4DFwXODgERAQIEFRoDUR2THgMXBSINAQWnHZMqAxcFpg4BFTIDCR0LNgMXBSoNAREDARVCAwkdC0YDFwUuDQEVTgMJHQtSAxcFMg0BFVoDCR0LXgMXBTYNARVmAwkdC2oDFwU+DQEVcgMJHQt2AxcFRg0BFX4DCR0LggMXBU4NARWKAwkdC44DFwVWDQElBwkAAAAAFZoDCR0LngMXBWINAQWpBasdV6kdC7IDFwU6DgEFrR2tqQMDq/8dxgPKAwWvFc4DCR0L0gMXBTYOAQMDGdoDEwsQAADgDwWxFeYDCR0L6gMXBUYOAR1Xrx0L9gMXBUIOAR07rwWzIw0FIQEAAAAAAAAAAAAAAAAAAAAds7UdCw4EFwVODgElBQkAAAAAHQsaBBcFVg4BHb+9FSYECR0LKgQXBVIOASMBCSEBAAAAAQAAAAIAAAAAAAAAFTYECR0LOgQXBW4OAR1XXR0LRgQXBXoOAR2tXR07XR2zyx0LWgQXBZYOAR0LYgQXBZ4OAR2/zxVuBAkdC3IEFwWaDgEVegQzHSF+BBcF2g4BHYVHHSGKBBcF3g4BHdmSBBcF4g4BIwEJIQEAAAABAAAABAAAAAAAAAAd2Z4EFwXmDgETCwEd56oEFwUSDQEd57IEFwUWDQEjdHB1LmRpbWVuc2lvbl9zZW1hbnRpY3M8YXJiaXRyYXJ5PgAjdHB1LmRvdF9kaW1lbnNpb25fbnVtYmVyczxbMV0sIFsxXSwgWzBdLCBbMF0sIFswLCAwLCAxLCAwXSwgW10sIFtdPgAjdHB1LmRvdF9kaW1lbnNpb25fbnVtYmVyczxbMV0sIFswXSwgWzBdLCBbMV0sIFswLCAwLCAxLCAxXSwgW10sIFtdPgABAgIDJwUCBCELJwUCBAIECxf7CQUFAgQhC2ULAQIEJwkFBQIEIQsX+wkFBQIEAgQLZQEJF/sFAgQhCw4CJwkFBQIEAgQLBR8BAQEBCQkJEREJEQkJFRUBBQkBAQEBCQEBAQEEKhYFAREB7QcDASkLEQHzBwPdJgMfAQEBAQEBAQEJAQkBCQERAREBCQERAQkBCQEVARUBAwN7BwMBEQd7gQMTBQcfGwbqAgMBAyEDAzUHAwERBzWJAxMFIyUdFDUDJwkDFTUDA+HfAwsZBuEDBQPdAwNLAwMDAwNLAwMDBQZLAwUHG+HjDwVLQwnfG+HjAwPp3wMLGQbpAwUD5wMDTQMDAwMDTQMDAwUGTQMFBx3r7Q8FTUMJ6R3r7RUANQMBBRUANQMDiwcDAQMDkY8DAR8HkZUDAQUpKwMDlwcDAQMDmY8DAR8HmZUDAQUvMQMDJQMDAwMDJQMDAwcGJQMDAzMDAyUDAwMFBiUDDwsLNTc5OwkGJQMFAz0DAycDAwMDAycDAwMHBicDAwMzAwMnAwMDBQYnAw8LDUFDRUcJBicDBQNJAwMpAwMDAwMpAwMDBwYpAwMDLQMDKQMDAwUGKQMPCwlNT1FTCQYpAwUDVQMDKwMDAwMDKwMDAwcGKwMDAy0DAysDAwMFBisDFwsPWVtdXwkGKwMHA2EDAy0DAwMDAy0DAwMHBi0DAwMtAwMtAwMDBQYtAxcLEWVnaWsJBi0DBwNtAwMvAwMDAwMvAwMDBwYvAwMDLQMDLwMDAwUGLwMPCxNxc3V3CQYvAwUDeQMDMQMDAwMDMQMDAwcGMQMDAy0DAzEDAwMFBjEDFwsVfX+BgwkGMQMHA4UDA52bAwcTB52fAwcHVz+JFweqA1MDBwNvIQe6Ax8DBwWLjSkHwgO+AwMHA48DA1nWAwMLGQZZAwcDkysHWR8DBwWVYxcH7gNTAwcDlyMH+gMfAwcFkZklBwYEsQMHA5sDA7m3AwUTB7m7AwUHnXufBwZbAwMDMwMDWwMDAwUGWwMFBx2jpScHHgQfAwUFp6EHBkEDAwMzAwNBAwMDBQZBAwUHHautDwVBQwmpHautAwPJmwMHEwfJnwMHB3tLsRcHPgRTAwcDhyEHSgQfAwcFs7UjB04EHwMHBbebJQdSBLEDBwO5AwPNtwMFEwfNuwMFB7tXvQcGXwMDAzMDA18DAwMFBl8DBQcbwcMnB2YEHwMFBcW/BwZFAwMDMwMDRQMDAwUGRQMFBxvJyw8FRUMJxxvJywMDl9EDAQMDi9EDAQMD0wcDAREH04EDEwUH0xsGggQDAQPVAwM3BwMBEQc3iQMTBdfZHRQ3A9sJAyldAwNhAwMDAwNhAwMDBQZhAwUHHd3fAwMbAwMDAwMbAwMDAwMbAwMDAwMbAwMDBQYbAw8LGePl5+kJBhsDBQPrCQYbAw8D4Q8FG9sN7xnj5efpAwNjAwMDAwNjAwMDBQZjAwUHG/HzAwMdAwMDAwMdAwMDAwMdAwMDAwMdAwMDBQYdAw8LF/f5+/0JBh0DBQP/CQYdAw8D9Q8FHdsNBgIX9/n7/RUANwMBBRUANw0AAQsRAXoCBwMNDwkBAQEBAQEBAQMDAQcDAQMDAQcDAQ0EAQkBAwcJCxEBfgIHAw0PCQEBAQEBAQEBAwMBBwMBAwMBBwMBDQQBCQEDBQkLEQGCAgcDDQ8JAQEBAQEBAQEDAwEHAwEDAwEHAwENBAEJAQMFCQsRAYYCBwMNDwkBAQEBAQEBAQMDAQcDAQMDAQcDAQ0EAQkBAwcJCxEBigIHAw0PCQEBAQEBAQEBAwMBBwMBAwMBBwMBDQQBCQEDBwkLEQGOAgcDDQ8JAQEBAQEBAQEDAwEHAwEDAwEHAwENBAEJAQMHCQsRAZICBwMNDwkBAQEBAQEBAQMDAQcDAQMDAQcDAQ0EAQkBAwcJCxEBlgIHAw0PCQEBAQEBAQEBAwMBBwMBAwMBBwMBDQQBCQEDBQkLEQGaAgcDDQ8JAQEBAQEBAQEDAwEHAwEDAwEHAwENBAEJAQMFCQYDAQUBAM4TtRkLCxMNFR0JJREnGzMdCyMhIyktJyUlCxEpGwsXCxMdHRUlDw0NLRVHCRkZGRkZGRkZGRENGws5DRsdJR0TD7cXExcjFxcXDxkjFxUXFyMZFSUjGR8PDQkdEWJ1aWx0aW4Ac3RhYmxlX21vc2FpYwB0cHUAYXJpdGgAbW9kdWxlAGFyaXRoLmNvbnN0YW50AHZlY3Rvci5sb2FkAGFyaXRoLmluZGV4X2Nhc3QAdmVjdG9yLnNoYXBlX2Nhc3QAZnVuYy5mdW5jAGZ1bmMucmV0dXJuAHRwdS52ZWN0b3Jfc3RvcmUAYXJpdGguY21waQB0cHUubWF0bXVsAHNjZi55aWVsZAB0cHUucmVwZWF0AHZlY3Rvci5icm9hZGNhc3QAYXJpdGguZXh0dWkAc2NmLmlmAGFyaXRoLm11bGkAYXJpdGguc3ViZgBhcml0aC5tdWxmAHZlY3Rvci50cmFuc3Bvc2UAYXJpdGguYWRkZgBtYXRoLmV4cABhcml0aC5kaXZmAC91c3IvbG9jYWwvbGliL3B5dGhvbjMuMTAvc2l0ZS1wYWNrYWdlcy9qYXgvZXhwZXJpbWVudGFsL3BhbGxhcy9vcHMvdHB1L2ZsYXNoX2F0dGVudGlvbi5weQBrX2JvZHkAc3ltX25hbWUAZnVuY3Rpb25fdHlwZQB0cmFuc2Zvcm1faW5kaWNlcwB3aW5kb3dfYm91bmRzAC9tYXNrZWRfbG9hZAB2YWx1ZQBfZmxhc2hfYXR0ZW50aW9uX2Rrdl9rZXJuZWwAL211bAAvZG90X2dlbmVyYWwAL3N3YXAAL3JlcGVhdAB0cmFuc2Zvcm1fMAB0cmFuc2Zvcm1fMQB0cmFuc2Zvcm1fMgB0cmFuc2Zvcm1fMwB0cmFuc2Zvcm1fNAB0cmFuc2Zvcm1fNQB0cmFuc2Zvcm1fNgB0cmFuc2Zvcm1fNwB0cmFuc2Zvcm1fOAAvZXEAL3RtcC9pcHlrZXJuZWxfMjE5MDgyNy84MDYwODI4MS5weQBwcmVkaWNhdGUAL2NvbnZlcnRfZWxlbWVudF90eXBlAC9jb25kAC9zY2FuAHFfYm9keQBkaW1lbnNpb25fbnVtYmVycwBwcmVjaXNpb24AdHJhbnNwb3NlX2xocwB0cmFuc3Bvc2VfcmhzAGZhc3RtYXRoAC9zdWIAL3RyYW5zcG9zZQAvYWRkAC9tYXNrZWRfc3dhcABvcGVyYW5kU2VnbWVudFNpemVzAHN0cmlkZXMAL2dldABlbmRfb2ZfcV9zZXF1ZW5jZQAvYnJvYWRjYXN0X2luX2RpbQBzdGFydF9uZXdfc2VxdWVuY2UAc3RhYmxlX21vc2FpYy52ZXJzaW9uAGRpbWVuc2lvbl9zZW1hbnRpY3MAaXRlcmF0aW9uX2JvdW5kcwBzY2FsYXJfcHJlZmV0Y2gAc2NyYXRjaF9vcGVyYW5kcwBtYWluAHdpbmRvd19wYXJhbXMAX2ZsYXNoX2F0dGVudGlvbl9id2RfZGt2AHRyYWNlX3BhbGxhcwBmYV9jdXN0b21fYmFja3dhcmQAZm9yd2FyZAA8ZXZhbF93aXRoX2tleT4uNwBydW4Ab3ZlcmZsb3dGbGFncwBkaW1lbnNpb24AdGltZXMAZGVub3JtYWwAL2V4cAAvZGl2AHBlcm11dGF0aW9uAA==\", \"serialization_format\": 1, \"needs_layout_passes\": true}}\n",
      "  get-tuple-element.373 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.372), index=0\n",
      "  custom-call.375 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.373), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.376 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.375), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.371 = f32[] constant(1)\n",
      "  broadcast.377 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.371), dimensions={}\n",
      "  multiply.378 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.376, broadcast.377)\n",
      "  get-tuple-element.374 = f32[2,1,128,8]{3,2,1,0} get-tuple-element(custom-call.372), index=1\n",
      "  custom-call.380 = f32[2,1,128,8]{3,2,1,0} custom-call(get-tuple-element.374), custom_call_target=\"Sharding\", sharding={manual}\n",
      "  custom-call.381 = f32[8,2,128,8]{3,2,1,0} custom-call(custom-call.380), custom_call_target=\"SPMDShardToFullShape\", sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.379 = f32[] constant(1)\n",
      "  broadcast.382 = f32[8,2,128,8]{3,2,1,0} broadcast(constant.379), dimensions={}\n",
      "  multiply.383 = f32[8,2,128,8]{3,2,1,0} multiply(custom-call.381, broadcast.382)\n",
      "  ROOT tuple.384 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}) tuple(multiply.303, multiply.370, multiply.378, multiply.383)\n",
      "} // FnComputation.299\n",
      "\n",
      "Body.385 {\n",
      "  p0.386 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.387 = s64[] get-tuple-element(p0.386), index=0\n",
      "  constant.502 = s64[] constant(1)\n",
      "  add.503 = s64[] add(get-tuple-element.387, constant.502)\n",
      "  get-tuple-element.388 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=1\n",
      "  get-tuple-element.389 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=2\n",
      "  constant.399 = s64[] constant(1)\n",
      "  subtract.400 = s64[] subtract(constant.399, get-tuple-element.387)\n",
      "  constant.401 = s64[] constant(0)\n",
      "  broadcast.402 = s64[] broadcast(constant.401), dimensions={}\n",
      "  constant.403 = s64[] constant(0)\n",
      "  broadcast.404 = s64[] broadcast(constant.403), dimensions={}\n",
      "  constant.405 = s64[] constant(0)\n",
      "  broadcast.406 = s64[] broadcast(constant.405), dimensions={}\n",
      "  constant.407 = s64[] constant(0)\n",
      "  broadcast.408 = s64[] broadcast(constant.407), dimensions={}\n",
      "  dynamic-slice.409 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.389, subtract.400, broadcast.402, broadcast.404, broadcast.406, /*index=5*/broadcast.408), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.410 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.409)\n",
      "  get-tuple-element.390 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=3\n",
      "  constant.411 = s64[] constant(0)\n",
      "  broadcast.412 = s64[] broadcast(constant.411), dimensions={}\n",
      "  constant.413 = s64[] constant(0)\n",
      "  broadcast.414 = s64[] broadcast(constant.413), dimensions={}\n",
      "  constant.415 = s64[] constant(0)\n",
      "  broadcast.416 = s64[] broadcast(constant.415), dimensions={}\n",
      "  constant.417 = s64[] constant(0)\n",
      "  broadcast.418 = s64[] broadcast(constant.417), dimensions={}\n",
      "  dynamic-slice.419 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.390, subtract.400, broadcast.412, broadcast.414, broadcast.416, /*index=5*/broadcast.418), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.420 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.419)\n",
      "  get-tuple-element.395 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.386), index=8\n",
      "  constant.459 = s64[] constant(0)\n",
      "  broadcast.460 = s64[] broadcast(constant.459), dimensions={}\n",
      "  constant.461 = s64[] constant(0)\n",
      "  broadcast.462 = s64[] broadcast(constant.461), dimensions={}\n",
      "  constant.463 = s64[] constant(0)\n",
      "  broadcast.464 = s64[] broadcast(constant.463), dimensions={}\n",
      "  dynamic-slice.465 = f32[1,8,2,128]{3,2,1,0} dynamic-slice(get-tuple-element.395, subtract.400, broadcast.460, broadcast.462, broadcast.464), dynamic_slice_sizes={1,8,2,128}\n",
      "  reshape.466 = f32[8,2,128]{2,1,0} reshape(dynamic-slice.465)\n",
      "  get-tuple-element.394 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(p0.386), index=7\n",
      "  constant.451 = s64[] constant(0)\n",
      "  broadcast.452 = s64[] broadcast(constant.451), dimensions={}\n",
      "  constant.453 = s64[] constant(0)\n",
      "  broadcast.454 = s64[] broadcast(constant.453), dimensions={}\n",
      "  constant.455 = s64[] constant(0)\n",
      "  broadcast.456 = s64[] broadcast(constant.455), dimensions={}\n",
      "  dynamic-slice.457 = f32[1,8,2,128]{3,2,1,0} dynamic-slice(get-tuple-element.394, subtract.400, broadcast.452, broadcast.454, broadcast.456), dynamic_slice_sizes={1,8,2,128}\n",
      "  reshape.458 = f32[8,2,128]{2,1,0} reshape(dynamic-slice.457)\n",
      "  get-tuple-element.393 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=6\n",
      "  constant.441 = s64[] constant(0)\n",
      "  broadcast.442 = s64[] broadcast(constant.441), dimensions={}\n",
      "  constant.443 = s64[] constant(0)\n",
      "  broadcast.444 = s64[] broadcast(constant.443), dimensions={}\n",
      "  constant.445 = s64[] constant(0)\n",
      "  broadcast.446 = s64[] broadcast(constant.445), dimensions={}\n",
      "  constant.447 = s64[] constant(0)\n",
      "  broadcast.448 = s64[] broadcast(constant.447), dimensions={}\n",
      "  dynamic-slice.449 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.393, subtract.400, broadcast.442, broadcast.444, broadcast.446, /*index=5*/broadcast.448), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.450 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.449)\n",
      "  get-tuple-element.392 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=5\n",
      "  constant.431 = s64[] constant(0)\n",
      "  broadcast.432 = s64[] broadcast(constant.431), dimensions={}\n",
      "  constant.433 = s64[] constant(0)\n",
      "  broadcast.434 = s64[] broadcast(constant.433), dimensions={}\n",
      "  constant.435 = s64[] constant(0)\n",
      "  broadcast.436 = s64[] broadcast(constant.435), dimensions={}\n",
      "  constant.437 = s64[] constant(0)\n",
      "  broadcast.438 = s64[] broadcast(constant.437), dimensions={}\n",
      "  dynamic-slice.439 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.392, subtract.400, broadcast.432, broadcast.434, broadcast.436, /*index=5*/broadcast.438), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.440 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.439)\n",
      "  get-tuple-element.391 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=4\n",
      "  constant.421 = s64[] constant(0)\n",
      "  broadcast.422 = s64[] broadcast(constant.421), dimensions={}\n",
      "  constant.423 = s64[] constant(0)\n",
      "  broadcast.424 = s64[] broadcast(constant.423), dimensions={}\n",
      "  constant.425 = s64[] constant(0)\n",
      "  broadcast.426 = s64[] broadcast(constant.425), dimensions={}\n",
      "  constant.427 = s64[] constant(0)\n",
      "  broadcast.428 = s64[] broadcast(constant.427), dimensions={}\n",
      "  dynamic-slice.429 = f32[1,8,2,128,8]{4,3,2,1,0} dynamic-slice(get-tuple-element.391, subtract.400, broadcast.422, broadcast.424, broadcast.426, /*index=5*/broadcast.428), dynamic_slice_sizes={1,8,2,128,8}\n",
      "  reshape.430 = f32[8,2,128,8]{3,2,1,0} reshape(dynamic-slice.429)\n",
      "  call.467 = (f32[2,8,2,128,8]{4,3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}, f32[8,2,128,8]{3,2,1,0}) call(get-tuple-element.388, reshape.410, reshape.420, reshape.466, reshape.458, /*index=5*/reshape.450, reshape.440, reshape.430), to_apply=FnComputation.299\n",
      "  get-tuple-element.468 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.467), index=0\n",
      "  get-tuple-element.396 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=9\n",
      "  get-tuple-element.469 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.467), index=1\n",
      "  broadcast.470 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.469), dimensions={1,2,3,4}\n",
      "  constant.471 = s64[] constant(0)\n",
      "  broadcast.472 = s64[] broadcast(constant.471), dimensions={}\n",
      "  constant.473 = s64[] constant(0)\n",
      "  broadcast.474 = s64[] broadcast(constant.473), dimensions={}\n",
      "  constant.475 = s64[] constant(0)\n",
      "  broadcast.476 = s64[] broadcast(constant.475), dimensions={}\n",
      "  constant.477 = s64[] constant(0)\n",
      "  broadcast.478 = s64[] broadcast(constant.477), dimensions={}\n",
      "  dynamic-update-slice.479 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.396, broadcast.470, subtract.400, broadcast.472, broadcast.474, /*index=5*/broadcast.476, broadcast.478)\n",
      "  get-tuple-element.397 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=10\n",
      "  get-tuple-element.480 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.467), index=2\n",
      "  broadcast.481 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.480), dimensions={1,2,3,4}\n",
      "  constant.482 = s64[] constant(0)\n",
      "  broadcast.483 = s64[] broadcast(constant.482), dimensions={}\n",
      "  constant.484 = s64[] constant(0)\n",
      "  broadcast.485 = s64[] broadcast(constant.484), dimensions={}\n",
      "  constant.486 = s64[] constant(0)\n",
      "  broadcast.487 = s64[] broadcast(constant.486), dimensions={}\n",
      "  constant.488 = s64[] constant(0)\n",
      "  broadcast.489 = s64[] broadcast(constant.488), dimensions={}\n",
      "  dynamic-update-slice.490 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.397, broadcast.481, subtract.400, broadcast.483, broadcast.485, /*index=5*/broadcast.487, broadcast.489)\n",
      "  get-tuple-element.398 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(p0.386), index=11\n",
      "  get-tuple-element.491 = f32[8,2,128,8]{3,2,1,0} get-tuple-element(call.467), index=3\n",
      "  broadcast.492 = f32[1,8,2,128,8]{4,3,2,1,0} broadcast(get-tuple-element.491), dimensions={1,2,3,4}\n",
      "  constant.493 = s64[] constant(0)\n",
      "  broadcast.494 = s64[] broadcast(constant.493), dimensions={}\n",
      "  constant.495 = s64[] constant(0)\n",
      "  broadcast.496 = s64[] broadcast(constant.495), dimensions={}\n",
      "  constant.497 = s64[] constant(0)\n",
      "  broadcast.498 = s64[] broadcast(constant.497), dimensions={}\n",
      "  constant.499 = s64[] constant(0)\n",
      "  broadcast.500 = s64[] broadcast(constant.499), dimensions={}\n",
      "  dynamic-update-slice.501 = f32[2,8,2,128,8]{4,3,2,1,0} dynamic-update-slice(get-tuple-element.398, broadcast.492, subtract.400, broadcast.494, broadcast.496, /*index=5*/broadcast.498, broadcast.500)\n",
      "  ROOT tuple.504 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) tuple(add.503, get-tuple-element.468, get-tuple-element.389, get-tuple-element.390, get-tuple-element.391, /*index=5*/get-tuple-element.392, get-tuple-element.393, get-tuple-element.394, get-tuple-element.395, dynamic-update-slice.479, /*index=10*/dynamic-update-slice.490, dynamic-update-slice.501)\n",
      "} // Body.385\n",
      "\n",
      "Condition.505 {\n",
      "  p0.506 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) parameter(0)\n",
      "  get-tuple-element.507 = s64[] get-tuple-element(p0.506), index=0\n",
      "  constant.519 = s64[] constant(2)\n",
      "  ROOT compare.520 = pred[] compare(get-tuple-element.507, constant.519), direction=LT\n",
      "}\n",
      "\n",
      "scan.521 {\n",
      "  p0.522 = s64[] parameter(0)\n",
      "  p1.523 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(1)\n",
      "  p2.524 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(2)\n",
      "  p3.525 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(3)\n",
      "  p4.526 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(4)\n",
      "  p5.527 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(5)\n",
      "  p6.528 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(6)\n",
      "  p7.529 = f32[2,8,2,128]{3,2,1,0} parameter(7)\n",
      "  p8.530 = f32[2,8,2,128]{3,2,1,0} parameter(8)\n",
      "  p9.531 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(9)\n",
      "  p10.532 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(10)\n",
      "  p11.533 = f32[2,8,2,128,8]{4,3,2,1,0} parameter(11)\n",
      "  tuple.534 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) tuple(p0.522, p1.523, p2.524, p3.525, p4.526, /*index=5*/p5.527, p6.528, p7.529, p8.530, p9.531, /*index=10*/p10.532, p11.533)\n",
      "  ROOT while.535 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) while(tuple.534), condition=Condition.505, body=Body.385\n",
      "} // scan.521\n",
      "\n",
      "MaxComputation.562 {\n",
      "  x.563 = f32[] parameter(0)\n",
      "  y.564 = f32[] parameter(1)\n",
      "  ROOT maximum.565 = f32[] maximum(x.563, y.564)\n",
      "}\n",
      "\n",
      "AddComputation.571 {\n",
      "  x.572 = f32[] parameter(0)\n",
      "  y.573 = f32[] parameter(1)\n",
      "  ROOT add.574 = f32[] add(x.572, y.573)\n",
      "}\n",
      "\n",
      "AddComputation.597 {\n",
      "  x.598 = f32[] parameter(0)\n",
      "  y.599 = f32[] parameter(1)\n",
      "  ROOT add.600 = f32[] add(x.598, y.599)\n",
      "}\n",
      "\n",
      "ENTRY SyncTensorsGraph.614 {\n",
      "  p0.1 = f32[16,2,128,8]{2,0,3,1} parameter(0), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  p1.2 = f32[16,2,128,8]{2,0,3,1} parameter(1), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  p2.3 = f32[16,2,128,8]{2,0,3,1} parameter(2), sharding={devices=[4,2,1,1]0,1,2,3,4,5,6,7}\n",
      "  constant.4 = f32[] constant(0)\n",
      "  reshape.5 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.4)\n",
      "  broadcast.6 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.5), dimensions={0,1,2,3,4}\n",
      "  reshape.7 = f32[] reshape(broadcast.6)\n",
      "  broadcast.8 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.7), dimensions={}\n",
      "  constant.47 = s64[] constant(0)\n",
      "  reshape.46 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p0.1)\n",
      "  reshape.45 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p1.2)\n",
      "  reshape.44 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(p2.3)\n",
      "  constant.39 = f32[] constant(0)\n",
      "  reshape.40 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.39)\n",
      "  broadcast.41 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.40), dimensions={0,1,2,3,4}\n",
      "  reshape.42 = f32[] reshape(broadcast.41)\n",
      "  broadcast.43 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.42), dimensions={}\n",
      "  constant.34 = f32[] constant(0)\n",
      "  reshape.35 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.34)\n",
      "  broadcast.36 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.35), dimensions={0,1,2,3,4}\n",
      "  reshape.37 = f32[] reshape(broadcast.36)\n",
      "  broadcast.38 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.37), dimensions={}\n",
      "  constant.29 = f32[] constant(0)\n",
      "  reshape.30 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.29)\n",
      "  broadcast.31 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.30), dimensions={0,1,2,3,4}\n",
      "  reshape.32 = f32[] reshape(broadcast.31)\n",
      "  broadcast.33 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.32), dimensions={}\n",
      "  constant.24 = f32[] constant(0)\n",
      "  reshape.25 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.24)\n",
      "  broadcast.26 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.25), dimensions={0,1,2,3,4}\n",
      "  reshape.27 = f32[] reshape(broadcast.26)\n",
      "  broadcast.28 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.27), dimensions={}\n",
      "  constant.19 = f32[] constant(0)\n",
      "  reshape.20 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.19)\n",
      "  broadcast.21 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.20), dimensions={0,1,2,3,4}\n",
      "  reshape.22 = f32[] reshape(broadcast.21)\n",
      "  broadcast.23 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.22), dimensions={}\n",
      "  constant.14 = f32[] constant(0)\n",
      "  reshape.15 = f32[1,1,1,1]{3,2,1,0} reshape(constant.14)\n",
      "  broadcast.16 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.15), dimensions={0,1,2,3}\n",
      "  reshape.17 = f32[] reshape(broadcast.16)\n",
      "  broadcast.18 = f32[2,8,2,128]{3,2,1,0} broadcast(reshape.17), dimensions={}\n",
      "  constant.9 = f32[] constant(0)\n",
      "  reshape.10 = f32[1,1,1,1]{3,2,1,0} reshape(constant.9)\n",
      "  broadcast.11 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.10), dimensions={0,1,2,3}\n",
      "  reshape.12 = f32[] reshape(broadcast.11)\n",
      "  broadcast.13 = f32[2,8,2,128]{3,2,1,0} broadcast(reshape.12), dimensions={}\n",
      "  call.253 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}) call(constant.47, broadcast.8, reshape.46, reshape.45, reshape.44, /*index=5*/broadcast.43, broadcast.38, broadcast.33, broadcast.28, broadcast.23, /*index=10*/broadcast.18, broadcast.13), to_apply=scan.238\n",
      "  get-tuple-element.259 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.253), index=5\n",
      "  reshape.266 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.259)\n",
      "  constant.294 = s64[] constant(0)\n",
      "  constant.289 = f32[] constant(0)\n",
      "  reshape.290 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.289)\n",
      "  broadcast.291 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.290), dimensions={0,1,2,3,4}\n",
      "  reshape.292 = f32[] reshape(broadcast.291)\n",
      "  broadcast.293 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.292), dimensions={}\n",
      "  constant.282 = f32[] constant(1)\n",
      "  broadcast.283 = f32[] broadcast(constant.282), dimensions={}\n",
      "  reshape.284 = f32[1,1,1,1]{3,2,1,0} reshape(broadcast.283)\n",
      "  broadcast.285 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.284), dimensions={0,1,2,3}\n",
      "  reshape.286 = f32[] reshape(broadcast.285)\n",
      "  broadcast.287 = f32[16,2,128,8]{3,2,1,0} broadcast(reshape.286), dimensions={}\n",
      "  reshape.288 = f32[2,8,2,128,8]{4,3,2,1,0} reshape(broadcast.287)\n",
      "  get-tuple-element.260 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.253), index=6\n",
      "  get-tuple-element.261 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.253), index=7\n",
      "  get-tuple-element.262 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.253), index=8\n",
      "  get-tuple-element.263 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.253), index=9\n",
      "  get-tuple-element.264 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(call.253), index=10\n",
      "  get-tuple-element.265 = f32[2,8,2,128]{3,2,1,0} get-tuple-element(call.253), index=11\n",
      "  constant.277 = f32[] constant(0)\n",
      "  reshape.278 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.277)\n",
      "  broadcast.279 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.278), dimensions={0,1,2,3,4}\n",
      "  reshape.280 = f32[] reshape(broadcast.279)\n",
      "  broadcast.281 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.280), dimensions={}\n",
      "  constant.272 = f32[] constant(0)\n",
      "  reshape.273 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.272)\n",
      "  broadcast.274 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.273), dimensions={0,1,2,3,4}\n",
      "  reshape.275 = f32[] reshape(broadcast.274)\n",
      "  broadcast.276 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.275), dimensions={}\n",
      "  constant.267 = f32[] constant(0)\n",
      "  reshape.268 = f32[1,1,1,1,1]{4,3,2,1,0} reshape(constant.267)\n",
      "  broadcast.269 = f32[1,1,1,1,1]{4,3,2,1,0} broadcast(reshape.268), dimensions={0,1,2,3,4}\n",
      "  reshape.270 = f32[] reshape(broadcast.269)\n",
      "  broadcast.271 = f32[2,8,2,128,8]{4,3,2,1,0} broadcast(reshape.270), dimensions={}\n",
      "  call.536 = (s64[], f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}) call(constant.294, broadcast.293, reshape.288, get-tuple-element.260, get-tuple-element.261, /*index=5*/get-tuple-element.262, get-tuple-element.263, get-tuple-element.264, get-tuple-element.265, broadcast.281, /*index=10*/broadcast.276, broadcast.271), to_apply=scan.521\n",
      "  get-tuple-element.538 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.536), index=1\n",
      "  get-tuple-element.546 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.536), index=9\n",
      "  get-tuple-element.547 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.536), index=10\n",
      "  get-tuple-element.548 = f32[2,8,2,128,8]{4,3,2,1,0} get-tuple-element(call.536), index=11\n",
      "  reshape.549 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.548)\n",
      "  reshape.550 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.547)\n",
      "  reshape.551 = f32[16,2,128,8]{3,2,1,0} reshape(get-tuple-element.546)\n",
      "  broadcast.557 = f32[16,2,128,8]{3,2,1,0} broadcast(p0.1), dimensions={0,1,2,3}\n",
      "  reshape.558 = f32[32,128,8]{2,1,0} reshape(broadcast.557)\n",
      "  transpose.554 = f32[16,2,8,128]{3,0,2,1} transpose(p1.2), dimensions={0,1,3,2}\n",
      "  broadcast.555 = f32[16,2,8,128]{3,2,1,0} broadcast(transpose.554), dimensions={0,1,2,3}\n",
      "  reshape.556 = f32[32,8,128]{2,1,0} reshape(broadcast.555)\n",
      "  dot.559 = f32[32,128,128]{2,1,0} dot(reshape.558, reshape.556), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.560 = f32[16,2,128,128]{3,2,1,0} reshape(dot.559)\n",
      "  constant.561 = f32[] constant(-inf)\n",
      "  reduce.566 = f32[16,2,128]{2,1,0} reduce(reshape.560, constant.561), dimensions={3}, to_apply=MaxComputation.562\n",
      "  broadcast.567 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.566), dimensions={0,1,2}\n",
      "  subtract.568 = f32[16,2,128,128]{3,2,1,0} subtract(reshape.560, broadcast.567)\n",
      "  exponential.569 = f32[16,2,128,128]{3,2,1,0} exponential(subtract.568)\n",
      "  constant.570 = f32[] constant(0)\n",
      "  reduce.575 = f32[16,2,128]{2,1,0} reduce(exponential.569, constant.570), dimensions={3}, to_apply=AddComputation.571\n",
      "  broadcast.576 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.575), dimensions={0,1,2}\n",
      "  divide.577 = f32[16,2,128,128]{3,2,1,0} divide(exponential.569, broadcast.576)\n",
      "  broadcast.578 = f32[16,2,128,128]{3,2,1,0} broadcast(divide.577), dimensions={0,1,2,3}\n",
      "  reshape.579 = f32[32,128,128]{2,1,0} reshape(broadcast.578)\n",
      "  broadcast.552 = f32[16,2,128,8]{3,2,1,0} broadcast(p2.3), dimensions={0,1,2,3}\n",
      "  reshape.553 = f32[32,128,8]{2,1,0} reshape(broadcast.552)\n",
      "  dot.580 = f32[32,128,8]{2,1,0} dot(reshape.579, reshape.553), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.581 = f32[16,2,128,8]{3,2,1,0} reshape(dot.580)\n",
      "  transpose.589 = f32[32,128,128]{1,2,0} transpose(reshape.579), dimensions={0,2,1}\n",
      "  constant.582 = f32[] constant(1)\n",
      "  broadcast.583 = f32[] broadcast(constant.582), dimensions={}\n",
      "  reshape.584 = f32[1,1,1,1]{3,2,1,0} reshape(broadcast.583)\n",
      "  broadcast.585 = f32[1,1,1,1]{3,2,1,0} broadcast(reshape.584), dimensions={0,1,2,3}\n",
      "  reshape.586 = f32[] reshape(broadcast.585)\n",
      "  broadcast.587 = f32[16,2,128,8]{3,2,1,0} broadcast(reshape.586), dimensions={}\n",
      "  reshape.588 = f32[32,128,8]{2,1,0} reshape(broadcast.587)\n",
      "  dot.590 = f32[32,128,8]{2,1,0} dot(transpose.589, reshape.588), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.591 = f32[16,2,128,8]{3,2,1,0} reshape(dot.590)\n",
      "  transpose.606 = f32[32,8,128]{1,2,0} transpose(reshape.558), dimensions={0,2,1}\n",
      "  transpose.592 = f32[32,8,128]{1,2,0} transpose(reshape.553), dimensions={0,2,1}\n",
      "  dot.593 = f32[32,128,128]{2,1,0} dot(reshape.588, transpose.592), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.594 = f32[16,2,128,128]{3,2,1,0} reshape(dot.593)\n",
      "  multiply.595 = f32[16,2,128,128]{3,2,1,0} multiply(reshape.594, divide.577)\n",
      "  constant.596 = f32[] constant(0)\n",
      "  reduce.601 = f32[16,2,128]{2,1,0} reduce(multiply.595, constant.596), dimensions={3}, to_apply=AddComputation.597\n",
      "  broadcast.602 = f32[16,2,128,128]{3,2,1,0} broadcast(reduce.601), dimensions={0,1,2}\n",
      "  subtract.603 = f32[16,2,128,128]{3,2,1,0} subtract(reshape.594, broadcast.602)\n",
      "  multiply.604 = f32[16,2,128,128]{3,2,1,0} multiply(divide.577, subtract.603)\n",
      "  reshape.605 = f32[32,128,128]{2,1,0} reshape(multiply.604)\n",
      "  dot.607 = f32[32,8,128]{2,1,0} dot(transpose.606, reshape.605), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  transpose.608 = f32[32,128,8]{1,2,0} transpose(reshape.556), dimensions={0,2,1}\n",
      "  dot.609 = f32[32,128,8]{2,1,0} dot(reshape.605, transpose.608), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, operand_precision={highest,highest}\n",
      "  reshape.610 = f32[16,2,128,8]{3,2,1,0} reshape(dot.609)\n",
      "  reshape.611 = f32[16,2,8,128]{3,2,1,0} reshape(dot.607)\n",
      "  transpose.612 = f32[16,2,128,8]{2,3,1,0} transpose(reshape.611), dimensions={0,1,3,2}\n",
      "  ROOT tuple.613 = (f32[16,2,128,8]{2,0,3,1}, f32[16,2,128,8]{2,0,3,1}, f32[16,2,128,8]{2,0,3,1}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=5*/f32[16,2,128,8]{3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, f32[2,8,2,128,8]{4,3,2,1,0}, /*index=10*/f32[16,2,128,8]{3,2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, /*index=15*/f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, f32[32,8,128]{2,1,0}, f32[32,128,8]{2,1,0}, f32[16,2,128,8]{3,2,1,0}, /*index=20*/f32[16,2,128,8]{2,3,1,0}) tuple(p0.1, p1.2, p2.3, broadcast.8, get-tuple-element.259, /*index=5*/reshape.266, get-tuple-element.538, get-tuple-element.546, get-tuple-element.547, get-tuple-element.548, /*index=10*/reshape.549, reshape.550, reshape.551, dot.580, reshape.581, /*index=15*/dot.590, reshape.591, dot.607, dot.609, reshape.610, /*index=20*/transpose.612)\n",
      "} // SyncTensorsGraph.614\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "for f in os.listdir('xla_dumps/aot-sharded-flash-attention'):\n",
    "  p = Path(f\"xla_dumps/aot-sharded-flash-attention/{f}\")\n",
    "  if p.is_file():\n",
    "    if \"before_optimizations\" in str(p.stem):\n",
    "      print(p.read_text())\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
