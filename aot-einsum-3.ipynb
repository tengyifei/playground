{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    }
   ],
   "source": [
    "# See https://dev-discuss.pytorch.org/t/a-small-mps-debugging-story/769\n",
    "%env TORCH_SHOW_DISPATCH_TRACE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einsum investigation\n",
    "\n",
    "This notebook is a minimal reproducer of the lowering issue of einsum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a utility that inspects the lowering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::zeros], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::zeros], key=[XLA]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[XLA]\n",
      "   [call] op=[aten::zero_], key=[Functionalize]\n",
      "    [call] op=[aten::zero_], key=[Meta]\n",
      "    [call] op=[aten::zero], key=[XLA]\n",
      "     [call] op=[aten::clone], key=[XLA]\n",
      "     [call] op=[aten::zero_], key=[XLA]\n",
      "    [call] op=[aten::_propagate_xla_data], key=[XLA]\n",
      " [call] op=[aten::zeros], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::zeros], key=[XLA]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[XLA]\n",
      "   [call] op=[aten::zero_], key=[Functionalize]\n",
      "    [call] op=[aten::zero_], key=[Meta]\n",
      "    [call] op=[aten::zero], key=[XLA]\n",
      "     [call] op=[aten::clone], key=[XLA]\n",
      "     [call] op=[aten::zero_], key=[XLA]\n",
      "    [call] op=[aten::_propagate_xla_data], key=[XLA]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import time\n",
    "\n",
    "X = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "Y = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "\n",
    "def test_lowering(func):\n",
    "  time.sleep(1)\n",
    "  out = func(X, Y)\n",
    "  time.sleep(1)\n",
    "  ir = torch_xla._XLAC._get_xla_tensors_text([out])\n",
    "  if 'einsum' not in ir:\n",
    "    print(\"!!!!!!!!!!WRONG!!!!!!!!!!! Did not find einsum in lowering\")\n",
    "    print(\"IR:\")\n",
    "    print(ir)\n",
    "  else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we test this on a regular einsum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::einsum], key=[AutogradXLA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "test_lowering(lambda a, b: torch.einsum('...n,mn->...m', a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a custom op that wraps said einsum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.library import custom_op\n",
    "\n",
    "@custom_op(\"xla::custom_linear_forward123\", schema=\"(Tensor input, Tensor weight) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward123(input: Tensor, weight: Tensor):\n",
    "    return torch.einsum('...n,mn->...m', input, weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the custom op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [callBoxed] op=[xla::custom_linear_forward123], key=[AutogradXLA]\n",
      "  [redispatchBoxed] op=[xla::custom_linear_forward123], key=[Functionalize]\n",
      "   [callBoxed] op=[xla::custom_linear_forward123], key=[XLA]\n",
      "    [call] op=[aten::einsum], key=[XLA]\n",
      "     [call] op=[aten::unsqueeze], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::unsqueeze], key=[XLA]\n",
      "       [call] op=[aten::as_strided], key=[XLA]\n",
      "     [call] op=[aten::permute], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::permute], key=[XLA]\n",
      "       [redispatchBoxed] op=[aten::permute], key=[Meta]\n",
      "        [call] op=[aten::as_strided], key=[Functionalize]\n",
      "         [call] op=[aten::as_strided], key=[Meta]\n",
      "         [call] op=[aten::as_strided_copy], key=[XLA]\n",
      "     [call] op=[aten::unsqueeze], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::unsqueeze], key=[XLA]\n",
      "       [call] op=[aten::as_strided], key=[XLA]\n",
      "     [call] op=[aten::permute], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::permute], key=[XLA]\n",
      "       [redispatchBoxed] op=[aten::permute], key=[Meta]\n",
      "        [call] op=[aten::as_strided], key=[Functionalize]\n",
      "         [call] op=[aten::as_strided], key=[Meta]\n",
      "         [call] op=[aten::as_strided_copy], key=[XLA]\n",
      "     [call] op=[aten::permute], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::permute], key=[XLA]\n",
      "       [redispatchBoxed] op=[aten::permute], key=[Meta]\n",
      "        [call] op=[aten::as_strided], key=[Functionalize]\n",
      "         [call] op=[aten::as_strided], key=[Meta]\n",
      "         [call] op=[aten::as_strided_copy], key=[XLA]\n",
      "     [call] op=[aten::reshape], key=[XLA]\n",
      "      [call] op=[aten::view], key=[ADInplaceOrView]\n",
      "       [redispatch] op=[aten::view], key=[XLA]\n",
      "     [call] op=[aten::permute], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::permute], key=[XLA]\n",
      "       [redispatchBoxed] op=[aten::permute], key=[Meta]\n",
      "        [call] op=[aten::as_strided], key=[Functionalize]\n",
      "         [call] op=[aten::as_strided], key=[Meta]\n",
      "         [call] op=[aten::as_strided_copy], key=[XLA]\n",
      "     [call] op=[aten::reshape], key=[XLA]\n",
      "      [call] op=[aten::view], key=[ADInplaceOrView]\n",
      "       [redispatch] op=[aten::view], key=[XLA]\n",
      "     [call] op=[aten::bmm], key=[XLA]\n",
      "     [call] op=[aten::view], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::view], key=[XLA]\n",
      "     [call] op=[aten::permute], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::permute], key=[XLA]\n",
      "       [redispatchBoxed] op=[aten::permute], key=[Meta]\n",
      "        [call] op=[aten::as_strided], key=[Functionalize]\n",
      "         [call] op=[aten::as_strided], key=[Meta]\n",
      "         [call] op=[aten::as_strided_copy], key=[XLA]\n",
      "     [call] op=[aten::view], key=[ADInplaceOrView]\n",
      "      [redispatch] op=[aten::view], key=[XLA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!WRONG!!!!!!!!!!! Did not find einsum in lowering\n",
      "IR:\n",
      "IR {\n",
      "  %0 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %1 = f32[3,3]{1,0} aten::expand(%0), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3,1]{2,1,0} aten::as_strided(%1), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::as_strided(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %6 = f32[3,3]{1,0} aten::expand(%5), xla_shape=f32[3,3]{1,0}\n",
      "  %7 = f32[3,3,1]{2,1,0} aten::as_strided(%6), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::as_strided(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{2,1,0} aten::as_strided(%11), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lowering(lambda a, b: custom_linear_forward123(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's different between these two traces?\n",
    "\n",
    "The first one has\n",
    "\n",
    "```\n",
    " [call] op=[aten::einsum], key=[AutogradXLA]\n",
    "```\n",
    "\n",
    "while the second one has\n",
    "\n",
    "```\n",
    " [call] op=[aten::einsum], key=[XLA]\n",
    "```\n",
    "\n",
    "followed by a whole bunch of decomposed aten operations.\n",
    "\n",
    "This suggests that when calling `torch.einsum` with the `XLA` dispatch key,\n",
    "our registered lowerings are bypassed. Instead, some other code in PyTorch\n",
    "handles it and turns the einsum into a bunch of permutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
