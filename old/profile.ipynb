{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_xla import runtime as xr\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "    self.fc1 = nn.Linear(64 * 54 * 54, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = nn.functional.relu(x)\n",
    "    x = nn.functional.max_pool2d(x, 2)\n",
    "    x = self.conv2(x)\n",
    "    x = nn.functional.relu(x)\n",
    "    x = nn.functional.max_pool2d(x, 2)\n",
    "    x = x.view(-1, 64 * 54 * 54)\n",
    "    x = self.fc1(x)\n",
    "    x = nn.functional.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class TrainResNetBase():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.img_dim = 224\n",
    "    self.batch_size = 256\n",
    "    self.num_steps = 100\n",
    "    self.num_epochs = 1\n",
    "    self.train_dataset_len = 1200000  # Roughly the size of Imagenet dataset.\n",
    "    # For the purpose of this example, we are going to use fake data.\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.zeros(self.batch_size, 3, self.img_dim, self.img_dim),\n",
    "              torch.zeros(self.batch_size, dtype=torch.int64)),\n",
    "        sample_count=self.train_dataset_len // self.batch_size //\n",
    "        xr.world_size())\n",
    "\n",
    "    self.device = torch_xla.device()\n",
    "    self.train_device_loader = pl.MpDeviceLoader(train_loader, self.device)\n",
    "    self.model = CNN().to(self.device)\n",
    "    self.optimizer = optim.SGD(self.model.parameters(), weight_decay=1e-4)\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "    self.compiled_step_fn = torch_xla.compile(self.step_fn, name=\"train_step\")\n",
    "\n",
    "  def _train_update(self, step, loss, tracker, epoch):\n",
    "    print(f'epoch: {epoch}, step: {step}, loss: {loss}, rate: {tracker.rate()}')\n",
    "\n",
    "  def run_optimizer(self):\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def step_fn(self, data, target):\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(data)\n",
    "    loss = self.loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    self.run_optimizer()\n",
    "\n",
    "  def train_loop_fn(self, loader, epoch):\n",
    "    import torch_xla.debug.profiler as xp\n",
    "    import os\n",
    "    profile_port = 9012\n",
    "    # you can also set profile_logdir to a gs bucket, for example\n",
    "    # profile_logdir = \"gs://your_gs_bucket/profile\"\n",
    "    profile_logdir = \"/workspaces/torch/playground/profile/\"\n",
    "    duration_ms = 180\n",
    "    assert profile_logdir.startswith('gs://') or os.path.exists(profile_logdir)\n",
    "    server = xp.start_server(profile_port)\n",
    "\n",
    "    tracker = xm.RateTracker()\n",
    "    self.model.train()\n",
    "    loader = itertools.islice(loader, self.num_steps)\n",
    "    for step, (data, target) in enumerate(loader):\n",
    "      loss = self.compiled_step_fn(data, target)  # type: ignore\n",
    "      tracker.add(self.batch_size)\n",
    "      if step % 10 == 0:\n",
    "        xm.add_step_closure(\n",
    "            self._train_update, args=(step, loss, tracker, epoch))\n",
    "      if step == 12:\n",
    "        xp.trace_detached(\n",
    "          f'localhost:{profile_port}', profile_logdir, duration_ms=duration_ms)\n",
    "\n",
    "  def start_training(self):\n",
    "\n",
    "    for epoch in range(1, self.num_epochs + 1):\n",
    "      xm.master_print('Epoch {} train begin {}'.format(\n",
    "          epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))\n",
    "      self.train_loop_fn(self.train_device_loader, epoch)\n",
    "      xm.master_print('Epoch {} train end {}'.format(\n",
    "          epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))\n",
    "    xm.wait_device_ops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# check https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#environment-variables\n",
    "os.environ[\"XLA_IR_DEBUG\"] = \"0\"\n",
    "os.environ[\"XLA_HLO_DEBUG\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train begin  2:05AM UTC on Jan 31, 2025\n",
      "epoch: 1, step: 0, loss: None, rate: 7836.330372716591\n",
      "epoch: 1, step: 10, loss: None, rate: 51903.29374240089\n",
      "Starting to trace for 180 ms. Remaining attempt(s): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 02:05:23.069579: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 968060 nanoseconds and will start immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 20, loss: None, rate: 28618.559073762506\n",
      "epoch: 1, step: 30, loss: None, rate: 18873.392118030828\n",
      "epoch: 1, step: 40, loss: None, rate: 14842.749830784665\n",
      "epoch: 1, step: 50, loss: None, rate: 13228.008486592364\n",
      "epoch: 1, step: 60, loss: None, rate: 13865.532707287919\n",
      "epoch: 1, step: 70, loss: None, rate: 12957.070395361821\n",
      "epoch: 1, step: 80, loss: None, rate: 11350.36131181967\n",
      "epoch: 1, step: 90, loss: None, rate: 12768.881452208418\n",
      "Epoch 1 train end  2:05AM UTC on Jan 31, 2025\n"
     ]
    }
   ],
   "source": [
    "base = TrainResNetBase()\n",
    "base.start_training()\n",
    "\n",
    "# You can view the profile at tensorboard by\n",
    "# 1. pip install tensorflow-cpu tensorboard-plugin-profile\n",
    "# 2. tensorboard --logdir /tmp/profile/ --port 6006\n",
    "# For more detail please take a look at https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
