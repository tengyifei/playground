{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Remember to pick /tmp/profile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_xla import runtime as xr\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TrainResNetBase():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.img_dim = 224\n",
    "    self.batch_size = 128\n",
    "    self.num_steps = 300\n",
    "    self.num_epochs = 1\n",
    "    self.train_dataset_len = 1200000  # Roughly the size of Imagenet dataset.\n",
    "    # For the purpose of this example, we are going to use fake data.\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.zeros(self.batch_size, 3, self.img_dim, self.img_dim),\n",
    "              torch.zeros(self.batch_size, dtype=torch.int64)),\n",
    "        sample_count=self.train_dataset_len // self.batch_size //\n",
    "        xr.world_size())\n",
    "\n",
    "    self.device = torch_xla.device()\n",
    "    self.train_device_loader = pl.MpDeviceLoader(train_loader, self.device)\n",
    "    self.model = torchvision.models.resnet50().to(self.device)\n",
    "    self.optimizer = optim.SGD(self.model.parameters(), weight_decay=1e-4)\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "    self.compiled_step_fn = torch_xla.experimental.compile(self.step_fn)\n",
    "\n",
    "  def _train_update(self, step, loss, tracker, epoch):\n",
    "    print(f'epoch: {epoch}, step: {step}, loss: {loss}, rate: {tracker.rate()}')\n",
    "\n",
    "  def run_optimizer(self):\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def step_fn(self, data, target):\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(data)\n",
    "    loss = self.loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    self.run_optimizer()\n",
    "\n",
    "  def train_loop_fn(self, loader, epoch):\n",
    "    tracker = xm.RateTracker()\n",
    "    self.model.train()\n",
    "    loader = itertools.islice(loader, self.num_steps)\n",
    "    for step, (data, target) in enumerate(loader):\n",
    "      loss = self.compiled_step_fn(data, target)\n",
    "      tracker.add(self.batch_size)\n",
    "      if step % 10 == 0:\n",
    "        xm.add_step_closure(\n",
    "            self._train_update, args=(step, loss, tracker, epoch))\n",
    "\n",
    "  def start_training(self):\n",
    "\n",
    "    for epoch in range(1, self.num_epochs + 1):\n",
    "      xm.master_print('Epoch {} train begin {}'.format(\n",
    "          epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))\n",
    "      self.train_loop_fn(self.train_device_loader, epoch)\n",
    "      xm.master_print('Epoch {} train end {}'.format(\n",
    "          epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))\n",
    "    xm.wait_device_ops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch_xla.debug.profiler as xp\n",
    "\n",
    "# check https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#environment-variables\n",
    "os.environ[\"XLA_IR_DEBUG\"] = \"1\"\n",
    "os.environ[\"XLA_HLO_DEBUG\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train begin  8:44PM UTC on Jul 31, 2024\n",
      "Starting to trace for 30000 ms. Remaining attempt(s): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 20:44:38.864042: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 2952170 nanoseconds and will start immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss: None, rate: 4.546404332686245\n",
      "epoch: 1, step: 10, loss: None, rate: 234.35643001468216\n",
      "epoch: 1, step: 20, loss: None, rate: 326.6896138542191\n",
      "epoch: 1, step: 30, loss: None, rate: 365.50556414045957\n",
      "epoch: 1, step: 40, loss: None, rate: 383.52400527549355\n",
      "epoch: 1, step: 50, loss: None, rate: 389.3695036014534\n",
      "epoch: 1, step: 60, loss: None, rate: 392.5179354161948\n",
      "epoch: 1, step: 70, loss: None, rate: 394.5766531756856\n",
      "epoch: 1, step: 80, loss: None, rate: 394.8321200405354\n",
      "epoch: 1, step: 90, loss: None, rate: 395.1472163241774\n",
      "epoch: 1, step: 100, loss: None, rate: 394.1538913699761\n",
      "epoch: 1, step: 110, loss: None, rate: 394.62894760046026\n",
      "epoch: 1, step: 120, loss: None, rate: 394.675310043137\n",
      "epoch: 1, step: 130, loss: None, rate: 394.3734698171106\n",
      "epoch: 1, step: 140, loss: None, rate: 392.9848480556546\n",
      "epoch: 1, step: 150, loss: None, rate: 394.31165761017155\n",
      "epoch: 1, step: 160, loss: None, rate: 394.52356824373067\n",
      "epoch: 1, step: 170, loss: None, rate: 394.4836033307845\n",
      "epoch: 1, step: 180, loss: None, rate: 393.29338617172044\n",
      "epoch: 1, step: 190, loss: None, rate: 394.2347426494489\n",
      "epoch: 1, step: 200, loss: None, rate: 393.983544563922\n",
      "epoch: 1, step: 210, loss: None, rate: 394.1148584509109\n",
      "epoch: 1, step: 220, loss: None, rate: 394.14689295221456\n",
      "epoch: 1, step: 230, loss: None, rate: 394.2619889944699\n",
      "epoch: 1, step: 240, loss: None, rate: 394.52185828955885\n",
      "epoch: 1, step: 250, loss: None, rate: 394.29333732383645\n",
      "epoch: 1, step: 260, loss: None, rate: 394.2313601620011\n",
      "epoch: 1, step: 270, loss: None, rate: 394.2205668920854\n",
      "epoch: 1, step: 280, loss: None, rate: 394.1336599468837\n",
      "epoch: 1, step: 290, loss: None, rate: 395.29373160154114\n",
      "Epoch 1 train end  8:46PM UTC on Jul 31, 2024\n"
     ]
    }
   ],
   "source": [
    "base = TrainResNetBase()\n",
    "profile_port = 9012\n",
    "# you can also set profile_logdir to a gs bucket, for example\n",
    "# profile_logdir = \"gs://your_gs_bucket/profile\"\n",
    "profile_logdir = \"/tmp/profile/\"\n",
    "duration_ms = 30000\n",
    "assert profile_logdir.startswith('gs://') or os.path.exists(profile_logdir)\n",
    "server = xp.start_server(profile_port)\n",
    "# Ideally you want to start the profile tracing after the initial compilation, for example\n",
    "# at step 5.\n",
    "xp.trace_detached(\n",
    "    f'localhost:{profile_port}', profile_logdir, duration_ms=duration_ms)\n",
    "base.start_training()\n",
    "# You can view the profile at tensorboard by\n",
    "# 1. pip install tensorflow-cpu tensorboard-plugin-profile\n",
    "# 2. tensorboard --logdir /tmp/profile/ --port 6006\n",
    "# For more detail please take a look at https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
