{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=CPU\n"
     ]
    }
   ],
   "source": [
    "# For quick debugging and crash recovery\n",
    "%env PJRT_DEVICE=CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, TypeVar\n",
    "\n",
    "import torch\n",
    "import torch_xla.core.xla_builder as xb\n",
    "from torch._ops import HigherOrderOperator\n",
    "from torch._C import DispatchKey\n",
    "from torch._higher_order_ops.utils import autograd_not_implemented\n",
    "\n",
    "import torch_xla\n",
    "\n",
    "Carry = TypeVar('Carry')\n",
    "X = TypeVar('X')\n",
    "Y = TypeVar('Y')\n",
    "\n",
    "\n",
    "class ScanOp(HigherOrderOperator):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(\"scan\")\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      fn: Callable[[Carry, X], tuple[Carry, Y]],\n",
    "      init: Carry,\n",
    "      xs: X,\n",
    "      /,\n",
    "  ) -> tuple[Carry, Y]:\n",
    "    return super().__call__(fn, init, xs)  # type: ignore\n",
    "\n",
    "\n",
    "scan_op = ScanOp()\n",
    "\n",
    "\n",
    "def scan(\n",
    "    fn: Callable[[Carry, X], tuple[Carry, Y]],\n",
    "    init: Carry,\n",
    "    xs: X,\n",
    ") -> tuple[Carry, Y]:\n",
    "  return scan_op(fn, init, xs)\n",
    "\n",
    "\n",
    "def dynamic_update_slice(ys: xb.Op, y: xb.Op, idx: xb.Op) -> xb.Op:\n",
    "  # See https://openxla.org/xla/operation_semantics#dynamicupdateslice.\n",
    "  y = y.broadcast([1])\n",
    "  indices = [idx]\n",
    "  for _ in range(ys.shape().rank - 1):\n",
    "    indices.append(idx.zeros_like())\n",
    "  # TODO: This is buggy\n",
    "  return ys.dynamic_update_slice(y, indices)\n",
    "\n",
    "\n",
    "def dynamic_slice(xs: xb.Op, idx: xb.Op) -> xb.Op:\n",
    "  indices = [idx]\n",
    "  for _ in range(xs.shape().rank - 1):\n",
    "    indices.append(idx.zeros_like())\n",
    "  slice_shape = list(xs.shape().sizes)\n",
    "  slice_shape[0] = 1\n",
    "  sliced = xs.dynamic_slice(indices, slice_shape)\n",
    "  shape = list(xs.shape().sizes)\n",
    "  shape = shape[1:]\n",
    "  return sliced.reshape(shape)\n",
    "\n",
    "\n",
    "# See https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md for\n",
    "# the meaning of CompositeExplicitAutogradNonFunctional.\n",
    "@scan_op.py_impl(DispatchKey.CompositeExplicitAutogradNonFunctional)\n",
    "@scan_op.py_impl(DispatchKey.XLA)\n",
    "def scan_dense(fn, init, xs):\n",
    "  \"\"\"Forward implementation of scan.\"\"\"\n",
    "\n",
    "  # Abstractly trace and lower `fn`.\n",
    "  # Later we will include `fn_computation` within the while loop body.\n",
    "  device = torch_xla.device()\n",
    "  fake_carry = torch.empty(init.size(), dtype=init.dtype).to(device)\n",
    "  fake_xs = torch.empty(xs[0].size(), dtype=xs[0].dtype).to(device)\n",
    "  fn_outputs = fn(fake_carry, fake_xs)\n",
    "  fn_ctx = torch_xla._XLAC.lowering.LoweringContext()\n",
    "  fn_ctx.set_name_string(\"my_ctx\")\n",
    "  fn_ctx.build(list(fn_outputs))\n",
    "  fn_hlo = fn_ctx.hlo()\n",
    "  fn_computation = xb.computation_from_module_proto(\"my_fn_computation\", fn_hlo)\n",
    "  xs_len = xs.shape[0]\n",
    "\n",
    "  # Figure out the shape of `ys` from the abstract tracing.\n",
    "  fn_carry_shape, fn_y_shape = (v.shape for v in fn_outputs)\n",
    "  assert fn_carry_shape == init.shape, f\"`fn` must keep the `carry` shape unchanged. \\\n",
    "    Got {fn_carry_shape} but expected {init.shape}\"\n",
    "\n",
    "  def cond_fn(num_iters: xb.Op, carry, xs, ys):\n",
    "    return num_iters > xb.Op.scalar(num_iters.builder(), 0, dtype=xb.Type.S64)\n",
    "\n",
    "  def body_fn(num_iters: xb.Op, carry: xb.Op, xs: xb.Op, ys: xb.Op):\n",
    "    xs_len_op = xb.Op.scalar(num_iters.builder(), xs_len, dtype=xb.Type.S64)\n",
    "    one = xb.Op.scalar(num_iters.builder(), 1, dtype=xb.Type.S64)\n",
    "    idx = xs_len_op - num_iters\n",
    "    x = dynamic_slice(xs, idx)\n",
    "    result = xb.Op.call(fn_computation, (carry, x))\n",
    "    carry = result.get_tuple_element(0)\n",
    "    y = result.get_tuple_element(1)\n",
    "    ys = dynamic_update_slice(ys, y, idx)\n",
    "    return xb.Op.tuple((num_iters - one, carry, xs, ys))\n",
    "\n",
    "  num_iters = torch.tensor(xs_len, device=device)\n",
    "  ys = torch.zeros((xs_len, *fn_y_shape), device=device)\n",
    "  carry = (num_iters, init, xs, ys)\n",
    "  builder = xb.create_builder('scan')\n",
    "  carry_param = []\n",
    "  for i, val in enumerate(carry):\n",
    "    carry_param.append(xb.mkparam(builder, i, xb.tensor_shape(val)))\n",
    "  res = xb.Op.mkwhile(tuple(carry_param), cond_fn, body_fn)\n",
    "  computation = res.build('scan')\n",
    "\n",
    "  _last_iter, carry, xs, ys = torch_xla._XLAC._xla_user_computation(\n",
    "      'xla::scan', carry, computation)\n",
    "\n",
    "  return carry, ys\n",
    "\n",
    "\n",
    "import torch.autograd\n",
    "\n",
    "\n",
    "class Scan(torch.autograd.Function):\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, fn, init, xs):\n",
    "    # Forward pass, save inputs for backward\n",
    "    ctx._fn = fn\n",
    "    with torch._C._AutoDispatchBelowAutograd():\n",
    "      carry, ys = scan(fn, init, xs)\n",
    "    ctx.save_for_backward(xs, carry)\n",
    "    return carry, ys\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_carry, grad_ys):\n",
    "    print(\"I'm in backward!!!!!!!!!!!!!!!!!\")\n",
    "    \n",
    "    fn = ctx._fn\n",
    "    xs, carry = ctx.saved_tensors\n",
    "\n",
    "    # Define the backward step function for each step in reverse\n",
    "    def backward_step_fn(grad_carry, grad_y, carry, x):\n",
    "      # Compute gradients for carry and x at each step\n",
    "      carry, y = fn(carry, x)\n",
    "      grad_x = grad_carry * x + grad_y\n",
    "      grad_carry = grad_carry * carry + grad_y\n",
    "      return grad_carry, grad_x\n",
    "\n",
    "    # Reverse loop to accumulate gradients using `scan`\n",
    "    # We flip the gradients and the input tensors to simulate reverse iteration\n",
    "    flipped_grad_ys = grad_ys.flip(dims=[0])\n",
    "    flipped_xs = xs.flip(dims=[0])\n",
    "\n",
    "    # Initial gradients for the carry and xs\n",
    "    grad_init = grad_carry.clone()\n",
    "    grad_xs = torch.zeros_like(xs)\n",
    "\n",
    "    with torch._C._AutoDispatchBelowAutograd():\n",
    "      _, reversed_grad_xs = scan(\n",
    "          lambda gc, gyx: backward_step_fn(gc, gyx[0], carry, gyx[1]),\n",
    "          grad_init, torch.stack([flipped_grad_ys, flipped_xs], dim=1))\n",
    "\n",
    "    # Flip the gradients back to original order\n",
    "    grad_xs = reversed_grad_xs.flip(dims=[0])\n",
    "\n",
    "    return None, grad_init, grad_xs\n",
    "\n",
    "\n",
    "# scan_op.py_impl(DispatchKey.AutogradXLA)(\n",
    "#     autograd_not_implemented(scan_op, deferred_error=True))\n",
    "scan_op.py_impl(DispatchKey.AutogradXLA)(Scan.apply)\n",
    "\n",
    "\n",
    "@scan_op.py_functionalize_impl\n",
    "def scan_func(ctx, fn, init, xs):\n",
    "  unwrapped_init = ctx.unwrap_tensors(init)\n",
    "  unwrapped_xs = ctx.unwrap_tensors(xs)\n",
    "  with ctx.redispatch_to_next() as m:\n",
    "    functional_fn = ctx.functionalize(fn)\n",
    "    ret = scan_op(\n",
    "        functional_fn,\n",
    "        unwrapped_init,\n",
    "        unwrapped_xs,\n",
    "    )\n",
    "    return ctx.wrap_tensors(ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `scan` operation when input and output have the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  FunctionalTensor(lvl=0, value=\\\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 3.,  3.,  3.],\n",
      "        [ 6.,  6.,  6.],\n",
      "        [10., 10., 10.]], device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "def cumsum(arr):\n",
    "\n",
    "  def scan_fn(carry, x):\n",
    "    return carry + x, carry + x\n",
    "\n",
    "  _, result = scan(scan_fn, torch.tensor([0.0] * 3, device=device), arr)\n",
    "  return result\n",
    "\n",
    "\n",
    "device = torch_xla.device()\n",
    "arr = torch.stack([\n",
    "    torch.tensor([1.0, 1.0, 1.0], device=torch.device(\"cpu\")) * i\n",
    "    for i in range(5)\n",
    "])\n",
    "arr = arr.to(device)\n",
    "\n",
    "cumulative_sum = cumsum(arr)\n",
    "torch_xla.sync()\n",
    "print(\"Result: \", cumulative_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `scan` operation when input and output have different shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  FunctionalTensor(lvl=0, value=\\\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n",
      "        [ 6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.],\n",
      "        [10., 10., 10., 10., 10., 10., 10., 10., 10.]], device='xla:0'))\n"
     ]
    }
   ],
   "source": [
    "def explode(arr):\n",
    "\n",
    "  def scan_fn(carry: torch.Tensor, x: torch.Tensor):\n",
    "    return carry + x, (carry + x).tile(3)\n",
    "\n",
    "  _, result = scan(scan_fn, torch.tensor([0.0] * 3, device=device), arr)\n",
    "  return result\n",
    "\n",
    "\n",
    "device = torch_xla.device()\n",
    "arr = torch.stack([\n",
    "    torch.tensor([1.0, 1.0, 1.0], device=torch.device(\"cpu\")) * i\n",
    "    for i in range(5)\n",
    "])\n",
    "arr = arr.to(device)\n",
    "\n",
    "res = explode(arr)\n",
    "torch_xla.sync()\n",
    "print(\"Result: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the backwards of `scan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(249., device='xla:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple function to be applied at each step of the scan\n",
    "def step_fn(carry, x):\n",
    "    new_carry = carry + x\n",
    "    y = carry * x\n",
    "    return new_carry, y\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "# Initial carry (let's make it a scalar with requires_grad)\n",
    "init_carry = torch.tensor([1.0, 1.0, 1.0], requires_grad=True, device=device)\n",
    "\n",
    "# Example input tensor of shape (batch_size, features)\n",
    "xs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], requires_grad=True, device=device)\n",
    "\n",
    "# Use the scan function\n",
    "final_carry, ys = scan(step_fn, init_carry, xs)\n",
    "\n",
    "# Loss for backward pass (sum of the outputs)\n",
    "loss = ys.sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def print_grad_fn(grad_fn, level=0):\n",
    "    if grad_fn is None:\n",
    "        return\n",
    "\n",
    "    print(\"  \" * level + str(grad_fn))\n",
    "    for next_fn in grad_fn.next_functions:\n",
    "        if next_fn[0] is not None:\n",
    "            print_grad_fn(next_fn[0], level + 1)\n",
    "\n",
    "print_grad_fn(loss.grad_fn)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: the following WIP impl supports dynamo tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode, track_tensor_tree\n",
    "import torch.utils._pytree as pytree\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch._higher_order_ops.utils import reenter_make_fx\n",
    "\n",
    "\n",
    "@scan_op.py_impl(ProxyTorchDispatchMode)\n",
    "def scan_tracing(mode, fn, init, xs):\n",
    "\n",
    "  def _trace_scan(proxy_mode, scan_op, fn, init, xs):\n",
    "    body_graph = reenter_make_fx(fn)(init)\n",
    "\n",
    "    next_name = None\n",
    "    i = 0\n",
    "    while not next_name:\n",
    "      candidate = f\"scan_graph_{i}\"\n",
    "      if hasattr(proxy_mode.tracer.root, candidate):\n",
    "        i += 1\n",
    "      else:\n",
    "        next_name = candidate\n",
    "    body_graph_name = next_name\n",
    "    assert not hasattr(proxy_mode.tracer.root, body_graph_name)\n",
    "    proxy_mode.tracer.root.register_module(body_graph_name, body_graph)\n",
    "    args = (body_graph, init, xs)\n",
    "    proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)\n",
    "    out_proxy = proxy_mode.tracer.create_proxy(\n",
    "        \"call_function\", scan_op, proxy_args, {}, name=\"scan\")\n",
    "\n",
    "    # fn return output with the same pytree and tensor meta data as carried_inputs\n",
    "    # so we could just return the output after one iteration.\n",
    "    out = fn(init, xs)\n",
    "    return track_tensor_tree(\n",
    "        out, out_proxy, constant=None, tracer=proxy_mode.tracer)\n",
    "\n",
    "  if mode.enable_tracing:\n",
    "    return _trace_scan(mode, scan_op, fn, init, xs)\n",
    "  else:\n",
    "    return scan_op(fn, init, xs)\n",
    "\n",
    "\n",
    "@scan_op.py_impl(FakeTensorMode)\n",
    "def while_loop_fake_tensor_mode(mode, cond_fn, body_fn, carried_inputs,\n",
    "                                additional_inputs):\n",
    "  with mode:\n",
    "    return body_fn(*carried_inputs, *additional_inputs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
