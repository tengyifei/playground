{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PJRT_DEVICE=TPU\n",
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
      "env: XLA_FLAGS=--xla_dump_to=xla_dumps\n",
      "env: LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n"
     ]
    }
   ],
   "source": [
    "%env PJRT_DEVICE=TPU\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env TPU_LIBRARY_PATH=/workspaces/torch/_libtpu.so\n",
    "%env XLA_FLAGS=--xla_dump_to=xla_dumps\n",
    "\n",
    "# MaxText flags except that we disable optimization barrier removal: crashes in native code\n",
    "%env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n",
    "\n",
    "# Still crashes in native code\n",
    "# %env LIBTPU_INIT_ARGS=--xla_enable_async_all_gather=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_decompose_all_gather_einsum=true --xla_tpu_decompose_einsum_reduce_scatter=true --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_spmd_rng_bit_generator_unsafe=true --xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100\n",
    "\n",
    "# Removed some more flags, and 1% scheduler shared memory limit: OK\n",
    "# %env LIBTPU_INIT_ARGS=--xla_tpu_overlap_compute_collective_tc=true --xla_tpu_use_enhanced_launch_barrier=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=2 --xla_tpu_aggressive_opt_barrier_removal=DISABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch\n",
    "from torch_xla import runtime as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.graph import saved_tensors_hooks\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "\n",
    "class OffloadingModule(torch.nn.Module):\n",
    "  def __init__(self, m):\n",
    "    super().__init__()\n",
    "    self.m = m\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "      return self.m(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoder_only_model\n",
    "from trainer import TrainDecoderOnlyBase\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2D FSDP+TP sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_xla.distributed.spmd as xs\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from torch_xla import runtime as xr\n",
    "from itertools import chain\n",
    "\n",
    "class TrainDecoderOnlyFSDPv2(TrainDecoderOnlyBase):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(decoder_only_model.DecoderOnlyConfig(\n",
    "       hidden_size=4096,\n",
    "       num_hidden_layers=2,\n",
    "       num_attention_heads=16,\n",
    "       num_key_value_heads=8,\n",
    "       intermediate_size=8192,\n",
    "       vocab_size=16384,\n",
    "    ))\n",
    "    # Define the mesh following common SPMD practice\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    tensor_axis = 4\n",
    "    fsdp_axis = num_devices // tensor_axis\n",
    "    mesh_shape = (fsdp_axis, tensor_axis)\n",
    "    print(f\"Single-slice sharding: mesh={mesh_shape}\")\n",
    "    spmd_mesh = xs.Mesh(list(range(num_devices)), mesh_shape, ('fsdp', 'tensor'))\n",
    "    xs.set_global_mesh(spmd_mesh)\n",
    "\n",
    "    model: decoder_only_model.DecoderOnlyModel = self.model  # type:ignore\n",
    "    self.model = model\n",
    "   \n",
    "    # Mark model weights to be sharded\n",
    "    for name, param in chain(model.named_parameters(), model.named_buffers()):\n",
    "      # Here we intentionally skip layernorm and moe.gate weights given they are small.\n",
    "      if 'embed_tokens' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'q_proj' in name or 'k_proj' in name or 'v_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "      elif 'o_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'gate_proj' in name or 'up_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('tensor', 'fsdp'))\n",
    "      elif 'down_proj' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, ('fsdp', 'tensor'))\n",
    "      elif 'lm_head' in name:\n",
    "          xs.mark_sharding(param, spmd_mesh, (('tensor', 'fsdp'), None))\n",
    "\n",
    "    # Shard the input.\n",
    "    # Scale the batch size with num_devices since there will be only one\n",
    "    # process that handles all runtime devices.\n",
    "    self.batch_size *= num_devices\n",
    "    train_loader = xu.SampleGenerator(\n",
    "        data=(torch.randint(\n",
    "            0,\n",
    "            self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "            dtype=torch.int64,\n",
    "            device='cpu'),\n",
    "              torch.randint(\n",
    "                  0,\n",
    "                  self.config.vocab_size, (self.batch_size, self.seq_len),\n",
    "                  dtype=torch.int64,\n",
    "                  device='cpu')),\n",
    "        sample_count=self.train_dataset_len // self.batch_size)\n",
    "    self.train_device_loader = pl.MpDeviceLoader(\n",
    "        train_loader,\n",
    "        self.device,\n",
    "        # Shard the input's batch dimension along the `fsdp` axis, no sharding along other dimensions\n",
    "        input_sharding=xs.ShardingSpec(spmd_mesh, ('fsdp', None)))  # type:ignore\n",
    "    \n",
    "    # Apply checkpoint to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = checkpoint_module(block)\n",
    "        \n",
    "    # Apply offloading to each DecoderLayer layer.\n",
    "    from torch_xla.distributed.fsdp import checkpoint_module\n",
    "    for i, block in enumerate(self.model.layers):\n",
    "        self.model.layers[i] = OffloadingModule(block)\n",
    "\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.00001)\n",
    "    torch_xla.sync(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-slice sharding: mesh=(2, 4)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "xr.use_spmd()\n",
    "base = TrainDecoderOnlyFSDPv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n",
      "Epoch 1 train begin  6:24AM UTC on Nov 10, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs), \\\n",
      "/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:184: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n",
      "https://symbolize.corp.google.com/r/?trace=7987c70fb0ad,7995a9c49dcf,7987c70fa2bc,7987c71020c3,7987cc08cc11,7987cc08c678,7987c70a6ee1,7987c709ceb0,7987c97c41b2,7987ceaf4d12,7987ceafaed5,7987ceb03a44,7987cec9f602,7995a9bf6ea6&map= \n",
      "*** SIGSEGV (@0x18), see go/stacktraces#s15 received by PID 14024 (TID 14962) on cpu 90; stack trace: ***\n",
      "PC: @     0x7987c70fb0ad  (unknown)  xla::jellyfish::(anonymous namespace)::HasActivationSemantics()\n",
      "    @     0x7987cedc80e1       1888  FailureSignalHandler()\n",
      "    @     0x7995a9c49dd0  (unknown)  (unknown)\n",
      "    @     0x7987c70fa2bd        384  xla::jellyfish::AllReduceDecomposer::TryMatchAllGatherEinsumOrEinsumReduceScatter()\n",
      "    @     0x7987c71020c4        400  xla::jellyfish::AllReduceDecomposer::Run()\n",
      "    @     0x7987cc08cc12        416  xla::HloPassPipeline::RunPassesInternal<>()\n",
      "    @     0x7987cc08c679         80  xla::HloPassPipeline::Run()\n",
      "    @     0x7987c70a6ee2       1120  xla::jellyfish::(anonymous namespace)::Phase2PreLayoutAssignment()\n",
      "    @     0x7987c709ceb1       1472  xla::jellyfish::(anonymous namespace)::HloOptimize()::$_0::operator()()\n",
      "    @     0x7987c97c41b3         32  absl::internal_any_invocable::LocalInvoker<>()\n",
      "    @     0x7987ceaf4d13        176  thread::Fiber::Body()\n",
      "    @     0x7987ceafaed6        112  thread::(anonymous namespace)::FutexDomainThread::WorkLoop()\n",
      "    @     0x7987ceb03a45         32  thread::CommonFiberDomainThread::Run()\n",
      "    @     0x7987cec9f603        256  Thread::ThreadBody()\n",
      "    @     0x7995a9bf6ea7  (unknown)  start_thread\n",
      "https://symbolize.corp.google.com/r/?trace=7987c70fb0ad,7987cedc80e0,7995a9c49dcf,7987c70fa2bc,7987c71020c3,7987cc08cc11,7987cc08c678,7987c70a6ee1,7987c709ceb0,7987c97c41b2,7987ceaf4d12,7987ceafaed5,7987ceb03a44,7987cec9f602,7995a9bf6ea6&map= \n",
      "E1110 06:24:29.184603   14962 coredump_hook.cc:301] RAW: Remote crash data gathering hook invoked.\n",
      "E1110 06:24:29.184611   14962 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\n",
      "E1110 06:24:29.184613   14962 coredump_hook.cc:396] RAW: Sending fingerprint to remote end.\n",
      "E1110 06:24:29.184637   14962 coredump_hook.cc:405] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\n",
      "E1110 06:24:29.184640   14962 coredump_hook.cc:457] RAW: Dumping core locally.\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling model\")\n",
    "base.num_steps = 3\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "print(\"Profiling model\")\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)\n",
    "xp.trace_detached(\n",
    "    service_addr=\"localhost:9012\", logdir=\"profile/\", duration_ms=15000)\n",
    "base.num_steps = 5\n",
    "base.start_training()\n",
    "torch_xla.sync(wait=True)\n",
    "del server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
