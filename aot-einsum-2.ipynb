{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an AOTAutograd friendly version of einsum that won't be decomposed into\n",
    "views and transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from torch_xla.core.xla_model import XLA_LIB\n",
    "from torch.library import impl, custom_op\n",
    "\n",
    "# Custom forward op: uses einsum internally\n",
    "@custom_op(\"xla::custom_linear_forward\", schema=\"(Tensor input, Tensor weight, Tensor? bias) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch.einsum('...n,mn->...m', input, weight)\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "\n",
    "@custom_linear_forward.register_fake\n",
    "def custom_linear_forward_fake(input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "    product = torch.einsum('...n,mn->...m', input, weight)\n",
    "    if bias is not None:\n",
    "        return product + bias\n",
    "    return product\n",
    "\n",
    "@custom_op(\"xla::custom_linear_backward1\", schema=\"(Tensor grad_output, Tensor input, Tensor weight, Tensor? bias, bool needs_input_grad_input, bool needs_input_grad_weight, bool needs_input_grad_bias) -> (Tensor, Tensor, Tensor)\", mutates_args=())\n",
    "def custom_linear_backward1(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "    if needs_input_grad_input:\n",
    "        # This doesn't work\n",
    "        grad_input = torch.einsum('...m,mn->...n', grad_output, weight)\n",
    "        print(f\"grad_input = grad_output @ weight\")\n",
    "        print(f\"grad_output = {grad_output}\")\n",
    "        print(f\"weight = {weight}\")\n",
    "        print(f\"grad_input = {grad_input}\")\n",
    "\n",
    "        # This works\n",
    "        grad_input = grad_output @ weight\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "\n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch.einsum('...m,...n->mn', grad_output, input)\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "\n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch.einsum('...m->m', grad_output)\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "@custom_linear_backward1.register_fake\n",
    "def custom_linear_backward_fake(\n",
    "    grad_output: Tensor,\n",
    "    input: Tensor,\n",
    "    weight: Tensor,\n",
    "    bias: Optional[Tensor],\n",
    "    needs_input_grad_input: bool,\n",
    "    needs_input_grad_weight: bool,\n",
    "    needs_input_grad_bias: bool\n",
    "):\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "    if needs_input_grad_input:\n",
    "        grad_input = torch.einsum('...m,mn->...n', grad_output, weight)\n",
    "        print(f\"grad_input = grad_output @ weight\")\n",
    "        print(f\"grad_output = {grad_output}\")\n",
    "        print(f\"weight = {weight}\")\n",
    "        print(f\"grad_input = {grad_input}\")\n",
    "    else:\n",
    "        grad_input = torch.zeros_like(input)\n",
    "    \n",
    "    if needs_input_grad_weight:\n",
    "        grad_weight = torch.einsum('...m,...n->mn', grad_output, input)\n",
    "    else:\n",
    "        grad_weight = torch.zeros_like(weight)\n",
    "\n",
    "    if bias is not None and needs_input_grad_bias:\n",
    "        grad_bias = torch.einsum('...m->m', grad_output)\n",
    "    else:\n",
    "        grad_bias = torch.zeros((weight.size(0),), dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n",
    "\n",
    "# Now define the XLAPatchedLinear function that uses the custom ops\n",
    "class XLAPatchedLinear(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A patched version of `torch.nn.functional.linear` that uses einsum via custom ops.\n",
    "    By wrapping these calls in custom ops, AOTAutograd won't decompose einsum.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        # Call our custom forward op\n",
    "        return torch.ops.xla.custom_linear_forward(input, weight, bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        needs_input_grad_input = ctx.needs_input_grad[0]\n",
    "        needs_input_grad_weight = ctx.needs_input_grad[1]\n",
    "        needs_input_grad_bias = False\n",
    "        if bias is not None:\n",
    "            needs_input_grad_bias = ctx.needs_input_grad[2]\n",
    "\n",
    "        print(f\"grad_output = {grad_output}\")\n",
    "        print(f\"input = {input}\")\n",
    "        print(f\"weight = {weight}\")\n",
    "        print(f\"bias = {bias}\")\n",
    "\n",
    "        # Call our custom backward op with the boolean flags\n",
    "        grad_input, grad_weight, grad_bias = torch.ops.xla.custom_linear_backward1(\n",
    "            grad_output,\n",
    "            input,\n",
    "            weight,\n",
    "            bias,\n",
    "            needs_input_grad_input,\n",
    "            needs_input_grad_weight,\n",
    "            needs_input_grad_bias\n",
    "        )\n",
    "        return grad_input, grad_weight, grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[ 1.3808,  0.8378,  0.0665],\n",
      "        [ 0.1580,  0.9724, -0.1232]], device='xla:0', requires_grad=True) tensor([[-1.8650, -0.2681,  1.7381],\n",
      "        [-0.6676,  0.2368, -0.7860],\n",
      "        [-0.2759, -0.5643, -0.3005],\n",
      "        [-1.5966,  1.1333, -0.5190]], device='xla:0', requires_grad=True) tensor([ 0.5979, -0.8140,  0.1973, -1.6715], device='xla:0',\n",
      "       requires_grad=True)\n",
      "grad_output = tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='xla:0')\n",
      "input = tensor([[ 1.3808,  0.8378,  0.0665],\n",
      "        [ 0.1580,  0.9724, -0.1232]], device='xla:0', requires_grad=True)\n",
      "weight = tensor([[-1.8650, -0.2681,  1.7381],\n",
      "        [-0.6676,  0.2368, -0.7860],\n",
      "        [-0.2759, -0.5643, -0.3005],\n",
      "        [-1.5966,  1.1333, -0.5190]], device='xla:0', requires_grad=True)\n",
      "bias = tensor([ 0.5979, -0.8140,  0.1973, -1.6715], device='xla:0',\n",
      "       requires_grad=True)\n",
      "grad_input = grad_output @ weight\n",
      "grad_output = tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='xla:0')\n",
      "weight = tensor([[-1.8650, -0.2681,  1.7381],\n",
      "        [-0.6676,  0.2368, -0.7860],\n",
      "        [-0.2759, -0.5643, -0.3005],\n",
      "        [-1.5966,  1.1333, -0.5190]], device='xla:0')\n",
      "grad_input = tensor([[-1.0684, -1.3857, -1.2812],\n",
      "        [-1.0684, -1.3857, -1.2812]], device='xla:0')\n",
      "Outputs:\n",
      "tensor([[-2.0926, -1.5915, -0.6737, -2.9629],\n",
      "        [-0.1712, -0.5923, -0.3564, -0.7579]], device='xla:0',\n",
      "       grad_fn=<XLAPatchedLinearBackward>) tensor([[-4.4043,  0.5400,  0.1289],\n",
      "        [-4.4043,  0.5400,  0.1289]], device='xla:0') tensor([[2.2188, 0.2246, 0.8496],\n",
      "        [2.2188, 0.2246, 0.8496],\n",
      "        [2.2188, 0.2246, 0.8496],\n",
      "        [2.2188, 0.2246, 0.8496]], device='xla:0') tensor([2., 2., 2., 2.], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "with torch_xla.runtime.xla_device():\n",
    "  x0 = torch.randn(2, 3, requires_grad=True)\n",
    "  w0 = torch.randn(4, 3, requires_grad=True)\n",
    "  b0 = torch.randn(4, requires_grad=True)\n",
    "  torch_xla.sync()\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(x0, w0, b0)\n",
    "\n",
    "x = x0.clone().detach().requires_grad_()\n",
    "w = w0.clone().detach().requires_grad_()\n",
    "b = b0.clone().detach().requires_grad_()\n",
    "\n",
    "# Run forward\n",
    "y = XLAPatchedLinear.apply(x, w, b)\n",
    "loss = y.sum()\n",
    "# Run backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"Outputs:\")\n",
    "print(y, x.grad, w.grad, b.grad)\n",
    "\n",
    "y0, xg0, wg0, bg0 = y.clone().detach(), x.grad.clone().detach(), w.grad.clone().detach(), b.grad.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[ 1.3808,  0.8378,  0.0665],\n",
      "        [ 0.1580,  0.9724, -0.1232]], device='xla:0', requires_grad=True) tensor([[-1.8650, -0.2681,  1.7381],\n",
      "        [-0.6676,  0.2368, -0.7860],\n",
      "        [-0.2759, -0.5643, -0.3005],\n",
      "        [-1.5966,  1.1333, -0.5190]], device='xla:0', requires_grad=True) tensor([ 0.5979, -0.8140,  0.1973, -1.6715], device='xla:0',\n",
      "       requires_grad=True)\n",
      "Outputs:\n",
      "tensor([[-2.0926, -1.5915, -0.6737, -2.9629],\n",
      "        [-0.1712, -0.5923, -0.3564, -0.7579]], device='xla:0',\n",
      "       grad_fn=<XLAPatchedLinearBackward>) tensor([[-4.4043,  0.5400,  0.1289],\n",
      "        [-4.4043,  0.5400,  0.1289]], device='xla:0') tensor([[ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566]], device='xla:0') tensor([2., 2., 2., 2.], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "from torch_xla.distributed.spmd.xla_sharding import XLAPatchedLinear as OGXLAPatchedLinear\n",
    "import torch_xla.runtime\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(x0, w0, b0)\n",
    "\n",
    "x = x0.clone().detach().requires_grad_()\n",
    "w = w0.clone().detach().requires_grad_()\n",
    "b = b0.clone().detach().requires_grad_()\n",
    "\n",
    "# Run forward\n",
    "y = OGXLAPatchedLinear.apply(x, w, b)\n",
    "loss = y.sum()\n",
    "# Run backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"Outputs:\")\n",
    "print(y, x.grad, w.grad, b.grad)\n",
    "\n",
    "y1, xg1, wg1, bg1 = y.clone().detach(), x.grad.clone().detach(), w.grad.clone().detach(), b.grad.clone().detach()\n",
    "torch.testing.assert_close(y0, y1)\n",
    "torch.testing.assert_close(xg0, xg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[ 1.3808,  0.8378,  0.0665],\n",
      "        [ 0.1580,  0.9724, -0.1232]], device='xla:0', requires_grad=True) tensor([[-1.8650, -0.2681,  1.7381],\n",
      "        [-0.6676,  0.2368, -0.7860],\n",
      "        [-0.2759, -0.5643, -0.3005],\n",
      "        [-1.5966,  1.1333, -0.5190]], device='xla:0', requires_grad=True) tensor([ 0.5979, -0.8140,  0.1973, -1.6715], device='xla:0',\n",
      "       requires_grad=True)\n",
      "Outputs:\n",
      "tensor([[-2.0926, -1.5915, -0.6737, -2.9629],\n",
      "        [-0.1712, -0.5923, -0.3564, -0.7579]], device='xla:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([[-4.4043,  0.5400,  0.1289],\n",
      "        [-4.4043,  0.5400,  0.1289]], device='xla:0') tensor([[ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566],\n",
      "        [ 1.5410,  1.8086, -0.0566]], device='xla:0') tensor([2., 2., 2., 2.], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.runtime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(x0, w0, b0)\n",
    "\n",
    "x = x0.clone().detach().requires_grad_()\n",
    "w = w0.clone().detach().requires_grad_()\n",
    "b = b0.clone().detach().requires_grad_()\n",
    "\n",
    "# Run forward\n",
    "y = F.linear(x, w, b)\n",
    "loss = y.sum()\n",
    "# Run backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"Outputs:\")\n",
    "print(y, x.grad, w.grad, b.grad)\n",
    "\n",
    "y2, xg2, wg2, bg2 = y.clone().detach(), x.grad.clone().detach(), w.grad.clone().detach(), b.grad.clone().detach()\n",
    "torch.testing.assert_close(y0, y2)\n",
    "torch.testing.assert_close(xg1, xg2)\n",
    "torch.testing.assert_close(xg0, xg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Graph ===\n",
      "<lambda>()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    custom_linear_forward = torch.ops.xla.custom_linear_forward.default(arg0_1, arg1_1, None);  arg0_1 = arg1_1 = None\n",
      "    return (custom_linear_forward,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "=== Output ===\n",
      "tensor([[ 1.3572, -1.4180,  0.3893],\n",
      "        [-1.2944, -0.0214,  0.0524],\n",
      "        [-1.0056, -0.4093, -1.5266]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "# A custom compiler function that prints the graph.\n",
    "def print_graph(gm, sample_inputs):\n",
    "    # Print the FX Graph to observe the operations after decomposition\n",
    "    print(\"=== Generated Graph ===\")\n",
    "    print(gm)\n",
    "    return gm.forward\n",
    "\n",
    "def my_einsum_func(x, y):\n",
    "    # A simple einsum expression to test decomposition\n",
    "    return XLAPatchedLinear.apply(x, y)\n",
    "\n",
    "# Wrap the function with aot_function, using our custom compilers that print the graph\n",
    "compiled_func = aot_function(\n",
    "    my_einsum_func,\n",
    "    fw_compiler=print_graph,\n",
    "    bw_compiler=print_graph\n",
    ")\n",
    "\n",
    "# Run the compiled function with sample inputs\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x = torch.randn(3, 3)\n",
    "  y = torch.randn(3, 3)\n",
    "  out = compiled_func(x, y)\n",
    "\n",
    "print(\"=== Output ===\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the HLO lowering of einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::as_strided(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[3,3,1]{2,1,0} aten::as_strided(%1), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %3 = f32[1,3,3]{2,1,0} aten::view(%2), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %4 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %5 = f32[3,3,1]{2,1,0} aten::as_strided(%4), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::as_strided(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[1,3,3]{2,1,0} aten::view(%6), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %8 = f32[1,3,3]{2,1,0} aten::matmul(%7, %3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %9 = f32[3,1,3]{2,1,0} aten::view(%8), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %10 = f32[3,3,1]{2,1,0} aten::as_strided(%9), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %11 = f32[3,3]{1,0} aten::view(%10), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.16, entry_computation_layout={(f32[3,3]{1,0}, f32[3,3]{1,0})->(f32[3,3]{1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.16 (p0.1: f32[3,3], p1.6: f32[3,3]) -> (f32[3,3]) {\n",
      "  %p1.6 = f32[3,3]{1,0} parameter(1)\n",
      "  %reshape.7 = f32[3,3,1]{2,1,0} reshape(f32[3,3]{1,0} %p1.6)\n",
      "  %reshape.8 = f32[3,1,3]{2,1,0} reshape(f32[3,3,1]{2,1,0} %reshape.7)\n",
      "  %transpose.9 = f32[3,3,1]{1,2,0} transpose(f32[3,1,3]{2,1,0} %reshape.8), dimensions={0,2,1}\n",
      "  %reshape.10 = f32[1,3,3]{2,1,0} reshape(f32[3,3,1]{1,2,0} %transpose.9)\n",
      "  %p0.1 = f32[3,3]{1,0} parameter(0)\n",
      "  %reshape.2 = f32[3,3,1]{2,1,0} reshape(f32[3,3]{1,0} %p0.1)\n",
      "  %reshape.3 = f32[1,3,3]{2,1,0} reshape(f32[3,3,1]{2,1,0} %reshape.2)\n",
      "  %transpose.4 = f32[3,3,1]{0,1,2} transpose(f32[1,3,3]{2,1,0} %reshape.3), dimensions={2,1,0}\n",
      "  %reshape.5 = f32[1,3,3]{2,1,0} reshape(f32[3,3,1]{0,1,2} %transpose.4)\n",
      "  %dot.11 = f32[1,3,3]{2,1,0} dot(f32[1,3,3]{2,1,0} %reshape.10, f32[1,3,3]{2,1,0} %reshape.5), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n",
      "  %reshape.12 = f32[3,1,3]{2,1,0} reshape(f32[1,3,3]{2,1,0} %dot.11)\n",
      "  %transpose.13 = f32[3,3,1]{1,2,0} transpose(f32[3,1,3]{2,1,0} %reshape.12), dimensions={0,2,1}\n",
      "  %reshape.14 = f32[3,3]{1,0} reshape(f32[3,3,1]{1,2,0} %transpose.13)\n",
      "  ROOT %tuple.15 = (f32[3,3]{1,0}) tuple(f32[3,3]{1,0} %reshape.14)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.runtime\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x = x.to('xla')\n",
    "  y = y.to('xla')\n",
    "  out = compiled_func(x, y)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
