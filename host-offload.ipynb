{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LIBTPU_INIT_ARGS=--xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=24 --xla_tpu_aggressive_opt_barrier_removal=ENABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2\n"
     ]
    }
   ],
   "source": [
    "# These are the offload flags used by MaxText.\n",
    "%env LIBTPU_INIT_ARGS=--xla_tpu_enable_all_experimental_scheduler_features=true --xla_tpu_enable_scheduler_memory_pressure_tracking=true --xla_tpu_host_transfer_overlap_limit=24 --xla_tpu_aggressive_opt_barrier_removal=ENABLED --xla_lhs_prioritize_async_depth_over_stall=ENABLED --xla_tpu_enable_ag_backward_pipelining=true --xla_should_allow_loop_variant_parameter_in_chain=ENABLED --xla_should_add_loop_invariant_op_in_chain=ENABLED --xla_max_concurrent_host_send_recv=100 --xla_tpu_scheduler_percent_shared_memory_limit=100 --xla_latency_hiding_scheduler_rerun=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "from torch_xla.experimental.stablehlo_custom_call import (\n",
    "  place_to_host, place_to_device\n",
    ")\n",
    "from torch.autograd.graph import saved_tensors_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch_xla.runtime.xla_device():\n",
    "  x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "  with saved_tensors_hooks(place_to_host, place_to_device):\n",
    "    a = torch.sin(x)\n",
    "    a = torch.sin(a)\n",
    "    a.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %1 = f32[10]{0} aten::expand(%0), xla_shape=f32[10]{0}\n",
      "  %2 = (f32[10]{0}) xla::custom_call(%1), xla_shape=(f32[10]{0})\n",
      "  %3 = (f32[10]{0}) xla::custom_call(%2), xla_shape=(f32[10]{0})\n",
      "  %4 = f32[10]{0} aten::cos(%3), xla_shape=f32[10]{0}\n",
      "  %5 = f32[10]{0} aten::sin(%1), xla_shape=f32[10]{0}\n",
      "  %6 = (f32[10]{0}) xla::custom_call(%5), xla_shape=(f32[10]{0})\n",
      "  %7 = (f32[10]{0}) xla::custom_call(%6), xla_shape=(f32[10]{0})\n",
      "  %8 = f32[10]{0} aten::cos(%7), xla_shape=f32[10]{0}\n",
      "  %9 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %10 = f32[] aten::expand(%9), xla_shape=f32[]\n",
      "  %11 = f32[10]{0} aten::expand(%10), xla_shape=f32[10]{0}\n",
      "  %12 = f32[10]{0} aten::mul(%11, %8), xla_shape=f32[10]{0}\n",
      "  %13 = f32[10]{0} aten::mul(%12, %4), xla_shape=f32[10]{0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch_xla._XLAC._get_xla_tensors_text([x.grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3600, 0.3600, 0.3600, 0.3600, 0.3600, 0.3600, 0.3600, 0.3600, 0.3600,\n",
       "        0.3600], device='xla:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_xla.sync()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test host offloading with optimization barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd.function import Function\n",
    "from collections.abc import Iterable\n",
    "import torch_xla.core.xla_model as xm\n",
    "import itertools\n",
    "\n",
    "\n",
    "def _extract_tensors_from_list(inputs):\n",
    "  tensor_inputs = []\n",
    "  if torch.is_tensor(inputs):\n",
    "    tensor_inputs.append(inputs)\n",
    "  # tensor is Iterable so we need to avoid iterating through tensor\n",
    "  elif isinstance(inputs, Iterable):\n",
    "    for input in inputs:\n",
    "      if torch.is_tensor(input):\n",
    "        tensor_inputs.append(input)\n",
    "  return tensor_inputs\n",
    "\n",
    "\n",
    "def offload1(module: torch.nn.Module):\n",
    "  \"\"\"Collate all intermediate tensors into an optimization barrier op.\"\"\"\n",
    "\n",
    "  def offloaded_fn(*args):\n",
    "    tensor_store = {}\n",
    "    counter = 0\n",
    "\n",
    "    def pack_fn(tensor: torch.Tensor) -> int:\n",
    "      nonlocal counter\n",
    "      idx = counter\n",
    "      counter = counter + 1\n",
    "\n",
    "      tensor_store[idx] = place_to_host(tensor)\n",
    "      # print(f\"Packing tensor {idx}\")\n",
    "      return idx\n",
    "\n",
    "    def unpack_fn(idx: int) -> torch.Tensor:\n",
    "      # print(f\"Unpacking tensor {idx}\")\n",
    "      return place_to_device(tensor_store[idx])\n",
    "\n",
    "    with saved_tensors_hooks(pack_fn, unpack_fn):\n",
    "      out = module(*args)\n",
    "\n",
    "    # Prevent offload operations from being moved after the output.\n",
    "    xm.optimization_barrier_(\n",
    "        _extract_tensors_from_list(itertools.chain(tensor_store.values(), out)))\n",
    "    return out\n",
    "\n",
    "  return offloaded_fn\n",
    "\n",
    "\n",
    "def offload2(module: torch.nn.Module):\n",
    "  \"\"\"Collate all intermediate tensors into an optimization barrier op,\n",
    "  and also clear the tensor store when done with those intermediate tensors.\n",
    "  \"\"\"\n",
    "\n",
    "  def offloaded_fn(*args):\n",
    "    tensor_store = {}\n",
    "    counter = [0]  # Use list to make it mutable within closures\n",
    "    refcount = [0]  # Manual reference counter\n",
    "\n",
    "    def pack_fn(tensor: torch.Tensor):\n",
    "      idx = counter[0]\n",
    "      counter[0] += 1\n",
    "      tensor_store[idx] = place_to_host(tensor)\n",
    "      # Increase the reference count\n",
    "      refcount[0] += 1\n",
    "\n",
    "      class PackedTensor:\n",
    "\n",
    "        def __init__(self, index):\n",
    "          self.index = index\n",
    "\n",
    "        def __del__(self):\n",
    "          # Decrease the reference count\n",
    "          refcount[0] -= 1\n",
    "          if refcount[0] == 0:\n",
    "            # Clear tensor_store when refcount reaches zero\n",
    "            tensor_store.clear()\n",
    "\n",
    "      return PackedTensor(idx)\n",
    "\n",
    "    def unpack_fn(packed_tensor):\n",
    "      idx = packed_tensor.index\n",
    "      return place_to_device(tensor_store[idx])\n",
    "\n",
    "    with saved_tensors_hooks(pack_fn, unpack_fn):\n",
    "      out = module(*args)\n",
    "\n",
    "    # Prevent offload operations from being moved after the output.\n",
    "    xm.optimization_barrier_(\n",
    "        _extract_tensors_from_list(itertools.chain(tensor_store.values(), out)))\n",
    "    return out\n",
    "\n",
    "  return offloaded_fn\n",
    "\n",
    "\n",
    "def offload3(module: torch.nn.Module):\n",
    "  \"\"\"No optimization barrier. Simply move tensors between host and device.\"\"\"\n",
    "\n",
    "  def offloaded_fn(*args):\n",
    "\n",
    "    def pack_fn(tensor: torch.Tensor):\n",
    "      return place_to_host(tensor)\n",
    "\n",
    "    def unpack_fn(input) -> torch.Tensor:\n",
    "      return place_to_device(input)\n",
    "\n",
    "    with saved_tensors_hooks(pack_fn, unpack_fn):\n",
    "      out = module(*args)\n",
    "\n",
    "    return out\n",
    "\n",
    "  return offloaded_fn\n",
    "\n",
    "\n",
    "def offload4(module: torch.nn.Module):\n",
    "  \"\"\"Each intermediate activation tensor gets its own optimization barrier.\"\"\"\n",
    "\n",
    "  def offloaded_fn(*args):\n",
    "\n",
    "    def pack_fn(tensor: torch.Tensor):\n",
    "      t = place_to_host(tensor)\n",
    "      xm.optimization_barrier_([t])\n",
    "      return t\n",
    "\n",
    "    def unpack_fn(input) -> torch.Tensor:\n",
    "      return place_to_device(input)\n",
    "\n",
    "    with saved_tensors_hooks(pack_fn, unpack_fn):\n",
    "      out = module(*args)\n",
    "\n",
    "    return out\n",
    "\n",
    "  return offloaded_fn\n",
    "\n",
    "\n",
    "from torch_xla.distributed.spmd.xla_sharding import apply_backward_optimization_barrier\n",
    "\n",
    "\n",
    "def offload5(module: torch.nn.Module):\n",
    "  \"\"\"Use an optimization barrier to tie together the transfer ops with the backward input.\"\"\"\n",
    "\n",
    "  counter = 0\n",
    "  tensor_store = {}\n",
    "  moved = False\n",
    "\n",
    "\n",
    "  def offloaded_fn(*args):\n",
    "    nonlocal counter\n",
    "    nonlocal tensor_store\n",
    "    nonlocal moved\n",
    "\n",
    "    def pack_fn(tensor: torch.Tensor) -> int:\n",
    "      nonlocal counter\n",
    "      nonlocal tensor_store\n",
    "      nonlocal moved\n",
    "\n",
    "      moved = False\n",
    "\n",
    "      # Record the tensor to some list\n",
    "      idx = counter\n",
    "      tensor_store[idx] = tensor\n",
    "      counter = counter + 1\n",
    "      print(f\"Packing tensor {idx}\")\n",
    "      return idx\n",
    "\n",
    "    def unpack_fn(idx: int) -> torch.Tensor:\n",
    "      print(f\"Unpacking tensor {idx}\")\n",
    "      return tensor_store[idx]\n",
    "\n",
    "    # Too late.\n",
    "    # torch.nn.modules.module.register_module_full_backward_hook(transfer_and_add_optimization_barrier)\n",
    "\n",
    "    # Too late.\n",
    "    # module.register_full_backward_hook(transfer_and_add_optimization_barrier, prepend=True)\n",
    "\n",
    "    with saved_tensors_hooks(pack_fn, unpack_fn):\n",
    "      out = module._xla_checkpointed_forward_original(*args)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def transfer_and_add_optimization_barrier(module, grad_output):\n",
    "    nonlocal tensor_store\n",
    "    nonlocal moved\n",
    "    nonlocal counter\n",
    "\n",
    "    if moved:\n",
    "      raise RuntimeError(\"Already moved once during a previous transfer_and_add_optimization_barrier\")\n",
    "\n",
    "    # Transfer all tensors to host\n",
    "    for k, v in tensor_store.items():\n",
    "      print(f\"Place tensor {k} to host\")\n",
    "      tensor_store[k] = place_to_host(v)\n",
    "\n",
    "    # Wrap with barrier\n",
    "    from torch_xla.utils.checkpoint import CheckpointFunction\n",
    "    gradients = []\n",
    "    for param in module.parameters():\n",
    "      if param.grad != None:\n",
    "        gradients.append(param.grad)\n",
    "    print(f\"Add optimization barrier\")\n",
    "    xm.optimization_barrier_(\n",
    "        CheckpointFunction._extract_tensors_from_list(list(tensor_store.values()) +\n",
    "                                                      gradients +\n",
    "                                                      list(grad_output)))\n",
    "\n",
    "    # Transfer all tensor to device\n",
    "    for k, v in tensor_store.items():\n",
    "      print(f\"Place tensor {k} to device\")\n",
    "      tensor_store[k] = place_to_device(v)\n",
    "\n",
    "    moved = True\n",
    "    counter = 0\n",
    "\n",
    "    # Return the modified grad_output\n",
    "    return tuple(grad_output)\n",
    "\n",
    "  # Just right.\n",
    "  module.register_full_backward_pre_hook(transfer_and_add_optimization_barrier, prepend=True)\n",
    "\n",
    "  def _xla_checkpointed_forward_no_kwargs(m, num_args, num_kwargs,\n",
    "                                          *packed_args):\n",
    "    # unpack packed_args into args and kwargs\n",
    "    assert num_args + num_kwargs * 2 == len(packed_args)\n",
    "    args = packed_args[:num_args]\n",
    "    kwargs = packed_args[num_args:]\n",
    "    kwargs = dict(zip(kwargs[:num_kwargs], kwargs[num_kwargs:]))\n",
    "    return m._xla_checkpointed_forward_original(*args, **kwargs)\n",
    "\n",
    "  def _forward_with_checkpoint(m, *args, **kwargs):\n",
    "    # pack args and kwargs together as `torch_xla.utils.checkpoint.checkpoint`\n",
    "    # doesn't support keyword arguments\n",
    "    packed_args = args + tuple(kwargs.keys()) + tuple(kwargs.values())\n",
    "    input_requires_grad = any(\n",
    "        isinstance(t, torch.Tensor) and t.requires_grad for t in packed_args)\n",
    "    if input_requires_grad:\n",
    "      outputs = offloaded_fn(len(args), len(kwargs), *packed_args)\n",
    "    else:\n",
    "      # No input requires gradients so we won't checkpoint this forward pass.\n",
    "      # Note that `m`` might have parameters that require gradients, but they\n",
    "      # are beyond what `torch_xla.utils.checkpoint.checkpoint` can handle.\n",
    "      outputs = m._xla_checkpointed_forward_original(*args, **kwargs)\n",
    "    return outputs\n",
    "\n",
    "  assert isinstance(module, torch.nn.Module)\n",
    "  # replace `module`'s forward method with its checkpointed version\n",
    "  module._xla_checkpointed_forward_original = module.forward  # type: ignore\n",
    "  module._xla_checkpointed_forward_no_kwargs = MethodType(    # type: ignore\n",
    "      _xla_checkpointed_forward_no_kwargs, module)\n",
    "  module.forward = MethodType(_forward_with_checkpoint, module)\n",
    "  return module\n",
    "\n",
    "\n",
    "# Test different approaches\n",
    "offload = offload5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m layer \u001b[38;5;241m=\u001b[39m offload(layer)\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 239\u001b[0m, in \u001b[0;36moffload5.<locals>._forward_with_checkpoint\u001b[0;34m(m, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m input_requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(t, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m packed_args)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_requires_grad:\n\u001b[0;32m--> 239\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43moffloaded_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpacked_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;66;03m# No input requires gradients so we won't checkpoint this forward pass.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;66;03m# Note that `m`` might have parameters that require gradients, but they\u001b[39;00m\n\u001b[1;32m    243\u001b[0m   \u001b[38;5;66;03m# are beyond what `torch_xla.utils.checkpoint.checkpoint` can handle.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39m_xla_checkpointed_forward_original(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[24], line 180\u001b[0m, in \u001b[0;36moffload5.<locals>.offloaded_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Too late.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# torch.nn.modules.module.register_module_full_backward_hook(transfer_and_add_optimization_barrier)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Too late.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# module.register_full_backward_hook(transfer_and_add_optimization_barrier, prepend=True)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saved_tensors_hooks(pack_fn, unpack_fn):\n\u001b[0;32m--> 180\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_checkpointed_forward_original\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "with torch_xla.runtime.xla_device():\n",
    "  layer = torch.nn.Linear(10, 10)\n",
    "  orig_layer = layer\n",
    "  layer = offload(layer)\n",
    "\n",
    "  x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "  y = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place tensor 0 to host\n",
      "Place tensor 1 to host\n",
      "Add optimization barrier\n",
      "Place tensor 0 to device\n",
      "Place tensor 1 to device\n",
      "Unpacking tensor 0\n",
      "Unpacking tensor 1\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule IrToHlo.54, entry_computation_layout={(s64[], f32[], f32[])->(f32[10,10]{0,1})}\n",
      "\n",
      "ENTRY %IrToHlo.54 (p0.14: s64[], p1.19: f32[], p2.20: f32[]) -> (f32[10,10]) {\n",
      "  %constant.17 = s64[] constant(2531011)\n",
      "  %constant.15 = s64[] constant(214013)\n",
      "  %p0.14 = s64[] parameter(0)\n",
      "  %multiply.16 = s64[] multiply(s64[] %constant.15, s64[] %p0.14)\n",
      "  %add.18 = s64[] add(s64[] %constant.17, s64[] %multiply.16)\n",
      "  %convert.21 = u64[] convert(s64[] %add.18)\n",
      "  %reshape.23 = u64[1]{0} reshape(u64[] %convert.21)\n",
      "  %constant.22 = u64[] constant(0)\n",
      "  %reshape.24 = u64[1]{0} reshape(u64[] %constant.22)\n",
      "  %concatenate.25 = u64[2]{0} concatenate(u64[1]{0} %reshape.23, u64[1]{0} %reshape.24), dimensions={0}\n",
      "  %rng-bit-generator.26 = (u64[2]{0}, u32[10,10]{1,0}) rng-bit-generator(u64[2]{0} %concatenate.25), algorithm=rng_default\n",
      "  %get-tuple-element.28 = u64[2]{0} get-tuple-element((u64[2]{0}, u32[10,10]{1,0}) %rng-bit-generator.26), index=0\n",
      "  %get-tuple-element.27 = u32[10,10]{1,0} get-tuple-element((u64[2]{0}, u32[10,10]{1,0}) %rng-bit-generator.26), index=1\n",
      "  %constant.29 = u32[] constant(9)\n",
      "  %broadcast.30 = u32[10,10]{1,0} broadcast(u32[] %constant.29), dimensions={}\n",
      "  %shift-right-logical.31 = u32[10,10]{1,0} shift-right-logical(u32[10,10]{1,0} %get-tuple-element.27, u32[10,10]{1,0} %broadcast.30)\n",
      "  %convert.32 = f32[10,10]{1,0} convert(u32[10,10]{1,0} %shift-right-logical.31)\n",
      "  %constant.33 = f32[] constant(1.1920929e-07)\n",
      "  %broadcast.34 = f32[10,10]{1,0} broadcast(f32[] %constant.33), dimensions={}\n",
      "  %multiply.35 = f32[10,10]{1,0} multiply(f32[10,10]{1,0} %convert.32, f32[10,10]{1,0} %broadcast.34)\n",
      "  %p1.19 = f32[] parameter(1)\n",
      "  %p2.20 = f32[] parameter(2)\n",
      "  %subtract.36 = f32[] subtract(f32[] %p1.19, f32[] %p2.20)\n",
      "  %broadcast.37 = f32[10,10]{1,0} broadcast(f32[] %subtract.36), dimensions={}\n",
      "  %multiply.38 = f32[10,10]{1,0} multiply(f32[10,10]{1,0} %multiply.35, f32[10,10]{1,0} %broadcast.37)\n",
      "  %broadcast.39 = f32[10,10]{1,0} broadcast(f32[] %p2.20), dimensions={}\n",
      "  %add.40 = f32[10,10]{1,0} add(f32[10,10]{1,0} %multiply.38, f32[10,10]{1,0} %broadcast.39)\n",
      "  %transpose.41 = f32[10,10]{0,1} transpose(f32[10,10]{1,0} %add.40), dimensions={1,0}\n",
      "  %custom-call.42 = f32[10,10] custom-call(f32[10,10]{0,1} %transpose.41), custom_call_target=\"annotate_device_placement\", custom_call_has_side_effect=true, api_version=API_VERSION_UNSPECIFIED, frontend_attributes={_xla_buffer_placement=\"pinned_host\"}\n",
      "  %constant.7 = f32[] constant(1)\n",
      "  %reshape.8 = f32[1]{0} reshape(f32[] %constant.7)\n",
      "  %broadcast.9 = f32[1]{0} broadcast(f32[1]{0} %reshape.8), dimensions={0}\n",
      "  %reshape.10 = f32[] reshape(f32[1]{0} %broadcast.9)\n",
      "  %broadcast.11 = f32[10]{0} broadcast(f32[] %reshape.10), dimensions={}\n",
      "  %reshape.12 = f32[1,10]{1,0} reshape(f32[10]{0} %broadcast.11)\n",
      "  %custom-call.13 = f32[1,10] custom-call(f32[1,10]{1,0} %reshape.12), custom_call_target=\"annotate_device_placement\", custom_call_has_side_effect=true, api_version=API_VERSION_UNSPECIFIED, frontend_attributes={_xla_buffer_placement=\"pinned_host\"}\n",
      "  %constant.1 = f32[] constant(1)\n",
      "  %broadcast.2 = f32[] broadcast(f32[] %constant.1), dimensions={}\n",
      "  %reshape.3 = f32[1]{0} reshape(f32[] %broadcast.2)\n",
      "  %broadcast.4 = f32[1]{0} broadcast(f32[1]{0} %reshape.3), dimensions={0}\n",
      "  %reshape.5 = f32[] reshape(f32[1]{0} %broadcast.4)\n",
      "  %broadcast.6 = f32[10]{0} broadcast(f32[] %reshape.5), dimensions={}\n",
      "  %tuple.43 = (f32[10,10], f32[1,10], f32[10]{0}) tuple(f32[10,10] %custom-call.42, f32[1,10] %custom-call.13, f32[10]{0} %broadcast.6)\n",
      "  %opt-barrier.44 = (f32[10,10], f32[1,10], f32[10]{0}) opt-barrier((f32[10,10], f32[1,10], f32[10]{0}) %tuple.43)\n",
      "  %get-tuple-element.45 = f32[10,10] get-tuple-element((f32[10,10], f32[1,10], f32[10]{0}) %opt-barrier.44), index=0\n",
      "  %get-tuple-element.46 = f32[1,10] get-tuple-element((f32[10,10], f32[1,10], f32[10]{0}) %opt-barrier.44), index=1\n",
      "  %custom-call.49 = f32[1,10] custom-call(f32[1,10] %get-tuple-element.46), custom_call_target=\"annotate_device_placement\", custom_call_has_side_effect=true, api_version=API_VERSION_UNSPECIFIED, frontend_attributes={_xla_buffer_placement=\"device\"}\n",
      "  %transpose.50 = f32[10,1] transpose(f32[1,10] %custom-call.49), dimensions={1,0}\n",
      "  %get-tuple-element.47 = f32[10]{0} get-tuple-element((f32[10,10], f32[1,10], f32[10]{0}) %opt-barrier.44), index=2\n",
      "  %reshape.48 = f32[1,10]{1,0} reshape(f32[10]{0} %get-tuple-element.47)\n",
      "  %dot.51 = f32[10,10]{1,0} dot(f32[10,1] %transpose.50, f32[1,10]{1,0} %reshape.48), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
      "  %transpose.52 = f32[10,10]{0,1} transpose(f32[10,10]{1,0} %dot.51), dimensions={1,0}\n",
      "  ROOT %tuple.53 = (f32[10,10]{0,1}) tuple(f32[10,10]{0,1} %transpose.52)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch_xla._XLAC._get_xla_tensors_hlo([orig_layer.weight.grad]))\n",
    "torch_xla.sync(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='xla:0')\n"
     ]
    }
   ],
   "source": [
    "print(orig_layer.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/torch/pytorch/xla/examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_only_model import DecoderOnlyConfig, DecoderOnlyModel\n",
    "\n",
    "device = torch_xla.device()\n",
    "config = DecoderOnlyConfig(hidden_size=1024, num_hidden_layers=40)\n",
    "config.intermediate_size = 4096\n",
    "config.vocab_size = 8192\n",
    "model = DecoderOnlyModel(config=config).to(device)\n",
    "batch_size = 16\n",
    "sequence_length = 512\n",
    "\n",
    "# Generate random input_ids within the range of the vocabulary size\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_length), device=device)\n",
    "torch_xla.sync(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch_xla.debug.profiler as xp\n",
    "server = xp.start_server(9012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to trace for 60000 ms. Remaining attempt(s): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 07:13:38.134417: W external/tsl/tsl/profiler/lib/profiler_session.cc:109] Profiling is late by 1118070 nanoseconds and will start immediately.\n"
     ]
    }
   ],
   "source": [
    "# No offload\n",
    "# Compile\n",
    "for i in range(10):\n",
    "  model.zero_grad()\n",
    "  output = model.forward(input_ids.clone())\n",
    "  output.sum().backward()\n",
    "  torch_xla.sync()\n",
    "torch_xla.sync(wait=True)\n",
    "model.zero_grad()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "# Start profiling\n",
    "xp.trace_detached(service_addr=\"localhost:9012\", logdir=\"profile/\", duration_ms=60000)\n",
    "time.sleep(1)\n",
    "for i in range(10):\n",
    "  model.zero_grad()\n",
    "  output = model.forward(input_ids.clone())\n",
    "  output.sum().backward()\n",
    "  torch_xla.sync()\n",
    "torch_xla.sync(wait=True)\n",
    "model.zero_grad()\n",
    "torch_xla.sync(wait=True)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.zero_grad()\n",
    "# model.requires_grad_(False)\n",
    "# model.layers[0].mlp.up_proj.weight.requires_grad_(True)\n",
    "# model.layers[0].self_attn.requires_grad_(True)\n",
    "# model.layers[0].requires_grad_(True)\n",
    "# model.layers[1].requires_grad_(True)\n",
    "# \n",
    "# # This doesn't work.\n",
    "# model.embed_tokens.requires_grad_(True)\n",
    "#\n",
    "# model_offloaded = offload(model)\n",
    "# output = model_offloaded(input_ids.clone())\n",
    "# output.sum().backward()\n",
    "# print(torch_xla._XLAC._get_xla_tensors_hlo([model.embed_tokens.weight.grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'register_full_backward_pre_hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     50\u001b[0m   model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m   output\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m   torch_xla\u001b[38;5;241m.\u001b[39msync()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspaces/torch/playground/decoder_only_model.py:229\u001b[0m, in \u001b[0;36mDecoderOnlyModel.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 229\u001b[0m   layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m   hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs\n\u001b[1;32m    232\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m, in \u001b[0;36moffload_module.<locals>._forward_with_checkpoint\u001b[0;34m(m, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m input_requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(t, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m packed_args)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_requires_grad:\n\u001b[0;32m---> 28\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43moffload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla_checkpointed_forward_no_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpacked_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;66;03m# No input requires gradients so we won't checkpoint this forward pass.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;66;03m# Note that `m`` might have parameters that require gradients, but they\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m# are beyond what `torch_xla.utils.checkpoint.checkpoint` can handle.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39m_xla_checkpointed_forward_original(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[15], line 209\u001b[0m, in \u001b[0;36moffload5.<locals>.offloaded_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(grad_output)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Too late.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# torch.nn.modules.module.register_module_full_backward_hook(transfer_and_add_optimization_barrier)\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Just right.\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_full_backward_pre_hook\u001b[49m(transfer_and_add_optimization_barrier, prepend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saved_tensors_hooks(pack_fn, unpack_fn):\n\u001b[1;32m    212\u001b[0m   out \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'register_full_backward_pre_hook'"
     ]
    }
   ],
   "source": [
    "# Everything offload\n",
    "# Compile\n",
    "from types import MethodType\n",
    "\n",
    "\n",
    "def offload_module(module):\n",
    "  \"\"\"\n",
    "  Wrap a `module`'s `forward` method with gradient checkpointing (also called\n",
    "  activation checkpointing) via `torch_xla.utils.checkpoint.checkpoint`.\n",
    "  \"\"\"\n",
    "\n",
    "  def _xla_checkpointed_forward_no_kwargs(m, num_args, num_kwargs,\n",
    "                                          *packed_args):\n",
    "    # unpack packed_args into args and kwargs\n",
    "    assert num_args + num_kwargs * 2 == len(packed_args)\n",
    "    args = packed_args[:num_args]\n",
    "    kwargs = packed_args[num_args:]\n",
    "    kwargs = dict(zip(kwargs[:num_kwargs], kwargs[num_kwargs:]))\n",
    "    return m._xla_checkpointed_forward_original(*args, **kwargs)\n",
    "\n",
    "  def _forward_with_checkpoint(m, *args, **kwargs):\n",
    "    # pack args and kwargs together as `torch_xla.utils.checkpoint.checkpoint`\n",
    "    # doesn't support keyword arguments\n",
    "    packed_args = args + tuple(kwargs.keys()) + tuple(kwargs.values())\n",
    "    input_requires_grad = any(\n",
    "        isinstance(t, torch.Tensor) and t.requires_grad for t in packed_args)\n",
    "    if input_requires_grad:\n",
    "      outputs = offload(m._xla_checkpointed_forward_no_kwargs)(len(args), len(kwargs), *packed_args)\n",
    "    else:\n",
    "      # No input requires gradients so we won't checkpoint this forward pass.\n",
    "      # Note that `m`` might have parameters that require gradients, but they\n",
    "      # are beyond what `torch_xla.utils.checkpoint.checkpoint` can handle.\n",
    "      outputs = m._xla_checkpointed_forward_original(*args, **kwargs)\n",
    "    return outputs\n",
    "\n",
    "  assert isinstance(module, torch.nn.Module)\n",
    "  # replace `module`'s forward method with its checkpointed version\n",
    "  module._xla_checkpointed_forward_original = module.forward  # type: ignore\n",
    "  module._xla_checkpointed_forward_no_kwargs = MethodType(    # type: ignore\n",
    "      _xla_checkpointed_forward_no_kwargs, module)\n",
    "  module.forward = MethodType(_forward_with_checkpoint, module)\n",
    "  return module\n",
    "\n",
    "\n",
    "for i, block in enumerate(model.layers):\n",
    "  model.layers[i] = offload_module(block)\n",
    "  apply_backward_optimization_barrier(model.layers[i])\n",
    "\n",
    "for i in range(10):\n",
    "  model.zero_grad()\n",
    "  output = model(input_ids.clone())\n",
    "  output.sum().backward()\n",
    "  torch_xla.sync()\n",
    "torch_xla.sync(wait=True)\n",
    "model.zero_grad()\n",
    "torch_xla.sync(wait=True)\n",
    "\n",
    "# Start profiling\n",
    "xp.trace_detached(service_addr=\"localhost:9012\", logdir=\"profile/\", duration_ms=60000)\n",
    "time.sleep(1)\n",
    "for i in range(10):\n",
    "  model.zero_grad()\n",
    "  output = model(input_ids.clone())\n",
    "  output.sum().backward()\n",
    "  torch_xla.sync()\n",
    "torch_xla.sync(wait=True)\n",
    "model.zero_grad()\n",
    "torch_xla.sync(wait=True)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
