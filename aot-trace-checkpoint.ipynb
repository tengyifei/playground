{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn without checkpoint\n",
    "\n",
    "We obtain the forward of `fn` in terms of aten ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 32, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 635, in _create_aot_dispatcher_function\n",
      "    fw_metadata = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 197, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 32, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 778, in _create_aot_dispatcher_function\n",
      "    compiled_fn, fw_metadata = compiler_fn(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 373, in aot_dispatch_autograd\n",
      "    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 310, in aot_dispatch_autograd_graph\n",
      "    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 55, in _create_graph\n",
      "    fx_g = make_fx(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2188, in wrapped\n",
      "    return make_fx_tracer.trace(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2126, in trace\n",
      "    return self._trace_inner(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2097, in _trace_inner\n",
      "    t = dispatch_trace(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1137, in dispatch_trace\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 843, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 700, in flatten_fn\n",
      "    tree_out = root_fn(*tree_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1192, in wrapped\n",
      "    out = f(*tensors)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 693, in inner_fn\n",
      "    outs = fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 644, in joint_helper\n",
      "    return _functionalized_f_helper(primals, tangents)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 413, in _functionalized_f_helper\n",
      "    f_outs = fn(*f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 280, in inner_fn_with_anomaly\n",
      "    return inner_fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 194, in inner_fn\n",
      "    outs, tangent_mask = fn(*primals)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 115, in inner_fn\n",
      "    outs = fn(*args_maybe_cloned)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "device = torch_xla.device()\n",
    "w = torch.randn(4, 4, requires_grad=False, device=device)\n",
    "torch_xla.sync()\n",
    "\n",
    "def print_traceback():\n",
    "  import traceback\n",
    "  traceback.print_stack()\n",
    "\n",
    "\n",
    "def fn(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  print_traceback()\n",
    "  time.sleep(1)\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a = torch.randn(4, 4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch_xla checkpoint\n",
    "\n",
    "Runtime error within torch_xla checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1073587478.py\", line 12, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 635, in _create_aot_dispatcher_function\n",
      "    fw_metadata = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 197, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1073587478.py\", line 5, in checkpointed_fn\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn, a, w)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 130, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1073587478.py\", line 12, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 778, in _create_aot_dispatcher_function\n",
      "    compiled_fn, fw_metadata = compiler_fn(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 373, in aot_dispatch_autograd\n",
      "    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 310, in aot_dispatch_autograd_graph\n",
      "    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 55, in _create_graph\n",
      "    fx_g = make_fx(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2188, in wrapped\n",
      "    return make_fx_tracer.trace(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2126, in trace\n",
      "    return self._trace_inner(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2097, in _trace_inner\n",
      "    t = dispatch_trace(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1137, in dispatch_trace\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 843, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 700, in flatten_fn\n",
      "    tree_out = root_fn(*tree_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1192, in wrapped\n",
      "    out = f(*tensors)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 693, in inner_fn\n",
      "    outs = fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 644, in joint_helper\n",
      "    return _functionalized_f_helper(primals, tangents)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 413, in _functionalized_f_helper\n",
      "    f_outs = fn(*f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 280, in inner_fn_with_anomaly\n",
      "    return inner_fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 194, in inner_fn\n",
      "    outs, tangent_mask = fn(*primals)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 115, in inner_fn\n",
      "    outs = fn(*args_maybe_cloned)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1073587478.py\", line 5, in checkpointed_fn\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn, a, w)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 130, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n",
      "/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in CheckpointFunctionBackward. Traceback of forward call that caused the error:\n",
      " (Triggered internally at /workspaces/torch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "ERROR:root:Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1664539/1073587478.py\", line 12, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 778, in _create_aot_dispatcher_function\n",
      "    compiled_fn, fw_metadata = compiler_fn(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 373, in aot_dispatch_autograd\n",
      "    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 310, in aot_dispatch_autograd_graph\n",
      "    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 55, in _create_graph\n",
      "    fx_g = make_fx(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2188, in wrapped\n",
      "    return make_fx_tracer.trace(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2126, in trace\n",
      "    return self._trace_inner(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2097, in _trace_inner\n",
      "    t = dispatch_trace(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1137, in dispatch_trace\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 843, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 700, in flatten_fn\n",
      "    tree_out = root_fn(*tree_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1192, in wrapped\n",
      "    out = f(*tensors)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 693, in inner_fn\n",
      "    outs = fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 644, in joint_helper\n",
      "    return _functionalized_f_helper(primals, tangents)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 413, in _functionalized_f_helper\n",
      "    f_outs = fn(*f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 280, in inner_fn_with_anomaly\n",
      "    return inner_fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 265, in inner_fn\n",
      "    backward_out = torch.autograd.grad(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 445, in grad\n",
      "    return handle_torch_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/overrides.py\", line 1719, in handle_torch_function\n",
      "    result = mode.__torch_function__(public_api, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1240, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 496, in grad\n",
      "    result = _engine_run_backward(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/function.py\", line 307, in apply\n",
      "    return user_fn(self, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 139, in backward\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch_xla.utils.checkpoint\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch_xla.utils.checkpoint.checkpoint(fn, a, w)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "\n",
    "try:\n",
    "  res = aot_print_fn(cloned_a, w)\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch non-reentrant checkpoint\n",
    "\n",
    "`aot_function` appears to skip over the checkpoint wrapper entirely. We still\n",
    "get back an identical aten forward that saves all the intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1474323637.py\", line 11, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 635, in _create_aot_dispatcher_function\n",
      "    fw_metadata = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\", line 197, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1474323637.py\", line 6, in checkpointed_fn\n",
      "    return torch.utils.checkpoint.checkpoint(fn, a, w, use_reentrant=False)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 496, in checkpoint\n",
      "    ret = function(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1664539/1474323637.py\", line 11, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 778, in _create_aot_dispatcher_function\n",
      "    compiled_fn, fw_metadata = compiler_fn(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 373, in aot_dispatch_autograd\n",
      "    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 310, in aot_dispatch_autograd_graph\n",
      "    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 55, in _create_graph\n",
      "    fx_g = make_fx(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2188, in wrapped\n",
      "    return make_fx_tracer.trace(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2126, in trace\n",
      "    return self._trace_inner(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2097, in _trace_inner\n",
      "    t = dispatch_trace(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1137, in dispatch_trace\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 843, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 700, in flatten_fn\n",
      "    tree_out = root_fn(*tree_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1192, in wrapped\n",
      "    out = f(*tensors)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 693, in inner_fn\n",
      "    outs = fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 644, in joint_helper\n",
      "    return _functionalized_f_helper(primals, tangents)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 413, in _functionalized_f_helper\n",
      "    f_outs = fn(*f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 280, in inner_fn_with_anomaly\n",
      "    return inner_fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 194, in inner_fn\n",
      "    outs, tangent_mask = fn(*primals)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 115, in inner_fn\n",
      "    outs = fn(*args_maybe_cloned)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn\n",
      "    tree_out = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1474323637.py\", line 6, in checkpointed_fn\n",
      "    return torch.utils.checkpoint.checkpoint(fn, a, w, use_reentrant=False)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 496, in checkpoint\n",
      "    ret = function(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 14, in fn\n",
      "    print_traceback()\n",
      "  File \"/tmp/ipykernel_1664539/1081952358.py\", line 7, in print_traceback\n",
      "    traceback.print_stack()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n",
      "Backward\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, mm, sigmoid, tangents_1):\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2)\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = t = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t_1);  mul = t_1 = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.checkpoint\n",
    "\n",
    "torch.xla = torch_xla.device()  # type:ignore\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn, a, w, use_reentrant=False)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)\n",
    "print(\"Backward\")\n",
    "res.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo to trace the checkpointed function\n",
    "\n",
    "We get a higher order `checkpoint` op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name                       target                       args                            kwargs\n",
      "-------------  -------------------------  ---------------------------  ------------------------------  ------------------------\n",
      "placeholder    l_a_                       L_a_                         ()                              {}\n",
      "placeholder    l_w_                       L_w_                         ()                              {}\n",
      "get_attr       wrap_body_0                wrap_body_0                  ()                              {}\n",
      "call_function  tag_activation_checkpoint  tag_activation_checkpoint    (wrap_body_0, l_a_, l_w_)       {'use_reentrant': False}\n",
      "call_function  getitem                    <built-in function getitem>  (tag_activation_checkpoint, 0)  {}\n",
      "output         output                     output                       ((getitem,),)                   {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_a_ : torch.Tensor, L_w_ : torch.Tensor):\n",
      "    l_a_ = L_a_\n",
      "    l_w_ = L_w_\n",
      "    wrap_body_0 = self.wrap_body_0\n",
      "    tag_activation_checkpoint = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_0, l_a_, l_w_, use_reentrant = False);  wrap_body_0 = l_a_ = l_w_ = None\n",
      "    getitem = tag_activation_checkpoint[0];  tag_activation_checkpoint = None\n",
      "    return (getitem,)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6119, 0.5826, 0.8878, 0.0959],\n",
       "        [0.4757, 0.3571, 0.6625, 0.1956],\n",
       "        [0.5700, 0.5923, 0.8227, 0.1141],\n",
       "        [0.7144, 0.5106, 0.8910, 0.2592]], device='xla:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return gm.forward  # return a python callable\n",
    "  \n",
    "def fn_2(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "  \n",
    "def checkpointed_fn_2(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=False)\n",
    "\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_compiler, fullgraph=True)\n",
    "dynamo_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo and then AOTAutograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                  {}\n",
      "placeholder    primals_2  primals_2             ()                                  {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                        {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                    {}\n",
      "call_function  cos        aten.cos.default      (mm,)                               {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                             {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm);  mm = None\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                    kwargs\n",
      "-------------  ----------------  -----------------------------  ----------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                      {}\n",
      "placeholder    primals_2         primals_2                      ()                      {}\n",
      "placeholder    tangents_1        tangents_1                     ()                      {}\n",
      "call_function  sin               aten.sin.default               (primals_1,)            {}\n",
      "call_function  mm                aten.mm.default                (sin, primals_2)        {}\n",
      "call_function  cos               aten.cos.default               (mm,)                   {}\n",
      "call_function  mm_1              aten.mm.default                (cos, primals_2)        {}\n",
      "call_function  sigmoid           aten.sigmoid.default           (mm_1,)                 {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)              {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)               {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)             {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)             {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_1, detach_3)  {}\n",
      "call_function  t                 aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_2              aten.mm.default                (sigmoid_backward, t)   {}\n",
      "call_function  sin_1             aten.sin.default               (mm,)                   {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                {}\n",
      "call_function  mul               aten.mul.Tensor                (mm_2, neg)             {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t)                {}\n",
      "call_function  cos_1             aten.cos.default               (primals_1,)            {}\n",
      "call_function  mul_1             aten.mul.Tensor                (mm_3, cos_1)           {}\n",
      "output         output            output                         ((mul_1, None),)        {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, tangents_1):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t);  mul = t = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_backend, fullgraph=True)\n",
    "o = dynamo_fn(cloned_a, w)\n",
    "assert o is not None\n",
    "o.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py:641: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.\n",
      "  warnings.warn(\n",
      "ERROR:root:'skip function check_backward_validity in file /usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py'\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_1664539/204424578.py\", line 2, in checkpointed_fn_3\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 87, in forward\n",
      "    check_backward_validity(args)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1664539/204424578.py\", line 7, in <module>\n",
      "    o = dynamo_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 556, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1447, in __call__\n",
      "    return self._torchdynamo_orig_callable(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 550, in __call__\n",
      "    return _compile(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 979, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 709, in compile_inner\n",
      "    return _compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 744, in _compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1348, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 234, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2914, in run\n",
      "    super().run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1828, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 349, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 125, in call_function\n",
      "    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 973, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3129, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3257, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1816, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "    return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "    return self.call_apply(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "    ).call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2359, in call_function\n",
      "    (fwd_out, _), fwd_graph, fwd_freevars = speculate_subgraph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 624, in speculate_subgraph\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 489, in speculate_subgraph\n",
      "    output = f.call_function(tx, args, sub_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 349, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 125, in call_function\n",
      "    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 973, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3129, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3257, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1738, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 751, in call_function\n",
      "    unimplemented(msg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 313, in unimplemented\n",
      "    raise Unsupported(msg, case_name=case_name)\n",
      "torch._dynamo.exc.Unsupported: 'skip function check_backward_validity in file /usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py'\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_1664539/204424578.py\", line 2, in checkpointed_fn_3\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 87, in forward\n",
      "    check_backward_validity(args)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def checkpointed_fn_3(a, w):\n",
    "  return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
    "\n",
    "try:\n",
    "  my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "  dynamo_fn = torch.compile(checkpointed_fn_3, backend=my_backend, fullgraph=True)\n",
    "  o = dynamo_fn(cloned_a, w)\n",
    "  assert o is not None\n",
    "  o.sum().backward()\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch.export with AOTAutograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, args_0: \"f32[4, 4]\", args_1: \"f32[4, 4]\"):\n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:17 in fn_2, code: a = torch.sin(a)\n",
      "            sin: \"f32[4, 4]\" = torch.ops.aten.sin.default(args_0);  args_0 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:18 in fn_2, code: a = a @ w\n",
      "            matmul: \"f32[4, 4]\" = torch.ops.aten.matmul.default(sin, args_1);  sin = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:19 in fn_2, code: a = torch.cos(a)\n",
      "            cos: \"f32[4, 4]\" = torch.ops.aten.cos.default(matmul);  matmul = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:20 in fn_2, code: a = a @ w\n",
      "            matmul_1: \"f32[4, 4]\" = torch.ops.aten.matmul.default(cos, args_1);  cos = args_1 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:21 in fn_2, code: a = torch.sigmoid(a)\n",
      "            sigmoid: \"f32[4, 4]\" = torch.ops.aten.sigmoid.default(matmul_1);  matmul_1 = None\n",
      "            return (sigmoid,)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='sigmoid'), target=None)])\n",
      "Range constraints: {}\n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                             kwargs\n",
      "-------------  ---------  --------------------  -----------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                               {}\n",
      "placeholder    primals_2  primals_2             ()                                               {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                                     {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                                 {}\n",
      "call_function  cos        aten.cos.default      (mm,)                                            {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                                 {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                                          {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2, mm, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                    kwargs\n",
      "-------------  ----------------  -----------------------------  ----------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                      {}\n",
      "placeholder    primals_2         primals_2                      ()                      {}\n",
      "placeholder    mm                mm                             ()                      {}\n",
      "placeholder    sigmoid           sigmoid                        ()                      {}\n",
      "placeholder    tangents_1        tangents_1                     ()                      {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)              {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)               {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)             {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)             {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_1, detach_3)  {}\n",
      "call_function  t                 aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_2              aten.mm.default                (sigmoid_backward, t)   {}\n",
      "call_function  sin_1             aten.sin.default               (mm,)                   {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                {}\n",
      "call_function  mul               aten.mul.Tensor                (mm_2, neg)             {}\n",
      "call_function  t_1               aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t_1)              {}\n",
      "call_function  cos_1             aten.cos.default               (primals_1,)            {}\n",
      "call_function  mul_1             aten.mul.Tensor                (mm_3, cos_1)           {}\n",
      "output         output            output                         ((mul_1, None),)        {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, mm, sigmoid, tangents_1):\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2)\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = t = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t_1);  mul = t_1 = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.export\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "  return make_boxed_func(gm.forward)\n",
    "  \n",
    "class FunctionModule(torch.nn.Module):\n",
    "  def __init__(self, f):\n",
    "    super().__init__()\n",
    "    self.f = f\n",
    "  \n",
    "  def forward(self, *args):\n",
    "    return self.f(*args)\n",
    "\n",
    "exported = torch.export.export_for_training(FunctionModule(checkpointed_fn_2), args=(cloned_a, w))\n",
    "print(exported)\n",
    "module = exported.module()\n",
    "\n",
    "# Now get the backward function\n",
    "from functorch.compile import aot_module\n",
    "aot_module(module, fw_compiler=my_compiler, bw_compiler=my_compiler)(cloned_a, w).sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use torch.compile with an exception trick to avoid evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                  {}\n",
      "placeholder    primals_2  primals_2             ()                                  {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                        {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                    {}\n",
      "call_function  cos        aten.cos.default      (mm,)                               {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                             {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm);  mm = None\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2)\n",
      "    \n",
      "\n",
      "(tensor([[ 0.0294,  0.8735,  1.2468, -0.3392],\n",
      "        [ 0.3750,  1.2185, -1.4892,  1.3335],\n",
      "        [ 1.2212, -0.4198, -0.7654,  0.6849],\n",
      "        [-0.7801,  1.0501, -1.3990, -1.5898]], device='xla:0',\n",
      "       requires_grad=True), tensor([[ 0.6220, -1.4707,  0.0057,  0.1287],\n",
      "        [-1.2112,  0.7024,  0.2291,  0.9017],\n",
      "        [ 0.9764,  0.0199,  0.4485, -0.9834],\n",
      "        [ 0.1413,  1.8033, -0.0187,  0.9632]], device='xla:0'))\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm);  mm = None\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReturnGraph(Exception):\n",
    "  graph: torch.fx.GraphModule\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "\n",
    "  def f(*args):\n",
    "    print(args)\n",
    "    raise ReturnGraph(gm)\n",
    "\n",
    "  return make_boxed_func(f)\n",
    "\n",
    "try:\n",
    "  my_backend = aot_autograd(fw_compiler=my_compiler, bw_compiler=my_compiler)\n",
    "  dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_backend, fullgraph=True)\n",
    "  o = dynamo_fn(cloned_a, w)\n",
    "  assert o is not None\n",
    "  o.sum().backward()\n",
    "except ReturnGraph as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, args_0: \"f32[4, 4]\", args_1: \"f32[4, 4]\"):\n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:17 in fn_2, code: a = torch.sin(a)\n",
      "            sin: \"f32[4, 4]\" = torch.ops.aten.sin.default(args_0);  args_0 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:18 in fn_2, code: a = a @ w\n",
      "            matmul: \"f32[4, 4]\" = torch.ops.aten.matmul.default(sin, args_1);  sin = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:19 in fn_2, code: a = torch.cos(a)\n",
      "            cos: \"f32[4, 4]\" = torch.ops.aten.cos.default(matmul);  matmul = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:20 in fn_2, code: a = a @ w\n",
      "            matmul_1: \"f32[4, 4]\" = torch.ops.aten.matmul.default(cos, args_1);  cos = args_1 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_1664539/1239672230.py:21 in fn_2, code: a = torch.sigmoid(a)\n",
      "            sigmoid: \"f32[4, 4]\" = torch.ops.aten.sigmoid.default(matmul_1);  matmul_1 = None\n",
      "            return (sigmoid,)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='sigmoid'), target=None)])\n",
      "Range constraints: {}\n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                             kwargs\n",
      "-------------  ---------  --------------------  -----------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                               {}\n",
      "placeholder    primals_2  primals_2             ()                                               {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                                     {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                                 {}\n",
      "call_function  cos        aten.cos.default      (mm,)                                            {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                                 {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                                          {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2, mm, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.sin.default(tensor([...], device='xla:0', size=(4, 4)))\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1664539/1049874456.py\", line 36, in <module>\n",
      "    aot_module(module, fw_compiler=my_compiler, bw_compiler=my_compiler)(cloned_a, w).sum().backward()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 945, in forward\n",
      "    return compiled_f(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 897, in returned_function\n",
      "    out = cached_fn(flat_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 309, in runtime_wrapper\n",
      "    all_outs = call_func_at_runtime_with_args(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n",
      "    return f(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1580, in forward\n",
      "    fw_outs = call_func_at_runtime_with_args(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 489, in wrapper\n",
      "    return compiled_fn(runtime_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 671, in inner_fn\n",
      "    outs = compiled_fn(args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n",
      "    return f(*args)\n",
      "  File \"<eval_with_key>.57\", line 5, in forward\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_ops.py\", line 723, in __call__\n",
      "    return self._op(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1271, in __torch_dispatch__\n",
      "    return self.dispatch(func, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1813, in dispatch\n",
      "    return self._cached_dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1381, in _cached_dispatch_impl\n",
      "    output = self._dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1911, in _dispatch_impl\n",
      "    (flat_args, flat_arg_fake_tensors) = self.validate_and_convert_non_fake_tensors(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2378, in validate_and_convert_non_fake_tensors\n",
      "    validated_args = [validate(a) for a in flat_args]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2378, in <listcomp>\n",
      "    validated_args = [validate(a) for a in flat_args]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2366, in validate\n",
      "    raise AssertionError(\n",
      "AssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.sin.default(tensor([...], device='xla:0', size=(4, 4)))\n"
     ]
    }
   ],
   "source": [
    "import torch.export\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "  return make_boxed_func(gm.forward)\n",
    "  \n",
    "class FunctionModule(torch.nn.Module):\n",
    "  def __init__(self, f):\n",
    "    super().__init__()\n",
    "    self.f = f\n",
    "  \n",
    "  def forward(self, *args):\n",
    "    return self.f(*args)\n",
    "\n",
    "try:\n",
    "  with FakeTensorMode():\n",
    "    fake_a = torch.randn(4, 4)\n",
    "    fake_w = torch.randn(4, 4)\n",
    "    exported = torch.export.export_for_training(FunctionModule(checkpointed_fn_2), args=(fake_a, fake_w))\n",
    "    print(exported)\n",
    "    module = exported.module()\n",
    "\n",
    "    # Now get the backward function\n",
    "    from functorch.compile import aot_module\n",
    "    aot_module(module, fw_compiler=my_compiler, bw_compiler=my_compiler)(cloned_a, w).sum().backward()\n",
    "except AssertionError as e:\n",
    "  logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dynamo + AOTAutograd on graphs with multiple inputs/outputs\n",
    "\n",
    "Not sure why, we need to clone any outputs that may be aliased.\n",
    "Otherwise, the extracted AOT autograd graph is missing outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target              args                                  kwargs\n",
      "-------------  ---------  ------------------  ------------------------------------  --------\n",
      "placeholder    primals_1  primals_1           ()                                    {}\n",
      "placeholder    primals_2  primals_2           ()                                    {}\n",
      "call_function  mm         aten.mm.default     (primals_1, primals_2)                {}\n",
      "call_function  clone      aten.clone.default  (mm,)                                 {}\n",
      "output         output     output              ((mm, clone, primals_1, primals_2),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    mm = torch.ops.aten.mm.default(primals_1, primals_2)\n",
      "    clone = torch.ops.aten.clone.default(mm)\n",
      "    return (mm, clone, primals_1, primals_2)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name        target           args                      kwargs\n",
      "-------------  ----------  ---------------  ------------------------  --------\n",
      "placeholder    primals_1   primals_1        ()                        {}\n",
      "placeholder    primals_2   primals_2        ()                        {}\n",
      "placeholder    tangents_1  tangents_1       ()                        {}\n",
      "placeholder    tangents_2  tangents_2       ()                        {}\n",
      "call_function  add         aten.add.Tensor  (tangents_1, tangents_2)  {}\n",
      "call_function  t           aten.t.default   (primals_1,)              {}\n",
      "call_function  mm_1        aten.mm.default  (t, add)                  {}\n",
      "call_function  t_1         aten.t.default   (primals_2,)              {}\n",
      "call_function  mm_2        aten.mm.default  (add, t_1)                {}\n",
      "output         output      output           ((mm_2, mm_1),)           {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, tangents_1, tangents_2):\n",
      "    add = torch.ops.aten.add.Tensor(tangents_1, tangents_2);  tangents_1 = tangents_2 = None\n",
      "    t = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    mm_1 = torch.ops.aten.mm.default(t, add);  t = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(add, t_1);  add = t_1 = None\n",
      "    return (mm_2, mm_1)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch_xla.experimental.scan import tree_map\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "def fn_4(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  a = a @ w\n",
    "  w = a\n",
    "  return a, w.clone()\n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "dynamo_fn = torch.compile(fn_4, backend=my_backend, fullgraph=True)\n",
    "fake_a = torch.empty_like(cloned_a, requires_grad=True, device=torch_xla.device())\n",
    "fake_w = torch.empty_like(w, requires_grad=True, device=torch_xla.device())\n",
    "o = dynamo_fn(fake_a, fake_w)\n",
    "# o = dynamo_fn(cloned_a, w)\n",
    "assert o is not None\n",
    "torch.autograd.backward(o, tree_map(lambda v: torch.ones_like(v), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dynamo + AOTAutograd argument ordering in complex graphs\n",
    "\n",
    "When we extract the forward/backward graph using dynamo, it turns out that the\n",
    "argument orders passed to our compiler isn't the same as the argument orders to\n",
    "`fn`. The example below demonstrates that and inspects the bytecode.\n",
    "\n",
    "Expectation: `fn` gets two PyTrees, `carry` and `x`. So `my_compiler` should\n",
    "get a `GraphModule` taking for tensors, `carry['a']`, `carry['b']`,\n",
    "`x['weights']`, `x['biases']`, respectively.\n",
    "\n",
    "Reality: `carry` and `x` are reversed in the `GraphModule` given to `my_compiler`.\n",
    "\n",
    "Hypothesis: Dyanmo is just using `my_compiler` as a backend. Within the bytecode\n",
    "generated by Dynamo, it can reorder things however it wants. It can even eliminate\n",
    "certain input/output arguments in case of aliasing. That's fine if `my_compiler`\n",
    "is lowering the graph. It should lower whatever input graph appropriately to e.g.\n",
    "CUDA, XLA, etc. However, we're abusing `my_compiler` to extract the forward and\n",
    "backward graphs. There is no guarantee that the graph given to `my_compiler` will\n",
    "have the same signature as `fn`. There is even no guarantee that `my_compiler`\n",
    "receives the whole graph if `fullgrah=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                                                          kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                                                            {}\n",
      "placeholder    primals_2  primals_2             ()                                                                            {}\n",
      "placeholder    primals_3  primals_3             ()                                                                            {}\n",
      "placeholder    primals_4  primals_4             ()                                                                            {}\n",
      "call_function  mm         aten.mm.default       (primals_3, primals_1)                                                        {}\n",
      "call_function  add        aten.add.Tensor       (mm, primals_2)                                                               {}\n",
      "call_function  sin        aten.sin.default      (add,)                                                                        {}\n",
      "call_function  mm_1       aten.mm.default       (primals_4, primals_1)                                                        {}\n",
      "call_function  add_1      aten.add.Tensor       (mm_1, primals_2)                                                             {}\n",
      "call_function  cos        aten.cos.default      (add_1,)                                                                      {}\n",
      "call_function  add_2      aten.add.Tensor       (sin, cos)                                                                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (add_2,)                                                                      {}\n",
      "output         output     output                ((sin, cos, sigmoid, primals_1, primals_3, primals_4, add, add_1, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4):\n",
      "    mm = torch.ops.aten.mm.default(primals_3, primals_1)\n",
      "    add = torch.ops.aten.add.Tensor(mm, primals_2);  mm = None\n",
      "    sin = torch.ops.aten.sin.default(add)\n",
      "    mm_1 = torch.ops.aten.mm.default(primals_4, primals_1)\n",
      "    add_1 = torch.ops.aten.add.Tensor(mm_1, primals_2);  mm_1 = primals_2 = None\n",
      "    cos = torch.ops.aten.cos.default(add_1)\n",
      "    add_2 = torch.ops.aten.add.Tensor(sin, cos)\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(add_2);  add_2 = None\n",
      "    return (sin, cos, sigmoid, primals_1, primals_3, primals_4, add, add_1, sigmoid)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                            kwargs\n",
      "-------------  ----------------  -----------------------------  ------------------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                              {}\n",
      "placeholder    primals_3         primals_3                      ()                              {}\n",
      "placeholder    primals_4         primals_4                      ()                              {}\n",
      "placeholder    add               add                            ()                              {}\n",
      "placeholder    add_1             add_1                          ()                              {}\n",
      "placeholder    sigmoid           sigmoid                        ()                              {}\n",
      "placeholder    tangents_1        tangents_1                     ()                              {}\n",
      "placeholder    tangents_2        tangents_2                     ()                              {}\n",
      "placeholder    tangents_3        tangents_3                     ()                              {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)                      {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)                       {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)                     {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)                     {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_3, detach_3)          {}\n",
      "call_function  add_3             aten.add.Tensor                (tangents_1, sigmoid_backward)  {}\n",
      "call_function  add_4             aten.add.Tensor                (tangents_2, sigmoid_backward)  {}\n",
      "call_function  sin_1             aten.sin.default               (add_1,)                        {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                        {}\n",
      "call_function  mul               aten.mul.Tensor                (add_4, neg)                    {}\n",
      "call_function  t                 aten.t.default                 (primals_4,)                    {}\n",
      "call_function  mm_2              aten.mm.default                (t, mul)                        {}\n",
      "call_function  t_1               aten.t.default                 (primals_1,)                    {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t_1)                      {}\n",
      "call_function  cos_1             aten.cos.default               (add,)                          {}\n",
      "call_function  mul_1             aten.mul.Tensor                (add_3, cos_1)                  {}\n",
      "call_function  add_5             aten.add.Tensor                (mul, mul_1)                    {}\n",
      "call_function  t_2               aten.t.default                 (primals_3,)                    {}\n",
      "call_function  mm_4              aten.mm.default                (t_2, mul_1)                    {}\n",
      "call_function  t_3               aten.t.default                 (primals_1,)                    {}\n",
      "call_function  mm_5              aten.mm.default                (mul_1, t_3)                    {}\n",
      "call_function  add_6             aten.add.Tensor                (mm_2, mm_4)                    {}\n",
      "output         output            output                         ((add_6, add_5, mm_5, mm_3),)   {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_3, primals_4, add, add_1, sigmoid, tangents_1, tangents_2, tangents_3):\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_3, detach_3);  tangents_3 = detach_3 = None\n",
      "    add_3 = torch.ops.aten.add.Tensor(tangents_1, sigmoid_backward);  tangents_1 = None\n",
      "    add_4 = torch.ops.aten.add.Tensor(tangents_2, sigmoid_backward);  tangents_2 = sigmoid_backward = None\n",
      "    sin_1 = torch.ops.aten.sin.default(add_1);  add_1 = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(add_4, neg);  add_4 = neg = None\n",
      "    t = torch.ops.aten.t.default(primals_4);  primals_4 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(t, mul);  t = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_1)\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t_1);  t_1 = None\n",
      "    cos_1 = torch.ops.aten.cos.default(add);  add = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(add_3, cos_1);  add_3 = cos_1 = None\n",
      "    add_5 = torch.ops.aten.add.Tensor(mul, mul_1);  mul = None\n",
      "    t_2 = torch.ops.aten.t.default(primals_3);  primals_3 = None\n",
      "    mm_4 = torch.ops.aten.mm.default(t_2, mul_1);  t_2 = None\n",
      "    t_3 = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    mm_5 = torch.ops.aten.mm.default(mul_1, t_3);  mul_1 = t_3 = None\n",
      "    add_6 = torch.ops.aten.add.Tensor(mm_2, mm_4);  mm_2 = mm_4 = None\n",
      "    return (add_6, add_5, mm_5, mm_3)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_xla.experimental.scan import tree_flatten\n",
    "\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "def fn(carry, x):\n",
    "  weights = x['weights']\n",
    "  biases = x['biases']\n",
    "  carry_a = carry['a']\n",
    "  carry_b = carry['b']\n",
    "  new_carry_a = torch.sin((carry_a @ weights) + biases)\n",
    "  new_carry_b = torch.cos((carry_b @ weights) + biases)\n",
    "  y = torch.sigmoid(new_carry_a + new_carry_b)\n",
    "  return {'a': new_carry_a, 'b': new_carry_b}, y\n",
    "\n",
    "init = {\n",
    "    'a': torch.randn(2, 3, requires_grad=True, device=device),\n",
    "    'b': torch.randn(2, 3, requires_grad=True, device=device)\n",
    "}\n",
    "x = {\n",
    "    'weights': torch.randn(3, 3, requires_grad=True, device=device),\n",
    "    'biases': torch.randn(2, 3, requires_grad=True, device=device)\n",
    "}\n",
    "\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "dynamo_fn = torch.compile(fn, backend=my_backend, fullgraph=True)\n",
    "fake_init = tree_map(lambda v: torch.empty_like(v, requires_grad=v.requires_grad, device=torch_xla.device()), init)\n",
    "fake_x = tree_map(lambda v: torch.empty_like(v, requires_grad=v.requires_grad, device=torch_xla.device()), x)\n",
    "o = dynamo_fn(fake_init, fake_x)\n",
    "assert o is not None\n",
    "o, _ = tree_flatten(o)\n",
    "torch.autograd.backward(o, tree_map(lambda v: torch.ones_like(v), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the generate bytecode. We'll see that Dynamo generates a small\n",
    "wrapper around the `__compiled_function_18`. Within the wrapper, it exchanged the\n",
    "`carry` and `x` arguments, i.e. `weights` and `biases` comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6           0 LOAD_GLOBAL              4 (__compiled_fn_14)\n",
      "              2 LOAD_FAST                1 (x)\n",
      "              4 LOAD_CONST               1 ('weights')\n",
      "              6 BINARY_SUBSCR\n",
      "              8 LOAD_FAST                1 (x)\n",
      "             10 LOAD_CONST               2 ('biases')\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LOAD_FAST                0 (carry)\n",
      "             16 LOAD_CONST               3 ('a')\n",
      "             18 BINARY_SUBSCR\n",
      "             20 LOAD_FAST                0 (carry)\n",
      "             22 LOAD_CONST               4 ('b')\n",
      "             24 BINARY_SUBSCR\n",
      "             26 CALL_FUNCTION            4\n",
      "             28 STORE_FAST               9 (graph_out_0)\n",
      "             30 LOAD_CONST               3 ('a')\n",
      "             32 LOAD_FAST                9 (graph_out_0)\n",
      "             34 LOAD_CONST               6 (0)\n",
      "             36 BINARY_SUBSCR\n",
      "             38 LOAD_CONST               4 ('b')\n",
      "             40 LOAD_FAST                9 (graph_out_0)\n",
      "             42 LOAD_CONST               7 (1)\n",
      "             44 BINARY_SUBSCR\n",
      "             46 BUILD_MAP                2\n",
      "             48 LOAD_FAST                9 (graph_out_0)\n",
      "             50 LOAD_CONST               8 (2)\n",
      "             52 BINARY_SUBSCR\n",
      "             54 BUILD_TUPLE              2\n",
      "             56 DELETE_FAST              9 (graph_out_0)\n",
      "             58 RETURN_VALUE\n",
      "def fn(carry, x):\n",
      "    graph_out_0 = __compiled_fn_14(x['weights'], x['biases'], carry['a'], carry\n",
      "        ['b'])\n",
      "    return {'a': graph_out_0[0], 'b': graph_out_0[1]}, graph_out_0[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\n",
    "cache_entries = _debug_get_cache_entry_list(innermost_fn(dynamo_fn))\n",
    "cache_entry = cache_entries[0]\n",
    "code = cache_entry.code\n",
    "# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\n",
    "import dis\n",
    "dis.dis(code)\n",
    "from depyf import decompile\n",
    "print(decompile(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__compiled_fn_18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m__compiled_fn_18\u001b[49m  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__compiled_fn_18' is not defined"
     ]
    }
   ],
   "source": [
    "__compiled_fn_18  # type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Dynamo invokes user compiler with a graph with unstable argument ordering.\n",
    "- The user compiler output is then weaved into the series of Python functions,\n",
    "  some of which are in bytecode.\n",
    "- In contrast, JAX models this as a stack of interpreters/function transforms.\n",
    "  The JAX approach is way more composable but you lose the ability to graph break\n",
    "  or to call into foreign functions.\n",
    "- Dynamo is missing a crucial API to get `GraphModule`s representing both\n",
    "  forward and backward passes, for a particular guarded monomorphic condition.\n",
    "  That will make our lives much easier.\n",
    "- We need to bend backwards to extract aten forward and backward graphs with\n",
    "  the expected argument ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                                                          kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                                                            {}\n",
      "placeholder    primals_2  primals_2             ()                                                                            {}\n",
      "placeholder    primals_3  primals_3             ()                                                                            {}\n",
      "placeholder    primals_4  primals_4             ()                                                                            {}\n",
      "call_function  mm         aten.mm.default       (primals_3, primals_1)                                                        {}\n",
      "call_function  add        aten.add.Tensor       (mm, primals_2)                                                               {}\n",
      "call_function  sin        aten.sin.default      (add,)                                                                        {}\n",
      "call_function  mm_1       aten.mm.default       (primals_4, primals_1)                                                        {}\n",
      "call_function  add_1      aten.add.Tensor       (mm_1, primals_2)                                                             {}\n",
      "call_function  cos        aten.cos.default      (add_1,)                                                                      {}\n",
      "call_function  add_2      aten.add.Tensor       (sin, cos)                                                                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (add_2,)                                                                      {}\n",
      "output         output     output                ((sin, cos, sigmoid, primals_1, primals_3, primals_4, add, add_1, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, primals_3, primals_4):\n",
      "    mm = torch.ops.aten.mm.default(primals_3, primals_1)\n",
      "    add = torch.ops.aten.add.Tensor(mm, primals_2);  mm = None\n",
      "    sin = torch.ops.aten.sin.default(add)\n",
      "    mm_1 = torch.ops.aten.mm.default(primals_4, primals_1)\n",
      "    add_1 = torch.ops.aten.add.Tensor(mm_1, primals_2);  mm_1 = primals_2 = None\n",
      "    cos = torch.ops.aten.cos.default(add_1)\n",
      "    add_2 = torch.ops.aten.add.Tensor(sin, cos)\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(add_2);  add_2 = None\n",
      "    return (sin, cos, sigmoid, primals_1, primals_3, primals_4, add, add_1, sigmoid)\n",
      "    \n",
      "{}\n",
      "graph():\n",
      "    %primals_1 : [num_users=3] = placeholder[target=primals_1]\n",
      "    %primals_2 : [num_users=2] = placeholder[target=primals_2]\n",
      "    %primals_3 : [num_users=2] = placeholder[target=primals_3]\n",
      "    %primals_4 : [num_users=2] = placeholder[target=primals_4]\n",
      "    %mm : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%primals_3, %primals_1), kwargs = {})\n",
      "    %add : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm, %primals_2), kwargs = {})\n",
      "    %sin : [num_users=2] = call_function[target=torch.ops.aten.sin.default](args = (%add,), kwargs = {})\n",
      "    %mm_1 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%primals_4, %primals_1), kwargs = {})\n",
      "    %add_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_1, %primals_2), kwargs = {})\n",
      "    %cos : [num_users=2] = call_function[target=torch.ops.aten.cos.default](args = (%add_1,), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sin, %cos), kwargs = {})\n",
      "    %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_2,), kwargs = {})\n",
      "    return (sin, cos, sigmoid, primals_1, primals_3, primals_4, add, add_1, sigmoid)\n",
      "...\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=meta) of size 9] xla:0 None\n",
      "...\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=meta) of size 6] xla:0 None\n",
      "...\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=meta) of size 6] xla:0 None\n",
      "...\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=meta) of size 6] xla:0 None\n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                            kwargs\n",
      "-------------  ----------------  -----------------------------  ------------------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                              {}\n",
      "placeholder    primals_3         primals_3                      ()                              {}\n",
      "placeholder    primals_4         primals_4                      ()                              {}\n",
      "placeholder    add               add                            ()                              {}\n",
      "placeholder    add_1             add_1                          ()                              {}\n",
      "placeholder    sigmoid           sigmoid                        ()                              {}\n",
      "placeholder    tangents_1        tangents_1                     ()                              {}\n",
      "placeholder    tangents_2        tangents_2                     ()                              {}\n",
      "placeholder    tangents_3        tangents_3                     ()                              {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)                      {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)                       {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)                     {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)                     {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_3, detach_3)          {}\n",
      "call_function  add_3             aten.add.Tensor                (tangents_1, sigmoid_backward)  {}\n",
      "call_function  add_4             aten.add.Tensor                (tangents_2, sigmoid_backward)  {}\n",
      "call_function  sin_1             aten.sin.default               (add_1,)                        {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                        {}\n",
      "call_function  mul               aten.mul.Tensor                (add_4, neg)                    {}\n",
      "call_function  t                 aten.t.default                 (primals_4,)                    {}\n",
      "call_function  mm_2              aten.mm.default                (t, mul)                        {}\n",
      "call_function  t_1               aten.t.default                 (primals_1,)                    {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t_1)                      {}\n",
      "call_function  cos_1             aten.cos.default               (add,)                          {}\n",
      "call_function  mul_1             aten.mul.Tensor                (add_3, cos_1)                  {}\n",
      "call_function  add_5             aten.add.Tensor                (mul, mul_1)                    {}\n",
      "call_function  t_2               aten.t.default                 (primals_3,)                    {}\n",
      "call_function  mm_4              aten.mm.default                (t_2, mul_1)                    {}\n",
      "call_function  t_3               aten.t.default                 (primals_1,)                    {}\n",
      "call_function  mm_5              aten.mm.default                (mul_1, t_3)                    {}\n",
      "call_function  add_6             aten.add.Tensor                (mm_2, mm_4)                    {}\n",
      "output         output            output                         ((add_6, add_5, mm_5, mm_3),)   {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_3, primals_4, add, add_1, sigmoid, tangents_1, tangents_2, tangents_3):\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_3, detach_3);  tangents_3 = detach_3 = None\n",
      "    add_3 = torch.ops.aten.add.Tensor(tangents_1, sigmoid_backward);  tangents_1 = None\n",
      "    add_4 = torch.ops.aten.add.Tensor(tangents_2, sigmoid_backward);  tangents_2 = sigmoid_backward = None\n",
      "    sin_1 = torch.ops.aten.sin.default(add_1);  add_1 = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(add_4, neg);  add_4 = neg = None\n",
      "    t = torch.ops.aten.t.default(primals_4);  primals_4 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(t, mul);  t = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_1)\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t_1);  t_1 = None\n",
      "    cos_1 = torch.ops.aten.cos.default(add);  add = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(add_3, cos_1);  add_3 = cos_1 = None\n",
      "    add_5 = torch.ops.aten.add.Tensor(mul, mul_1);  mul = None\n",
      "    t_2 = torch.ops.aten.t.default(primals_3);  primals_3 = None\n",
      "    mm_4 = torch.ops.aten.mm.default(t_2, mul_1);  t_2 = None\n",
      "    t_3 = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    mm_5 = torch.ops.aten.mm.default(mul_1, t_3);  mul_1 = t_3 = None\n",
      "    add_6 = torch.ops.aten.add.Tensor(mm_2, mm_4);  mm_2 = mm_4 = None\n",
      "    return (add_6, add_5, mm_5, mm_3)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_xla.experimental.scan import tree_flatten, tree_unflatten\n",
    "import torch.func\n",
    "\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "def get_fn():\n",
    "  # placeholder = torch.ones(3, device=device, dtype=torch.float32)\n",
    "  def fn(carry, x):\n",
    "    weights = x['weights']\n",
    "    biases = x['biases']\n",
    "    carry_a = carry['a']\n",
    "    carry_b = carry['b']\n",
    "    new_carry_a = torch.sin((carry_a @ weights) + biases)\n",
    "    new_carry_b = torch.cos((carry_b @ weights) + biases)\n",
    "    y = torch.sigmoid(new_carry_a + new_carry_b)\n",
    "    return {'a': new_carry_a, 'b': new_carry_b}, y\n",
    "  return fn\n",
    "\n",
    "fn = get_fn()\n",
    "\n",
    "init = {\n",
    "    'a': torch.randn(2, 3, requires_grad=True, device=device),\n",
    "    'b': torch.randn(2, 3, requires_grad=True, device=device)\n",
    "}\n",
    "x = {\n",
    "    'weights': torch.randn(3, 3, requires_grad=True, device=device),\n",
    "    'biases': torch.randn(2, 3, requires_grad=True, device=device)\n",
    "}\n",
    "\n",
    "\n",
    "fake_init = tree_map(lambda v: torch.empty_like(v, requires_grad=v.requires_grad, device=torch_xla.device()), init)\n",
    "fake_x = tree_map(lambda v: torch.empty_like(v, requires_grad=v.requires_grad, device=torch_xla.device()), x)\n",
    "\n",
    "flat_fake_init, _ = tree_flatten(fake_init)\n",
    "flat_fake_x, _ = tree_flatten(fake_x)\n",
    "\n",
    "\n",
    "flat_init, init_spec = tree_flatten(init)\n",
    "flat_xs, xs_spec = tree_flatten(x)\n",
    "flat_init_len = len(flat_init)\n",
    "flat_xs_len = len(flat_xs)\n",
    "\n",
    "def fn_flattened(*args):\n",
    "  flat_init = args[:flat_init_len]\n",
    "  flat_xs = args[flat_init_len:]\n",
    "  return fn(\n",
    "      tree_unflatten(flat_init, init_spec), tree_unflatten(flat_xs, xs_spec))\n",
    "\n",
    "\n",
    "def determine_args_ordering(example_inputs: List[torch.Tensor]):\n",
    "  for i, t in enumerate(example_inputs):\n",
    "    try:\n",
    "      idx = flat_fake_init.index(t)\n",
    "    except:\n",
    "      idx = flat_fake_x.index(t)\n",
    "      idx += len(flat_fake_init)\n",
    "    print(f\"arg {i} should be idx {i}\")\n",
    "\n",
    "\n",
    "def fw_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print(gm.meta)\n",
    "    print(gm.graph)\n",
    "    \n",
    "    # Idea: inspect tensors in example_inputs correspond to which tensors in the input.\n",
    "    # determine_args_ordering(example_inputs)\n",
    "    for t in example_inputs:\n",
    "      print(t.storage(), t.device, t.grad_fn)\n",
    "\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "  \n",
    "def bw_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n",
    "dynamo_fn = torch.compile(fn_flattened, backend=my_backend, fullgraph=True)\n",
    "o = dynamo_fn(*flat_fake_init, *flat_fake_x)\n",
    "assert o is not None\n",
    "o, _ = tree_flatten(o)\n",
    "torch.autograd.backward(o, tree_map(lambda v: torch.ones_like(v), o))\n",
    "\n",
    "# These don't work\n",
    "# AOTAutograd of dynamo\n",
    "# aot_dynamo_fn = aot_function(dynamo_fn, fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n",
    "# o = aot_dynamo_fn(fake_init, fake_x)\n",
    "# grad_and_value of dynamo\n",
    "# o = torch.func.grad_and_value(dynamo_fn)(fake_init, fake_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44           0 LOAD_GLOBAL              6 (__compiled_fn_54)\n",
      "              2 LOAD_FAST                0 (args)\n",
      "              4 LOAD_CONST               1 (2)\n",
      "              6 BINARY_SUBSCR\n",
      "              8 LOAD_FAST                0 (args)\n",
      "             10 LOAD_CONST               2 (3)\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LOAD_FAST                0 (args)\n",
      "             16 LOAD_CONST               3 (0)\n",
      "             18 BINARY_SUBSCR\n",
      "             20 LOAD_FAST                0 (args)\n",
      "             22 LOAD_CONST               4 (1)\n",
      "             24 BINARY_SUBSCR\n",
      "             26 CALL_FUNCTION            4\n",
      "             28 STORE_FAST               3 (graph_out_0)\n",
      "             30 LOAD_CONST               5 ('a')\n",
      "             32 LOAD_FAST                3 (graph_out_0)\n",
      "             34 LOAD_CONST               3 (0)\n",
      "             36 BINARY_SUBSCR\n",
      "             38 LOAD_CONST               6 ('b')\n",
      "             40 LOAD_FAST                3 (graph_out_0)\n",
      "             42 LOAD_CONST               4 (1)\n",
      "             44 BINARY_SUBSCR\n",
      "             46 BUILD_MAP                2\n",
      "             48 LOAD_FAST                3 (graph_out_0)\n",
      "             50 LOAD_CONST               1 (2)\n",
      "             52 BINARY_SUBSCR\n",
      "             54 BUILD_TUPLE              2\n",
      "             56 DELETE_FAST              3 (graph_out_0)\n",
      "             58 RETURN_VALUE\n",
      "def fn_flattened(*args):\n",
      "    graph_out_0 = __compiled_fn_54(args[2], args[3], args[0], args[1])\n",
      "    return {'a': graph_out_0[0], 'b': graph_out_0[1]}, graph_out_0[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\n",
    "cache_entries = _debug_get_cache_entry_list(innermost_fn(dynamo_fn))\n",
    "cache_entry = cache_entries[0]\n",
    "code = cache_entry.code\n",
    "# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\n",
    "import dis\n",
    "dis.dis(code)\n",
    "from depyf import decompile\n",
    "print(decompile(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.fn_flattened(*args)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "cached_func = created_function = types.FunctionType(cache_entry.code, globals())\n",
    "cached_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tracing the cached function and skipping the guard? Still broke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in CompiledFunctionBackward. Traceback of forward call that caused the error:\n",
      " (Triggered internally at /workspaces/torch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nDuring the backward, we encountered a tensor subclass where we guessed its\nmetadata incorrectly.\n\nExpected metadata: None, expected type: <class 'torch.Tensor'>\n\nRuntime metadata: None, runtime type: <class 'torch._subclasses.functional_tensor.FunctionalTensor'>\n\nshape: torch.Size([2, 3])\nTo fix this, your tensor subclass must implement the dunder method __force_to_same_metadata__.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# AOTAutograd of dynamo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m aot_dynamo_fn \u001b[38;5;241m=\u001b[39m aot_function(cached_func, fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler, bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler)\n\u001b[0;32m----> 3\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43maot_dynamo_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_fake_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_fake_x\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m     (fake_mode, shape_env) \u001b[38;5;241m=\u001b[39m construct_fake_mode(flat_args, aot_config)\n\u001b[1;32m    884\u001b[0m     fake_flat_args: FakifiedFlatArgs \u001b[38;5;241m=\u001b[39m process_inputs(\n\u001b[1;32m    885\u001b[0m         flat_args, aot_config, fake_mode, shape_env\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 887\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:527\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    520\u001b[0m     flat_fn,\n\u001b[1;32m    521\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:778\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[1;32m    776\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 778\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:373\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    364\u001b[0m flat_fn, flat_args, fw_metadata \u001b[38;5;241m=\u001b[39m pre_compile(\n\u001b[1;32m    365\u001b[0m     wrappers,\n\u001b[1;32m    366\u001b[0m     flat_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m     fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m fw_metadata\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled()\n\u001b[0;32m--> 373\u001b[0m fx_g, joint_inputs, maybe_subclass_meta \u001b[38;5;241m=\u001b[39m \u001b[43maot_dispatch_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Copied from aot_dispatch_autograd_graph.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m disable_amp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:310\u001b[0m, in \u001b[0;36maot_dispatch_autograd_graph\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    305\u001b[0m saved_updated_joint_inputs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    306\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mdetach(), updated_joint_inputs\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m maybe_subclass_meta \u001b[38;5;241m=\u001b[39m subclass_tracing_info\u001b[38;5;241m.\u001b[39mmaybe_subclass_meta\n\u001b[0;32m--> 310\u001b[0m fx_g \u001b[38;5;241m=\u001b[39m \u001b[43m_create_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_fn_to_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_joint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m assert_functional_graph(fx_g\u001b[38;5;241m.\u001b[39mgraph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:55\u001b[0m, in \u001b[0;36m_create_graph\u001b[0;34m(f, args, aot_config)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_graph\u001b[39m(f, args, \u001b[38;5;241m*\u001b[39m, aot_config: AOTConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode must be enabled here.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# See Note [Accessing .grad_fn on FunctionalTensor]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_python_dispatcher(), FunctionalTensorMode(\n\u001b[1;32m     50\u001b[0m         pre_dispatch\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[1;32m     51\u001b[0m         export\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mis_export,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Allow token discovery for joint fn tracing as tokens can be used in backward.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         _allow_token_discovery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m     ):\n\u001b[0;32m---> 55\u001b[0m         fx_g \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fx_g\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2188\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 2188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2126\u001b[0m, in \u001b[0;36m_MakefxTracer.trace\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrace\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m fx\u001b[38;5;241m.\u001b[39mGraphModule:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_modes_from_inputs(f, args):\n\u001b[0;32m-> 2126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2097\u001b[0m, in \u001b[0;36m_MakefxTracer._trace_inner\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfx_tracer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2097\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2103\u001b[0m     trace_structured(\n\u001b[1;32m   2104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifact\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2105\u001b[0m         metadata_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         )\u001b[38;5;241m.\u001b[39msrc,\n\u001b[1;32m   2115\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:721\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1137\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_trace\u001b[39m(\n\u001b[1;32m   1133\u001b[0m     root: Union[Module, Callable],\n\u001b[1;32m   1134\u001b[0m     tracer: Tracer,\n\u001b[1;32m   1135\u001b[0m     concrete_args: Optional[Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 1137\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;66;03m# NB: be careful not to DCE .item() calls\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpure_pred\u001b[39m(n: fx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:721\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    837\u001b[0m             _autowrap_check(\n\u001b[1;32m    838\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    839\u001b[0m             )\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    841\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 843\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    844\u001b[0m             {},\n\u001b[1;32m    845\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:700\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    699\u001b[0m     tree_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(\u001b[38;5;28mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 700\u001b[0m     tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mroot_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtree_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     out_args, out_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_codegen, _PyTreeCodeGen)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1192\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies, **_unused)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tensor_proxy_slot\u001b[39m(t: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Proxy]:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mproxy)\n\u001b[0;32m-> 1192\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(Tensor, get_tensor_proxy_slot, out)\n\u001b[1;32m   1194\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m   1195\u001b[0m     _AnyScriptObject, \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x), out\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:693\u001b[0m, in \u001b[0;36mhandle_effect_tokens_fn.<locals>.inner_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    690\u001b[0m         functional_tensor_mode\u001b[38;5;241m.\u001b[39m_tokens[k] \u001b[38;5;241m=\u001b[39m f_tokens[i]\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Return both the tokens and the outputs\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:644\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>.joint_helper\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoint_helper\u001b[39m(primals, tangents):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_functionalized_f_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:413\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>._functionalized_f_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    410\u001b[0m     f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(to_fun, args)\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mf_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# We support a limited amount of mutation of graph inputs during the backward pass.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;66;03m# (This is used e.g. by Float8, which needs to update buffers during the backward pass)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m#   the bw by running our analysis first on the fw-only graph, and then on the joint graph. This would\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m#   require an extra round of tracing though, so it's more efficient to do in-line here.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:280\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn_with_anomaly\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    278\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly Detection has been enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(check_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:265\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m    259\u001b[0m             backward_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    260\u001b[0m                 needed_outs,\n\u001b[1;32m    261\u001b[0m                 grad_primals,\n\u001b[1;32m    262\u001b[0m                 allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m             )\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m             backward_out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                \u001b[49m\u001b[43mneeded_outs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrad_primals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeded_tangents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m backward_out_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(backward_out)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, [\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mnext\u001b[39m(backward_out_iter) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs_needs_grads\n\u001b[1;32m    274\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:445\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    443\u001b[0m overridable_args \u001b[38;5;241m=\u001b[39m t_outputs \u001b[38;5;241m+\u001b[39m t_inputs\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverridable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_inputs:\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/overrides.py:1719\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1719\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1240\u001b[0m, in \u001b[0;36mTorchFunctionMetadataMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_metadata \u001b[38;5;241m=\u001b[39m func\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts[func] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts\u001b[38;5;241m.\u001b[39mget(func, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1692\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39mflat_args):\n\u001b[0;32m-> 1692\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m \u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_prologue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpl_fn\u001b[39m(double_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1695\u001b[0m         out \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39m_backward_impl(ctx, all_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1925\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_prologue\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m   1907\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1908\u001b[0m         runtime_unwrap_tensor_subclasses(\n\u001b[1;32m   1909\u001b[0m             all_args[:tangents_start_idx],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         )\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1925\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1926\u001b[0m         (\n\u001b[1;32m   1927\u001b[0m             AOTDispatchAutograd\u001b[38;5;241m.\u001b[39mprocess_runtime_tangent(\n\u001b[1;32m   1928\u001b[0m                 t,\n\u001b[1;32m   1929\u001b[0m                 CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39msubclass_tangent_meta[\n\u001b[1;32m   1930\u001b[0m                     i \u001b[38;5;241m-\u001b[39m tangents_start_idx\n\u001b[1;32m   1931\u001b[0m                 ],\n\u001b[1;32m   1932\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1933\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (tangents_start_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m<\u001b[39m tangents_end_idx)\n\u001b[1;32m   1934\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m   1935\u001b[0m         )\n\u001b[1;32m   1936\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_args)\n\u001b[1;32m   1937\u001b[0m     ]\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# Backward with forward inputs mutations is not supported in double backward.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mindices_of_inputs_that_requires_grad_with_mutations_in_bw\n\u001b[1;32m   1943\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1927\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1907\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1908\u001b[0m         runtime_unwrap_tensor_subclasses(\n\u001b[1;32m   1909\u001b[0m             all_args[:tangents_start_idx],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         )\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1925\u001b[0m     all_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1926\u001b[0m         (\n\u001b[0;32m-> 1927\u001b[0m             \u001b[43mAOTDispatchAutograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_runtime_tangent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m                \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m                \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubclass_tangent_meta\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtangents_start_idx\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1933\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (tangents_start_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m<\u001b[39m tangents_end_idx)\n\u001b[1;32m   1934\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m   1935\u001b[0m         )\n\u001b[1;32m   1936\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_args)\n\u001b[1;32m   1937\u001b[0m     ]\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# Backward with forward inputs mutations is not supported in double backward.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mindices_of_inputs_that_requires_grad_with_mutations_in_bw\n\u001b[1;32m   1943\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1498\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.process_runtime_tangent\u001b[0;34m(x, meta)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         x \u001b[38;5;241m=\u001b[39m maybe_coerce(x)\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1498\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1499\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124mDuring the backward, we encountered a tensor subclass where we guessed its\u001b[39m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124mmetadata incorrectly.\u001b[39m\n\u001b[1;32m   1502\u001b[0m \n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124mExpected metadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(expected_meta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(expected_type)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \n\u001b[1;32m   1505\u001b[0m \u001b[38;5;124mRuntime metadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(runtime_meta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, runtime type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(runtime_type)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124mshape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(orig_x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;124mTo fix this, your tensor subclass must implement the dunder method __force_to_same_metadata__.\u001b[39m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m             )\n\u001b[1;32m   1512\u001b[0m         \u001b[38;5;66;03m# Coerce to expected memory format\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mis_contiguous(memory_format\u001b[38;5;241m=\u001b[39mmeta\u001b[38;5;241m.\u001b[39mmemory_format):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nDuring the backward, we encountered a tensor subclass where we guessed its\nmetadata incorrectly.\n\nExpected metadata: None, expected type: <class 'torch.Tensor'>\n\nRuntime metadata: None, runtime type: <class 'torch._subclasses.functional_tensor.FunctionalTensor'>\n\nshape: torch.Size([2, 3])\nTo fix this, your tensor subclass must implement the dunder method __force_to_same_metadata__.\n"
     ]
    }
   ],
   "source": [
    "# AOTAutograd of dynamo\n",
    "aot_dynamo_fn = aot_function(cached_func, fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n",
    "o = aot_dynamo_fn(*flat_fake_init, *flat_fake_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disassemble and modify the bytecode to load a function from kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
