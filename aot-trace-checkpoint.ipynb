{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "from torch_xla.utils.checkpoint import checkpoint\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn without checkpoint\n",
    "\n",
    "We obtain the forward of `fn` in terms of aten ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "device = torch_xla.device()\n",
    "w = torch.randn(4, 4, requires_grad=False, device=device)\n",
    "torch_xla.sync()\n",
    "\n",
    "def fn(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a = torch.randn(4, 4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch_xla checkpoint\n",
    "\n",
    "Runtime error within torch_xla checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in CheckpointFunctionBackward. Traceback of forward call that caused the error:\n",
      " (Triggered internally at /workspaces/torch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cloned_a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m torch_xla\u001b[38;5;241m.\u001b[39msync()\n\u001b[0;32m----> 9\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43maot_print_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloned_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:887\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m     (fake_mode, shape_env) \u001b[38;5;241m=\u001b[39m construct_fake_mode(flat_args, aot_config)\n\u001b[1;32m    884\u001b[0m     fake_flat_args: FakifiedFlatArgs \u001b[38;5;241m=\u001b[39m process_inputs(\n\u001b[1;32m    885\u001b[0m         flat_args, aot_config, fake_mode, shape_env\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 887\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:527\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    520\u001b[0m     flat_fn,\n\u001b[1;32m    521\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:778\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[1;32m    776\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 778\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:373\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    364\u001b[0m flat_fn, flat_args, fw_metadata \u001b[38;5;241m=\u001b[39m pre_compile(\n\u001b[1;32m    365\u001b[0m     wrappers,\n\u001b[1;32m    366\u001b[0m     flat_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m     fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m fw_metadata\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled()\n\u001b[0;32m--> 373\u001b[0m fx_g, joint_inputs, maybe_subclass_meta \u001b[38;5;241m=\u001b[39m \u001b[43maot_dispatch_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Copied from aot_dispatch_autograd_graph.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m disable_amp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:310\u001b[0m, in \u001b[0;36maot_dispatch_autograd_graph\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    305\u001b[0m saved_updated_joint_inputs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    306\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mdetach(), updated_joint_inputs\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m maybe_subclass_meta \u001b[38;5;241m=\u001b[39m subclass_tracing_info\u001b[38;5;241m.\u001b[39mmaybe_subclass_meta\n\u001b[0;32m--> 310\u001b[0m fx_g \u001b[38;5;241m=\u001b[39m \u001b[43m_create_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_fn_to_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_joint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m assert_functional_graph(fx_g\u001b[38;5;241m.\u001b[39mgraph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:55\u001b[0m, in \u001b[0;36m_create_graph\u001b[0;34m(f, args, aot_config)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_graph\u001b[39m(f, args, \u001b[38;5;241m*\u001b[39m, aot_config: AOTConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# FunctionalTensorMode must be enabled here.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# See Note [Accessing .grad_fn on FunctionalTensor]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_python_dispatcher(), FunctionalTensorMode(\n\u001b[1;32m     50\u001b[0m         pre_dispatch\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[1;32m     51\u001b[0m         export\u001b[38;5;241m=\u001b[39maot_config\u001b[38;5;241m.\u001b[39mis_export,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Allow token discovery for joint fn tracing as tokens can be used in backward.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         _allow_token_discovery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m     ):\n\u001b[0;32m---> 55\u001b[0m         fx_g \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fx_g\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2188\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 2188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2126\u001b[0m, in \u001b[0;36m_MakefxTracer.trace\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrace\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m fx\u001b[38;5;241m.\u001b[39mGraphModule:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_modes_from_inputs(f, args):\n\u001b[0;32m-> 2126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:2097\u001b[0m, in \u001b[0;36m_MakefxTracer._trace_inner\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfx_tracer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2097\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2103\u001b[0m     trace_structured(\n\u001b[1;32m   2104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifact\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2105\u001b[0m         metadata_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         )\u001b[38;5;241m.\u001b[39msrc,\n\u001b[1;32m   2115\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:721\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1137\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_trace\u001b[39m(\n\u001b[1;32m   1133\u001b[0m     root: Union[Module, Callable],\n\u001b[1;32m   1134\u001b[0m     tracer: Tracer,\n\u001b[1;32m   1135\u001b[0m     concrete_args: Optional[Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m-> 1137\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;66;03m# NB: be careful not to DCE .item() calls\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpure_pred\u001b[39m(n: fx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:843\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    837\u001b[0m             _autowrap_check(\n\u001b[1;32m    838\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    839\u001b[0m             )\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    841\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 843\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    844\u001b[0m             {},\n\u001b[1;32m    845\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:700\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    699\u001b[0m     tree_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(\u001b[38;5;28mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 700\u001b[0m     tree_out \u001b[38;5;241m=\u001b[39m \u001b[43mroot_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtree_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     out_args, out_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(tree_out)\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_codegen, _PyTreeCodeGen)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1192\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies, **_unused)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tensor_proxy_slot\u001b[39m(t: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Proxy]:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mproxy)\n\u001b[0;32m-> 1192\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(Tensor, get_tensor_proxy_slot, out)\n\u001b[1;32m   1194\u001b[0m out \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m   1195\u001b[0m     _AnyScriptObject, \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x), out\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:693\u001b[0m, in \u001b[0;36mhandle_effect_tokens_fn.<locals>.inner_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    690\u001b[0m         functional_tensor_mode\u001b[38;5;241m.\u001b[39m_tokens[k] \u001b[38;5;241m=\u001b[39m f_tokens[i]\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Return both the tokens and the outputs\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:644\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>.joint_helper\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoint_helper\u001b[39m(primals, tangents):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_functionalized_f_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:413\u001b[0m, in \u001b[0;36mcreate_functionalized_fn.<locals>._functionalized_f_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    410\u001b[0m     f_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map(to_fun, args)\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     f_outs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mf_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# We support a limited amount of mutation of graph inputs during the backward pass.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;66;03m# (This is used e.g. by Float8, which needs to update buffers during the backward pass)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m#   the bw by running our analysis first on the fw-only graph, and then on the joint graph. This would\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m#   require an extra round of tracing though, so it's more efficient to do in-line here.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:280\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn_with_anomaly\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    278\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly Detection has been enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(check_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:265\u001b[0m, in \u001b[0;36mcreate_joint.<locals>.inner_fn\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m    259\u001b[0m             backward_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    260\u001b[0m                 needed_outs,\n\u001b[1;32m    261\u001b[0m                 grad_primals,\n\u001b[1;32m    262\u001b[0m                 allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m             )\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m             backward_out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                \u001b[49m\u001b[43mneeded_outs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrad_primals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeded_tangents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m backward_out_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(backward_out)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, [\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mnext\u001b[39m(backward_out_iter) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs_needs_grads\n\u001b[1;32m    274\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:445\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    443\u001b[0m overridable_args \u001b[38;5;241m=\u001b[39m t_outputs \u001b[38;5;241m+\u001b[39m t_inputs\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverridable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_inputs:\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/overrides.py:1719\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1719\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1240\u001b[0m, in \u001b[0;36mTorchFunctionMetadataMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_metadata \u001b[38;5;241m=\u001b[39m func\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts[func] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mtorch_fn_counts\u001b[38;5;241m.\u001b[39mget(func, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py:139\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m chkpt_status\u001b[38;5;241m.\u001b[39min_chkpt_bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_is_checkpoint_valid():\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpointing is not compatible with .grad() or when an `inputs` parameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is passed to .backward(). Please use .backward() and do not pass its `inputs`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Copy the list to avoid modifying original list.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ctx\u001b[38;5;241m.\u001b[39minputs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument."
     ]
    }
   ],
   "source": [
    "import torch_xla.utils.checkpoint\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch_xla.utils.checkpoint.checkpoint(fn, a, w)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch non-reentrant checkpoint\n",
    "\n",
    "`aot_function` appears to skip over the checkpoint wrapper entirely. We still\n",
    "get back an identical aten forward that saves all the intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch.utils.checkpoint\n",
    "\n",
    "torch.xla = torch_xla.device()  # type:ignore\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn, a, w, use_reentrant=False)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo to trace the checkpointed function\n",
    "\n",
    "We get a higher order `checkpoint` op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name                       target                       args                            kwargs\n",
      "-------------  -------------------------  ---------------------------  ------------------------------  ------------------------\n",
      "placeholder    l_a_                       L_a_                         ()                              {}\n",
      "placeholder    l_w_                       L_w_                         ()                              {}\n",
      "get_attr       wrap_body_0                wrap_body_0                  ()                              {}\n",
      "call_function  tag_activation_checkpoint  tag_activation_checkpoint    (wrap_body_0, l_a_, l_w_)       {'use_reentrant': False}\n",
      "call_function  getitem                    <built-in function getitem>  (tag_activation_checkpoint, 0)  {}\n",
      "output         output                     output                       ((getitem,),)                   {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_a_ : torch.Tensor, L_w_ : torch.Tensor):\n",
      "    l_a_ = L_a_\n",
      "    l_w_ = L_w_\n",
      "    wrap_body_0 = self.wrap_body_0\n",
      "    tag_activation_checkpoint = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_0, l_a_, l_w_, use_reentrant = False);  wrap_body_0 = l_a_ = l_w_ = None\n",
      "    getitem = tag_activation_checkpoint[0];  tag_activation_checkpoint = None\n",
      "    return (getitem,)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6009, 0.4689, 0.3773, 0.6870],\n",
       "        [0.7962, 0.3210, 0.7358, 0.7793],\n",
       "        [0.7023, 0.3950, 0.6428, 0.6828],\n",
       "        [0.6641, 0.4357, 0.4625, 0.7164]], device='xla:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return gm.forward  # return a python callable\n",
    "  \n",
    "def fn_2(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "  \n",
    "def checkpointed_fn_2(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=False)\n",
    "\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_compiler, fullgraph=True)\n",
    "dynamo_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo and then AOTAutograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                  {}\n",
      "placeholder    primals_2  primals_2             ()                                  {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                        {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                    {}\n",
      "call_function  cos        aten.cos.default      (mm,)                               {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                             {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm);  mm = None\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                    kwargs\n",
      "-------------  ----------------  -----------------------------  ----------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                      {}\n",
      "placeholder    primals_2         primals_2                      ()                      {}\n",
      "placeholder    tangents_1        tangents_1                     ()                      {}\n",
      "call_function  sin               aten.sin.default               (primals_1,)            {}\n",
      "call_function  mm                aten.mm.default                (sin, primals_2)        {}\n",
      "call_function  cos               aten.cos.default               (mm,)                   {}\n",
      "call_function  mm_1              aten.mm.default                (cos, primals_2)        {}\n",
      "call_function  sigmoid           aten.sigmoid.default           (mm_1,)                 {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)              {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)               {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)             {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)             {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_1, detach_3)  {}\n",
      "call_function  t                 aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_2              aten.mm.default                (sigmoid_backward, t)   {}\n",
      "call_function  sin_1             aten.sin.default               (mm,)                   {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                {}\n",
      "call_function  mul               aten.mul.Tensor                (mm_2, neg)             {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t)                {}\n",
      "call_function  cos_1             aten.cos.default               (primals_1,)            {}\n",
      "call_function  mul_1             aten.mul.Tensor                (mm_3, cos_1)           {}\n",
      "output         output            output                         ((mul_1, None),)        {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, tangents_1):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t);  mul = t = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_backend, fullgraph=True)\n",
    "o = dynamo_fn(cloned_a, w)\n",
    "assert o is not None\n",
    "o.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
