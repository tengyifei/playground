{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_IR_DEBUG=1\n",
      "env: XLA_HLO_DEBUG=1\n",
      "env: PJRT_DEVICE=TPU\n"
     ]
    }
   ],
   "source": [
    "# Debugging flags\n",
    "%env XLA_IR_DEBUG=1\n",
    "%env XLA_HLO_DEBUG=1\n",
    "%env PJRT_DEVICE=TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn without checkpoint\n",
    "\n",
    "We obtain the forward of `fn` in terms of aten ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "device = torch_xla.device()\n",
    "w = torch.randn(4, 4, requires_grad=False, device=device)\n",
    "torch_xla.sync()\n",
    "\n",
    "def fn(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  print(\"a:\", type(a), a.shape)\n",
    "  time.sleep(1)\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "\n",
    "def compiler_fn(m: torch.fx.GraphModule, _):\n",
    "  print(m.code)\n",
    "  return m\n",
    "\n",
    "a = torch.randn(4, 4, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch_xla checkpoint\n",
    "\n",
    "Runtime error within torch_xla checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in CheckpointFunctionBackward. Traceback of forward call that caused the error:\n",
      " (Triggered internally at /workspaces/torch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "ERROR:root:Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_393588/1073587478.py\", line 12, in <module>\n",
      "    res = aot_print_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 887, in returned_function\n",
      "    compiled_fn, _ = create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 527, in create_aot_dispatcher_function\n",
      "    return _create_aot_dispatcher_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 778, in _create_aot_dispatcher_function\n",
      "    compiled_fn, fw_metadata = compiler_fn(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 373, in aot_dispatch_autograd\n",
      "    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 310, in aot_dispatch_autograd_graph\n",
      "    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py\", line 55, in _create_graph\n",
      "    fx_g = make_fx(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2188, in wrapped\n",
      "    return make_fx_tracer.trace(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2126, in trace\n",
      "    return self._trace_inner(f, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2097, in _trace_inner\n",
      "    t = dispatch_trace(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 721, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1137, in dispatch_trace\n",
      "    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 843, in trace\n",
      "    (self.create_arg(fn(*args)),),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 700, in flatten_fn\n",
      "    tree_out = root_fn(*tree_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1192, in wrapped\n",
      "    out = f(*tensors)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 693, in inner_fn\n",
      "    outs = fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 644, in joint_helper\n",
      "    return _functionalized_f_helper(primals, tangents)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 413, in _functionalized_f_helper\n",
      "    f_outs = fn(*f_args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 280, in inner_fn_with_anomaly\n",
      "    return inner_fn(*args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 265, in inner_fn\n",
      "    backward_out = torch.autograd.grad(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 445, in grad\n",
      "    return handle_torch_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/overrides.py\", line 1719, in handle_torch_function\n",
      "    result = mode.__torch_function__(public_api, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1240, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 496, in grad\n",
      "    result = _engine_run_backward(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/autograd/function.py\", line 307, in apply\n",
      "    return user_fn(self, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 139, in backward\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch_xla.utils.checkpoint\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch_xla.utils.checkpoint.checkpoint(fn, a, w)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "\n",
    "try:\n",
    "  res = aot_print_fn(cloned_a, w)\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace fn with a torch non-reentrant checkpoint\n",
    "\n",
    "`aot_function` appears to skip over the checkpoint wrapper entirely. We still\n",
    "get back an identical aten forward that saves all the intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: <class 'torch._subclasses.functional_tensor.FunctionalTensor'> torch.Size([4, 4])\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch.utils.checkpoint\n",
    "\n",
    "torch.xla = torch_xla.device()  # type:ignore\n",
    "\n",
    "def checkpointed_fn(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn, a, w, use_reentrant=False)\n",
    "\n",
    "aot_print_fn = aot_function(checkpointed_fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "cloned_a = a.clone().detach().requires_grad_(True)\n",
    "torch_xla.sync()\n",
    "res = aot_print_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo to trace the checkpointed function\n",
    "\n",
    "We get a higher order `checkpoint` op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name                       target                       args                            kwargs\n",
      "-------------  -------------------------  ---------------------------  ------------------------------  ------------------------\n",
      "placeholder    l_a_                       L_a_                         ()                              {}\n",
      "placeholder    l_w_                       L_w_                         ()                              {}\n",
      "get_attr       wrap_body_0                wrap_body_0                  ()                              {}\n",
      "call_function  tag_activation_checkpoint  tag_activation_checkpoint    (wrap_body_0, l_a_, l_w_)       {'use_reentrant': False}\n",
      "call_function  getitem                    <built-in function getitem>  (tag_activation_checkpoint, 0)  {}\n",
      "output         output                     output                       ((getitem,),)                   {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_a_ : torch.Tensor, L_w_ : torch.Tensor):\n",
      "    l_a_ = L_a_\n",
      "    l_w_ = L_w_\n",
      "    wrap_body_0 = self.wrap_body_0\n",
      "    tag_activation_checkpoint = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_0, l_a_, l_w_, use_reentrant = False);  wrap_body_0 = l_a_ = l_w_ = None\n",
      "    getitem = tag_activation_checkpoint[0];  tag_activation_checkpoint = None\n",
      "    return (getitem,)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5832, 0.6852, 0.6427, 0.7385],\n",
       "        [0.7401, 0.1634, 0.5956, 0.1051],\n",
       "        [0.6172, 0.5593, 0.6466, 0.6149],\n",
       "        [0.3401, 0.9659, 0.6556, 0.6364]], device='xla:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return gm.forward  # return a python callable\n",
    "  \n",
    "def fn_2(a, w):\n",
    "  \"\"\"A simple function containing a few layers.\"\"\"\n",
    "  a = torch.sin(a)\n",
    "  a = a @ w\n",
    "  a = torch.cos(a)\n",
    "  a = a @ w\n",
    "  a = torch.sigmoid(a)\n",
    "  return a\n",
    "  \n",
    "def checkpointed_fn_2(a, w):\n",
    "  return torch.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=False)\n",
    "\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_compiler, fullgraph=True)\n",
    "dynamo_fn(cloned_a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamo and then AOTAutograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                kwargs\n",
      "-------------  ---------  --------------------  ----------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                  {}\n",
      "placeholder    primals_2  primals_2             ()                                  {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                        {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                    {}\n",
      "call_function  cos        aten.cos.default      (mm,)                               {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                    {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                             {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm);  mm = None\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                    kwargs\n",
      "-------------  ----------------  -----------------------------  ----------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                      {}\n",
      "placeholder    primals_2         primals_2                      ()                      {}\n",
      "placeholder    tangents_1        tangents_1                     ()                      {}\n",
      "call_function  sin               aten.sin.default               (primals_1,)            {}\n",
      "call_function  mm                aten.mm.default                (sin, primals_2)        {}\n",
      "call_function  cos               aten.cos.default               (mm,)                   {}\n",
      "call_function  mm_1              aten.mm.default                (cos, primals_2)        {}\n",
      "call_function  sigmoid           aten.sigmoid.default           (mm_1,)                 {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)              {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)               {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)             {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)             {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_1, detach_3)  {}\n",
      "call_function  t                 aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_2              aten.mm.default                (sigmoid_backward, t)   {}\n",
      "call_function  sin_1             aten.sin.default               (mm,)                   {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                {}\n",
      "call_function  mul               aten.mul.Tensor                (mm_2, neg)             {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t)                {}\n",
      "call_function  cos_1             aten.cos.default               (primals_1,)            {}\n",
      "call_function  mul_1             aten.mul.Tensor                (mm_3, cos_1)           {}\n",
      "output         output            output                         ((mul_1, None),)        {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, tangents_1):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t);  mul = t = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"my_compiler() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    print()\n",
    "    print(\"FX graph code:\")\n",
    "    print(gm.code)\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    return make_boxed_func(gm.forward)\n",
    "  \n",
    "my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_backend, fullgraph=True)\n",
    "o = dynamo_fn(cloned_a, w)\n",
    "assert o is not None\n",
    "o.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py:641: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.\n",
      "  warnings.warn(\n",
      "ERROR:root:'skip function check_backward_validity in file /usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py'\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_393588/204424578.py\", line 2, in checkpointed_fn_3\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 87, in forward\n",
      "    check_backward_validity(args)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_393588/204424578.py\", line 7, in <module>\n",
      "    o = dynamo_fn(cloned_a, w)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 556, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1447, in __call__\n",
      "    return self._torchdynamo_orig_callable(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 550, in __call__\n",
      "    return _compile(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 979, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 709, in compile_inner\n",
      "    return _compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 744, in _compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1348, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 234, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2914, in run\n",
      "    super().run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1828, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 349, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 125, in call_function\n",
      "    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 973, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3129, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3257, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1816, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "    return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "    return self.call_apply(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "    ).call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2359, in call_function\n",
      "    (fwd_out, _), fwd_graph, fwd_freevars = speculate_subgraph(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 624, in speculate_subgraph\n",
      "    raise ex\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 489, in speculate_subgraph\n",
      "    output = f.call_function(tx, args, sub_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 349, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 125, in call_function\n",
      "    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 973, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3129, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3257, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n",
      "    while self.step():\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 640, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1738, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 967, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 751, in call_function\n",
      "    unimplemented(msg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 313, in unimplemented\n",
      "    raise Unsupported(msg, case_name=case_name)\n",
      "torch._dynamo.exc.Unsupported: 'skip function check_backward_validity in file /usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py'\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_393588/204424578.py\", line 2, in checkpointed_fn_3\n",
      "    return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 292, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/workspaces/torch/pytorch/xla/torch_xla/utils/checkpoint.py\", line 87, in forward\n",
      "    check_backward_validity(args)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def checkpointed_fn_3(a, w):\n",
    "  return torch_xla.utils.checkpoint.checkpoint(fn_2, a, w, use_reentrant=True)\n",
    "\n",
    "try:\n",
    "  my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "  dynamo_fn = torch.compile(checkpointed_fn_3, backend=my_backend, fullgraph=True)\n",
    "  o = dynamo_fn(cloned_a, w)\n",
    "  assert o is not None\n",
    "  o.sum().backward()\n",
    "except RuntimeError as e:\n",
    "  logging.exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch.export with AOTAutograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, args_0: \"f32[4, 4]\", args_1: \"f32[4, 4]\"):\n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:17 in fn_2, code: a = torch.sin(a)\n",
      "            sin: \"f32[4, 4]\" = torch.ops.aten.sin.default(args_0);  args_0 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:18 in fn_2, code: a = a @ w\n",
      "            matmul: \"f32[4, 4]\" = torch.ops.aten.matmul.default(sin, args_1);  sin = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:19 in fn_2, code: a = torch.cos(a)\n",
      "            cos: \"f32[4, 4]\" = torch.ops.aten.cos.default(matmul);  matmul = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:20 in fn_2, code: a = a @ w\n",
      "            matmul_1: \"f32[4, 4]\" = torch.ops.aten.matmul.default(cos, args_1);  cos = args_1 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:21 in fn_2, code: a = torch.sigmoid(a)\n",
      "            sigmoid: \"f32[4, 4]\" = torch.ops.aten.sigmoid.default(matmul_1);  matmul_1 = None\n",
      "            return (sigmoid,)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='sigmoid'), target=None)])\n",
      "Range constraints: {}\n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                             kwargs\n",
      "-------------  ---------  --------------------  -----------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                               {}\n",
      "placeholder    primals_2  primals_2             ()                                               {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                                     {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                                 {}\n",
      "call_function  cos        aten.cos.default      (mm,)                                            {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                                 {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                                          {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2, mm, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name              target                         args                    kwargs\n",
      "-------------  ----------------  -----------------------------  ----------------------  --------\n",
      "placeholder    primals_1         primals_1                      ()                      {}\n",
      "placeholder    primals_2         primals_2                      ()                      {}\n",
      "placeholder    mm                mm                             ()                      {}\n",
      "placeholder    sigmoid           sigmoid                        ()                      {}\n",
      "placeholder    tangents_1        tangents_1                     ()                      {}\n",
      "call_function  detach            aten.detach.default            (sigmoid,)              {}\n",
      "call_function  detach_1          aten.detach.default            (detach,)               {}\n",
      "call_function  detach_2          aten.detach.default            (detach_1,)             {}\n",
      "call_function  detach_3          aten.detach.default            (detach_2,)             {}\n",
      "call_function  sigmoid_backward  aten.sigmoid_backward.default  (tangents_1, detach_3)  {}\n",
      "call_function  t                 aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_2              aten.mm.default                (sigmoid_backward, t)   {}\n",
      "call_function  sin_1             aten.sin.default               (mm,)                   {}\n",
      "call_function  neg               aten.neg.default               (sin_1,)                {}\n",
      "call_function  mul               aten.mul.Tensor                (mm_2, neg)             {}\n",
      "call_function  t_1               aten.t.default                 (primals_2,)            {}\n",
      "call_function  mm_3              aten.mm.default                (mul, t_1)              {}\n",
      "call_function  cos_1             aten.cos.default               (primals_1,)            {}\n",
      "call_function  mul_1             aten.mul.Tensor                (mm_3, cos_1)           {}\n",
      "output         output            output                         ((mul_1, None),)        {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, mm, sigmoid, tangents_1):\n",
      "    detach = torch.ops.aten.detach.default(sigmoid);  sigmoid = None\n",
      "    detach_1 = torch.ops.aten.detach.default(detach);  detach = None\n",
      "    detach_2 = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
      "    detach_3 = torch.ops.aten.detach.default(detach_2);  detach_2 = None\n",
      "    sigmoid_backward = torch.ops.aten.sigmoid_backward.default(tangents_1, detach_3);  tangents_1 = detach_3 = None\n",
      "    t = torch.ops.aten.t.default(primals_2)\n",
      "    mm_2 = torch.ops.aten.mm.default(sigmoid_backward, t);  sigmoid_backward = t = None\n",
      "    sin_1 = torch.ops.aten.sin.default(mm);  mm = None\n",
      "    neg = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "    mul = torch.ops.aten.mul.Tensor(mm_2, neg);  mm_2 = neg = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_3 = torch.ops.aten.mm.default(mul, t_1);  mul = t_1 = None\n",
      "    cos_1 = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(mm_3, cos_1);  mm_3 = cos_1 = None\n",
      "    return (mul_1, None)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.export\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "  return make_boxed_func(gm.forward)\n",
    "  \n",
    "class FunctionModule(torch.nn.Module):\n",
    "  def __init__(self, f):\n",
    "    super().__init__()\n",
    "    self.f = f\n",
    "  \n",
    "  def forward(self, *args):\n",
    "    return self.f(*args)\n",
    "\n",
    "exported = torch.export.export_for_training(FunctionModule(checkpointed_fn_2), args=(cloned_a, w))\n",
    "print(exported)\n",
    "module = exported.module()\n",
    "\n",
    "# Now get the backward function\n",
    "from functorch.compile import aot_module\n",
    "aot_module(module, fw_compiler=my_compiler, bw_compiler=my_compiler)(cloned_a, w).sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use torch.compile with an exception trick to avoid evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='compiler_fn' raised:\nAssertionError: fake mode (<torch._subclasses.fake_tensor.FakeTensorMode object at 0x75aa061c6ce0>) from tracing context 0 doesn't match mode (<torch._subclasses.fake_tensor.FakeTensorMode object at 0x75a9e3f31870>) from fake tensor input 0\n\nfake mode from tracing context 0 allocated at:\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_393588/2443371153.py\", line 23, in <module>\n    o = dynamo_fn(fake_a, fake_w)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 556, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1447, in __call__\n    return self._torchdynamo_orig_callable(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 550, in __call__\n    return _compile(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 979, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 709, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 744, in _compile_inner\n    out_code = transform_code_object(code, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1348, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 234, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n    tracer.run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2914, in run\n    super().run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n    while self.step():\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3105, in RETURN_VALUE\n    self._return(inst)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3090, in _return\n    self.output.compile_subgraph(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1077, in compile_subgraph\n    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1337, in compile_and_call_fx_graph\n    backend_fake_mode = torch._subclasses.FakeTensorMode(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1204, in __init__\n    self._stack_trace = traceback.extract_stack()\n\nfake mode from fake tensor input 0 allocated at:\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_393588/2443371153.py\", line 18, in <module>\n    with FakeTensorMode():\n  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1204, in __init__\n    self._stack_trace = traceback.extract_stack()\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m my_backend \u001b[38;5;241m=\u001b[39m aot_autograd(fw_compiler\u001b[38;5;241m=\u001b[39mmy_compiler)  \u001b[38;5;66;03m# bw_compiler=my_compiler\u001b[39;00m\n\u001b[1;32m     22\u001b[0m dynamo_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(checkpointed_fn_2, backend\u001b[38;5;241m=\u001b[39mmy_backend, fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43mdynamo_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m o\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:556\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    552\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    560\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1447\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1442\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1443\u001b[0m             )\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:550\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    547\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:979\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    977\u001b[0m codecache_metrics\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 979\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     put_code_state()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:709\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    705\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    706\u001b[0m         _WaitCounter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch.wait_counter.dynamo_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mguard()\n\u001b[1;32m    707\u001b[0m     )\n\u001b[1;32m    708\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_utils_internal.py:95\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     98\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     99\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:744\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    742\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1348\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1345\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1346\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1348\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:234\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:663\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 663\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    665\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2914\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2914\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1120\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1032\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3105\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 3105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3090\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3085\u001b[0m _step_logger()(\n\u001b[1;32m   3086\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   3087\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3088\u001b[0m )\n\u001b[1;32m   3089\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 3090\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3097\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   3100\u001b[0m )\n\u001b[1;32m   3101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1077\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1074\u001b[0m append_prefix_insts()\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m-> 1077\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m   1079\u001b[0m )\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m   1082\u001b[0m     [\n\u001b[1;32m   1083\u001b[0m         PyCodegen(tx, overridden_sources\u001b[38;5;241m=\u001b[39moverridden_sources)\u001b[38;5;241m.\u001b[39mcreate_store(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     ]\n\u001b[1;32m   1088\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1346\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1346\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1395\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1392\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1393\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1394\u001b[0m     ):\n\u001b[0;32m-> 1395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1444\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1445\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1446\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m signpost_event(\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     },\n\u001b[1;32m   1457\u001b[0m )\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1425\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1424\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1425\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:130\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:130\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/__init__.py:2346\u001b[0m, in \u001b[0;36m_TorchCompileWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m-> 2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:72\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 72\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1073\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[1;32m   1058\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m   1059\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     cache_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1072\u001b[0m )\n\u001b[0;32m-> 1073\u001b[0m fake_mode, shape_env \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_fake_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m fake_flat_args \u001b[38;5;241m=\u001b[39m process_inputs(full_args, aot_config, fake_mode, shape_env)\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_and_compile\u001b[39m():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:510\u001b[0m, in \u001b[0;36mconstruct_fake_mode\u001b[0;34m(flat_args, aot_config)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_fake_mode\u001b[39m(\n\u001b[1;32m    508\u001b[0m     flat_args: List[Any], aot_config: AOTConfig\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[FakeTensorMode, Optional[ShapeEnv]]:\n\u001b[0;32m--> 510\u001b[0m     fake_mode \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_fake_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fake_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m         shape_env \u001b[38;5;241m=\u001b[39m ShapeEnv() \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mdynamic_shapes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_guards.py:974\u001b[0m, in \u001b[0;36mdetect_fake_mode\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    972\u001b[0m     fake_mode, desc1, i1 \u001b[38;5;241m=\u001b[39m fake_modes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m, desc2, i2 \u001b[38;5;129;01min\u001b[39;00m fake_modes[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m--> 974\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m fake_mode \u001b[38;5;129;01mis\u001b[39;00m m, (\n\u001b[1;32m    975\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake mode (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfake_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match mode (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake mode from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m allocated at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfake_mode\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    977\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake mode from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m allocated at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    978\u001b[0m         )\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fake_mode\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='compiler_fn' raised:\nAssertionError: fake mode (<torch._subclasses.fake_tensor.FakeTensorMode object at 0x75aa061c6ce0>) from tracing context 0 doesn't match mode (<torch._subclasses.fake_tensor.FakeTensorMode object at 0x75a9e3f31870>) from fake tensor input 0\n\nfake mode from tracing context 0 allocated at:\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_393588/2443371153.py\", line 23, in <module>\n    o = dynamo_fn(fake_a, fake_w)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 556, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1447, in __call__\n    return self._torchdynamo_orig_callable(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 550, in __call__\n    return _compile(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 979, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 709, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 744, in _compile_inner\n    out_code = transform_code_object(code, transform)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1348, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 234, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n    tracer.run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2914, in run\n    super().run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1120, in run\n    while self.step():\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1032, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3105, in RETURN_VALUE\n    self._return(inst)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3090, in _return\n    self.output.compile_subgraph(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1077, in compile_subgraph\n    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1337, in compile_and_call_fx_graph\n    backend_fake_mode = torch._subclasses.FakeTensorMode(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1204, in __init__\n    self._stack_trace = traceback.extract_stack()\n\nfake mode from fake tensor input 0 allocated at:\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/root/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/root/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_393588/2443371153.py\", line 18, in <module>\n    with FakeTensorMode():\n  File \"/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1204, in __init__\n    self._stack_trace = traceback.extract_stack()\n\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "  return make_boxed_func(gm.forward)\n",
    "  \n",
    "with FakeTensorMode():\n",
    "  fake_a = torch.randn(4, 4)\n",
    "  fake_w = torch.randn(4, 4)\n",
    "  my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n",
    "  dynamo_fn = torch.compile(checkpointed_fn_2, backend=my_backend, fullgraph=True)\n",
    "  o = dynamo_fn(fake_a, fake_w)\n",
    "  assert o is not None\n",
    "  o.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, args_0: \"f32[4, 4]\", args_1: \"f32[4, 4]\"):\n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:17 in fn_2, code: a = torch.sin(a)\n",
      "            sin: \"f32[4, 4]\" = torch.ops.aten.sin.default(args_0);  args_0 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:18 in fn_2, code: a = a @ w\n",
      "            matmul: \"f32[4, 4]\" = torch.ops.aten.matmul.default(sin, args_1);  sin = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:19 in fn_2, code: a = torch.cos(a)\n",
      "            cos: \"f32[4, 4]\" = torch.ops.aten.cos.default(matmul);  matmul = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:20 in fn_2, code: a = a @ w\n",
      "            matmul_1: \"f32[4, 4]\" = torch.ops.aten.matmul.default(cos, args_1);  cos = args_1 = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_393588/1239672230.py:21 in fn_2, code: a = torch.sigmoid(a)\n",
      "            sigmoid: \"f32[4, 4]\" = torch.ops.aten.sigmoid.default(matmul_1);  matmul_1 = None\n",
      "            return (sigmoid,)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_0'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='args_1'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='sigmoid'), target=None)])\n",
      "Range constraints: {}\n",
      "\n",
      "my_compiler() called with FX graph:\n",
      "opcode         name       target                args                                             kwargs\n",
      "-------------  ---------  --------------------  -----------------------------------------------  --------\n",
      "placeholder    primals_1  primals_1             ()                                               {}\n",
      "placeholder    primals_2  primals_2             ()                                               {}\n",
      "call_function  sin        aten.sin.default      (primals_1,)                                     {}\n",
      "call_function  mm         aten.mm.default       (sin, primals_2)                                 {}\n",
      "call_function  cos        aten.cos.default      (mm,)                                            {}\n",
      "call_function  mm_1       aten.mm.default       (cos, primals_2)                                 {}\n",
      "call_function  sigmoid    aten.sigmoid.default  (mm_1,)                                          {}\n",
      "output         output     output                ((sigmoid, primals_1, primals_2, mm, sigmoid),)  {}\n",
      "\n",
      "FX graph code:\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    sin = torch.ops.aten.sin.default(primals_1)\n",
      "    mm = torch.ops.aten.mm.default(sin, primals_2);  sin = None\n",
      "    cos = torch.ops.aten.cos.default(mm)\n",
      "    mm_1 = torch.ops.aten.mm.default(cos, primals_2);  cos = None\n",
      "    sigmoid = torch.ops.aten.sigmoid.default(mm_1);  mm_1 = None\n",
      "    return (sigmoid, primals_1, primals_2, mm, sigmoid)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.sin.default(tensor([...], device='xla:0', size=(4, 4)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Now get the backward function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aot_module\n\u001b[0;32m---> 35\u001b[0m \u001b[43maot_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_compiler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_compiler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloned_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:945\u001b[0m, in \u001b[0;36maot_module.<locals>.AOTModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_f\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamed_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamed_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:897\u001b[0m, in \u001b[0;36maot_function.<locals>.returned_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m     cached_res \u001b[38;5;241m=\u001b[39m (compiled_fn, out_spec)\n\u001b[1;32m    896\u001b[0m cached_fn, out_spec \u001b[38;5;241m=\u001b[39m cached_res\n\u001b[0;32m--> 897\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_spec\u001b[38;5;241m.\u001b[39munflatten(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:309\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     ), torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 309\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     grad_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1580\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     ctx\u001b[38;5;241m.\u001b[39m_compiled_autograd_backward_state \u001b[38;5;241m=\u001b[39m bw_state\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[0;32m-> 1580\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1586\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   1587\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:489\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    482\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    483\u001b[0m         runtime_metadata,\n\u001b[1;32m    484\u001b[0m         out,\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    486\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:671\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    668\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    669\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 671\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<eval_with_key>.61:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, primals_1, primals_2)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, primals_1, primals_2):\n\u001b[0;32m----> 5\u001b[0m     sin \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     mm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mmm\u001b[38;5;241m.\u001b[39mdefault(sin, primals_2);  sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mcos\u001b[38;5;241m.\u001b[39mdefault(mm)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_ops.py:723\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1271\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1268\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m ), func\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1273\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1813\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1381\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache_bypasses[e\u001b[38;5;241m.\u001b[39mreason] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m _UNASSIGNED:\n\u001b[0;32m-> 1381\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1911\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         avoiding_device_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Recompute flat_arg_fake_tensors here again in case some of the inputs\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;66;03m# were real tensors and fakified in validate_and_convert_non_fake_tensors\u001b[39;00m\n\u001b[0;32m-> 1911\u001b[0m (flat_args, flat_arg_fake_tensors) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_and_convert_non_fake_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m args, kwargs  \u001b[38;5;66;03m# Invalidated\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# The current constant handling only support tracing systems\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# (aot autograd, torchdynamo) where each operation is run consecutively.\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Because each operation is run in order, we can trace out and support\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m \n\u001b[1;32m   1925\u001b[0m \u001b[38;5;66;03m# We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2378\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors\u001b[0;34m(self, func, converter, flat_args, args_spec)\u001b[0m\n\u001b[1;32m   2375\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2378\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [validate(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2375\u001b[0m     flat_arg_fake_tensors\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2378\u001b[0m validated_args \u001b[38;5;241m=\u001b[39m [\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m flat_args]\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validated_args, flat_arg_fake_tensors\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:2366\u001b[0m, in \u001b[0;36mFakeTensorMode.validate_and_convert_non_fake_tensors.<locals>.validate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2364\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing fake modes NYI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2365\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_args, args_spec)\n\u001b[0;32m-> 2366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m   2367\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease convert all Tensors to FakeTensors first or instantiate FakeTensorMode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2368\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_non_fake_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_call(func,\u001b[38;5;250m \u001b[39margs,\u001b[38;5;250m \u001b[39mkwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2369\u001b[0m         )\n\u001b[1;32m   2371\u001b[0m     out \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mfrom_real_tensor(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.sin.default(tensor([...], device='xla:0', size=(4, 4)))"
     ]
    }
   ],
   "source": [
    "import torch.export\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func  # type:ignore\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "  print(\"my_compiler() called with FX graph:\")\n",
    "  gm.graph.print_tabular()\n",
    "  print()\n",
    "  print(\"FX graph code:\")\n",
    "  print(gm.code)\n",
    "  print()\n",
    "  time.sleep(1)\n",
    "  return make_boxed_func(gm.forward)\n",
    "  \n",
    "class FunctionModule(torch.nn.Module):\n",
    "  def __init__(self, f):\n",
    "    super().__init__()\n",
    "    self.f = f\n",
    "  \n",
    "  def forward(self, *args):\n",
    "    return self.f(*args)\n",
    "\n",
    "with FakeTensorMode():\n",
    "  fake_a = torch.randn(4, 4)\n",
    "  fake_w = torch.randn(4, 4)\n",
    "  exported = torch.export.export_for_training(FunctionModule(checkpointed_fn_2), args=(fake_a, fake_w))\n",
    "  print(exported)\n",
    "  module = exported.module()\n",
    "\n",
    "  # Now get the backward function\n",
    "  from functorch.compile import aot_module\n",
    "  aot_module(module, fw_compiler=my_compiler, bw_compiler=my_compiler)(cloned_a, w).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
