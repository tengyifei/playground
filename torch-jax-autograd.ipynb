{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def fwd_and_bwd(f):\n",
    "  def fwd(*args):\n",
    "    return jax.vjp(f, *args)\n",
    "  def bwd(f_vjp, out_grad):\n",
    "    return f_vjp(out_grad)\n",
    "  return fwd, bwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def foo(inp):\n",
    "  a, b = inp\n",
    "  return jnp.sin(a) @ jnp.cos(b)\n",
    "\n",
    "fwd, bwd = fwd_and_bwd(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jax.random.normal(jax.random.key(1), shape=(2, 2))\n",
    "b = jax.random.normal(jax.random.key(2), shape=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_primals, partial = fwd((a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partially evaluated VJP function is actually a PyTree. The jaxpr that is run\n",
    "backwards later is in the tree metadata. See discussion: https://github.com/jax-ml/jax/issues/26579#issuecomment-2670531713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "flat_residual, spec = tree_flatten(partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[0.98809826, 0.9964145 ],\n",
       "        [0.9907689 , 0.98800594]], dtype=float32),\n",
       " Array([[ 0.3528105 ,  0.9594295 ],\n",
       "        [-0.6733448 ,  0.92576915]], dtype=float32),\n",
       " Array([[0.9356948 , 0.28194872],\n",
       "        [0.7393286 , 0.3780893 ]], dtype=float32),\n",
       " Array([[-0.15382399,  0.08460601],\n",
       "        [-0.13556181, -0.15441589]], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are simple arrays now\n",
    "flat_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef(CustomNode(Partial[_HashableCallableShim(functools.partial(<function _vjp_pullback_wrapper at 0x7f1f82f305e0>, 'foo', [ShapedArray(float32[2,2])], (PyTreeDef(*), PyTreeDef(((*, *),)))))], [(CustomNode(Partial[_HashableCallableShim(functools.partial(<function vjp.<locals>.unbound_vjp at 0x7f1f8290add0>, [(ShapedArray(float32[2,2]), None)], { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22ma\u001b[35m:f32[2,2]\u001b[39m b\u001b[35m:f32[2,2]\u001b[39m c\u001b[35m:f32[2,2]\u001b[39m d\u001b[35m:f32[2,2]\u001b[39m; e\u001b[35m:f32[2,2]\u001b[39m f\u001b[35m:f32[2,2]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mg\u001b[35m:f32[2,2]\u001b[39m = pjit[\n",
       "      name=sin\n",
       "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; e\u001b[35m:f32[2,2]\u001b[39m a\u001b[35m:f32[2,2]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m g\u001b[35m:f32[2,2]\u001b[39m = mul e a \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(g,) }\n",
       "    ] e a\n",
       "    h\u001b[35m:f32[2,2]\u001b[39m = pjit[\n",
       "      name=cos\n",
       "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; f\u001b[35m:f32[2,2]\u001b[39m b\u001b[35m:f32[2,2]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "          \u001b[39m\u001b[22m\u001b[22mi\u001b[35m:f32[2,2]\u001b[39m = mul f b\n",
       "          h\u001b[35m:f32[2,2]\u001b[39m = neg i\n",
       "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(h,) }\n",
       "    ] f b\n",
       "    j\u001b[35m:f32[2,2]\u001b[39m = pjit[\n",
       "      name=matmul\n",
       "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; g\u001b[35m:f32[2,2]\u001b[39m h\u001b[35m:f32[2,2]\u001b[39m c\u001b[35m:f32[2,2]\u001b[39m d\u001b[35m:f32[2,2]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "          \u001b[39m\u001b[22m\u001b[22mk\u001b[35m:f32[2,2]\u001b[39m = dot_general[\n",
       "            dimension_numbers=(([1], [0]), ([], []))\n",
       "            preferred_element_type=float32\n",
       "          ] g c\n",
       "          l\u001b[35m:f32[2,2]\u001b[39m = dot_general[\n",
       "            dimension_numbers=(([1], [0]), ([], []))\n",
       "            preferred_element_type=float32\n",
       "          ] d h\n",
       "          j\u001b[35m:f32[2,2]\u001b[39m = add_any k l\n",
       "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }\n",
       "    ] g h c d\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }))], [((*, *, *, *),), {}]),), {}]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see the jaxpr in the tree spec\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/lucidrains/jax2torch/blob/main/jax2torch/jax2torch.py, which is\n",
    "# inspired by https://gist.github.com/mattjj/e8b51074fed081d765d2f3ff90edf0e9\n",
    "\n",
    "import torch\n",
    "from torch.utils import dlpack as torch_dlpack\n",
    "\n",
    "import jax\n",
    "from jax import dlpack as jax_dlpack\n",
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_map, tree_flatten, tree_unflatten\n",
    "\n",
    "from inspect import signature\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def j2t(x_jax):\n",
    "  x_torch = torch_dlpack.from_dlpack(jax_dlpack.to_dlpack(x_jax))\n",
    "  return x_torch\n",
    "\n",
    "\n",
    "def t2j(x_torch):\n",
    "  x_torch = x_torch.contiguous()  # https://github.com/google/jax/issues/8082\n",
    "  x_jax = jax_dlpack.from_dlpack(torch_dlpack.to_dlpack(x_torch))\n",
    "  return x_jax\n",
    "\n",
    "\n",
    "def tree_t2j(x_torch):\n",
    "  return tree_map(lambda t: t2j(t)\n",
    "                  if isinstance(t, torch.Tensor) else t, x_torch)\n",
    "\n",
    "\n",
    "def tree_j2t(x_jax):\n",
    "  return tree_map(lambda t: j2t(t) if isinstance(t, jnp.ndarray) else t, x_jax)\n",
    "\n",
    "\n",
    "def jax2torch(fn):\n",
    "\n",
    "  @wraps(fn)\n",
    "  def inner(*args, **kwargs):\n",
    "\n",
    "    class JaxFun(torch.autograd.Function):\n",
    "\n",
    "      @staticmethod\n",
    "      def forward(ctx, *args):\n",
    "        args = tree_t2j(args)\n",
    "        y_, fun_vjp = jax.vjp(fn, *args)\n",
    "        residuals, ctx.vjp_spec = tree_flatten(fun_vjp)\n",
    "        ctx.save_for_backward(*map(j2t, residuals))\n",
    "        return tree_j2t(y_)\n",
    "\n",
    "      @staticmethod\n",
    "      def backward(ctx, *grad_args):\n",
    "        fun_vjp = tree_unflatten(ctx.vjp_spec, map(t2j, ctx.saved_tensors))\n",
    "        grad_args = tree_t2j(grad_args) if len(grad_args) > 1 else t2j(grad_args[0])\n",
    "        grads = fun_vjp(grad_args)\n",
    "        grads = tuple(\n",
    "            map(lambda t: t if isinstance(t, jnp.ndarray) else None, grads))\n",
    "        return tree_j2t(grads)\n",
    "\n",
    "    sig = signature(fn)\n",
    "    bound = sig.bind(*args, **kwargs)\n",
    "    bound.apply_defaults()\n",
    "    return JaxFun.apply(*bound.arguments.values())\n",
    "\n",
    "  return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  8., 27.])\n",
      "tensor([12., 27.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1081037/2466919997.py:23: DeprecationWarning: Calling from_dlpack with a DLPack tensor is deprecated. The argument to from_dlpack should be an array from another framework that implements the __dlpack__ protocol.\n",
      "  x_jax = jax_dlpack.from_dlpack(torch_dlpack.to_dlpack(x_torch))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import torch\n",
    "\n",
    "# Jax function\n",
    "\n",
    "@jax.jit\n",
    "def jax_pow(x, y = 2):\n",
    "  return x ** y\n",
    "\n",
    "# convert to Torch function\n",
    "\n",
    "torch_pow = jax2torch(jax_pow)\n",
    "\n",
    "# run it on Torch data!\n",
    "\n",
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch_pow(x, y = 3)\n",
    "print(y)  # tensor([1., 8., 27.])\n",
    "\n",
    "# And differentiate!\n",
    "\n",
    "x = torch.tensor([2., 3.], requires_grad=True)\n",
    "y = torch.sum(torch_pow(x, y=3))\n",
    "y.backward()\n",
    "print(x.grad) # tensor([12., 27.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
